(dp0
S'train_loss'
p1
(lp2
F4.652063903808593
aF3.8529571533203124
aF3.3020974731445314
aF2.96251708984375
aF2.820589599609375
aF2.7480429077148436
aF2.724054260253906
aF2.6943984985351563
aF2.663436279296875
aF2.612790832519531
aF2.6199420166015623
aF2.6087860107421874
aF2.5709466552734375
aF2.5798956298828126
aF2.559905700683594
aF2.55157470703125
aF2.543073272705078
aF2.5127943420410155
aF2.530303497314453
aF2.5334103393554686
aF2.516123046875
aF2.5064479064941407
aF2.4812252807617186
aF2.486450500488281
aF2.4892649841308594
aF2.4823426818847656
aF2.469942321777344
aF2.476307220458984
aF2.4698196411132813
aF2.4604209899902343
aF2.4473594665527343
aF2.4612411499023437
aF2.454875183105469
aF2.462881774902344
aF2.4430162048339845
aF2.4481126403808595
aF2.420318603515625
aF2.4130189514160154
aF2.412553253173828
aF2.4214299011230467
aF2.405565185546875
aF2.4086851501464843
aF2.3907479858398437
aF2.387557067871094
aF2.389731750488281
aF2.391962127685547
aF2.363799743652344
aF2.3553976440429687
aF2.3511387634277345
aF2.3680975341796877
aF2.335372314453125
aF2.3412249755859373
aF2.3449156188964846
aF2.331074676513672
aF2.3324725341796877
aF2.322615203857422
aF2.2892034912109374
aF2.3133319091796873
aF2.308143310546875
aF2.299149169921875
aF2.3038101196289062
aF2.283485107421875
aF2.291576232910156
aF2.2843142700195314
aF2.2697653198242187
aF2.2741754150390623
aF2.259371032714844
aF2.2654342651367188
aF2.2499693298339842
aF2.259218444824219
aF2.274942169189453
aF2.2573373413085935
aF2.2227001953125
aF2.2319593811035157
aF2.2393658447265623
aF2.2262057495117187
aF2.232091064453125
aF2.2254620361328126
aF2.2048280334472654
aF2.2149388122558595
aF2.223419647216797
aF2.2025953674316407
aF2.203840789794922
aF2.2070777893066404
aF2.186512298583984
aF2.1901431274414063
aF2.202524871826172
aF2.1855433654785155
aF2.208169403076172
aF2.168868103027344
aF2.1899974060058596
aF2.176244354248047
aF2.1662701416015624
aF2.1727220153808595
aF2.1585379028320313
aF2.1551519775390626
aF2.1546690368652346
aF2.1523556518554687
aF2.1541819763183594
aF2.126239929199219
asS'test_loss'
p3
(lp4
F3.849487609863281
aF3.2790557861328127
aF2.9312313842773436
aF2.843092346191406
aF2.7511550903320314
aF2.694605712890625
aF2.695706787109375
aF2.6396115112304686
aF2.6042660522460936
aF2.5969183349609377
aF2.578575439453125
aF2.5764208984375
aF2.566619873046875
aF2.562422790527344
aF2.528079071044922
aF2.542890167236328
aF2.539192810058594
aF2.5235214233398438
aF2.5173324584960937
aF2.4947996520996094
aF2.494545440673828
aF2.4873202514648436
aF2.49006591796875
aF2.4822622680664064
aF2.4769277954101563
aF2.470919494628906
aF2.4616261291503907
aF2.457674713134766
aF2.468761444091797
aF2.431993865966797
aF2.451252288818359
aF2.438011016845703
aF2.448358917236328
aF2.4198255920410157
aF2.439561614990234
aF2.4308872985839844
aF2.423378143310547
aF2.398631286621094
aF2.4170352172851564
aF2.414633941650391
aF2.392559814453125
aF2.3853656005859376
aF2.382912139892578
aF2.377482147216797
aF2.38546630859375
aF2.371643371582031
aF2.353465728759766
aF2.3502537536621095
aF2.3464729309082033
aF2.3334800720214846
aF2.3341236877441407
aF2.336972961425781
aF2.307790985107422
aF2.325420379638672
aF2.310445556640625
aF2.3174560546875
aF2.3115740966796876
aF2.3044912719726565
aF2.2998077392578127
aF2.2976028442382814
aF2.2679981994628906
aF2.2694215393066406
aF2.2684695434570314
aF2.2824052429199218
aF2.2630184936523436
aF2.267061309814453
aF2.2518373107910157
aF2.257242889404297
aF2.2668768310546876
aF2.2393443298339846
aF2.231978759765625
aF2.2354972839355467
aF2.230287628173828
aF2.214905090332031
aF2.2396449279785156
aF2.2175303649902345
aF2.2192503356933595
aF2.2153919982910155
aF2.2027626037597656
aF2.198055419921875
aF2.18967529296875
aF2.204286804199219
aF2.1979237365722657
aF2.20604736328125
aF2.216806640625
aF2.194549560546875
aF2.1753919982910155
aF2.1875926208496095
aF2.1664453125
aF2.1699369812011717
aF2.184556427001953
aF2.1675375366210936
aF2.1574607849121095
aF2.162070007324219
aF2.145199890136719
aF2.151589202880859
aF2.1244190979003905
aF2.148451843261719
aF2.143132781982422
aF2.127003936767578
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008928858452514468
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 6s'
p10
sS'final_test_loss'
p11
F2.127003936767578
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xc5\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.