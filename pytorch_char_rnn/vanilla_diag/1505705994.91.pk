(dp0
S'train_loss'
p1
(lp2
F4.598125305175781
aF3.8776409912109373
aF3.2320291137695314
aF2.9109646606445314
aF2.8215536499023437
aF2.76975341796875
aF2.716532897949219
aF2.702081298828125
aF2.6632794189453124
aF2.628084411621094
aF2.601142578125
aF2.609365234375
aF2.5794717407226564
aF2.5619781494140623
aF2.561543273925781
aF2.5320111083984376
aF2.5523382568359376
aF2.531674346923828
aF2.5202174377441406
aF2.5077674865722654
aF2.5056044006347657
aF2.518649444580078
aF2.4940750122070314
aF2.481282196044922
aF2.5031805419921875
aF2.4737449645996095
aF2.4683290100097657
aF2.4697647094726562
aF2.457181091308594
aF2.447967224121094
aF2.4602589416503906
aF2.440257720947266
aF2.438002166748047
aF2.4344772338867187
aF2.430150604248047
aF2.414864349365234
aF2.4174452209472657
aF2.411071014404297
aF2.405794372558594
aF2.394986114501953
aF2.4077633666992186
aF2.3833828735351563
aF2.3795538330078125
aF2.3771441650390623
aF2.38115478515625
aF2.3722660827636717
aF2.35654541015625
aF2.331651458740234
aF2.3350836181640626
aF2.349947509765625
aF2.3383909606933595
aF2.3350465393066404
aF2.339742889404297
aF2.3232786560058596
aF2.3285508728027344
aF2.303221893310547
aF2.309748229980469
aF2.3208193969726563
aF2.283078155517578
aF2.284563446044922
aF2.282445526123047
aF2.2686833190917968
aF2.278461608886719
aF2.2536463928222656
aF2.2622938537597657
aF2.257012939453125
aF2.2621197509765625
aF2.2606930541992187
aF2.2400888061523436
aF2.2633853149414063
aF2.2460443115234376
aF2.2425091552734373
aF2.224412536621094
aF2.2316622924804688
aF2.21073486328125
aF2.2170034790039064
aF2.229312744140625
aF2.2260369873046875
aF2.2178465270996095
aF2.2001307678222655
aF2.1935662841796875
aF2.1947454833984374
aF2.1909962463378907
aF2.191123046875
aF2.1834686279296873
aF2.2007707214355468
aF2.190574035644531
aF2.1679100036621093
aF2.181931457519531
aF2.177215118408203
aF2.1592481994628905
aF2.1742262268066406
aF2.168287658691406
aF2.1558956909179687
aF2.1529315185546873
aF2.1171632385253907
aF2.1512457275390626
aF2.14727294921875
aF2.139856414794922
aF2.1257977294921875
asS'test_loss'
p3
(lp4
F3.8793704223632814
aF3.228210754394531
aF2.8584353637695314
aF2.8202749633789064
aF2.7276815795898437
aF2.7187155151367186
aF2.6966790771484375
aF2.647911376953125
aF2.6174880981445314
aF2.585932312011719
aF2.59044189453125
aF2.575748291015625
aF2.5676922607421875
aF2.559880676269531
aF2.557139129638672
aF2.523144073486328
aF2.50713134765625
aF2.500690765380859
aF2.5158505249023437
aF2.4971327209472656
aF2.4968197631835936
aF2.4732539367675783
aF2.4774708557128906
aF2.471639862060547
aF2.4591937255859375
aF2.4491722106933596
aF2.4591293334960938
aF2.4556187438964843
aF2.4378091430664064
aF2.434992218017578
aF2.4319190979003906
aF2.4215658569335936
aF2.4160690307617188
aF2.4035870361328127
aF2.4163056945800783
aF2.3965171813964843
aF2.3981578063964846
aF2.396446075439453
aF2.3875723266601563
aF2.3837562561035157
aF2.38576416015625
aF2.372566375732422
aF2.372714385986328
aF2.356270294189453
aF2.360742492675781
aF2.3425837707519532
aF2.3595448303222657
aF2.330128479003906
aF2.3431990051269533
aF2.3155445861816406
aF2.333077850341797
aF2.3160145568847654
aF2.3223304748535156
aF2.312633209228516
aF2.3054776000976562
aF2.312041778564453
aF2.2976039123535155
aF2.283774719238281
aF2.2751693725585938
aF2.2751806640625
aF2.2760389709472655
aF2.261583709716797
aF2.2683451843261717
aF2.2684152221679685
aF2.25462890625
aF2.2460545349121093
aF2.2582196044921874
aF2.247239227294922
aF2.2187286376953126
aF2.2481452941894533
aF2.2343565368652345
aF2.222557373046875
aF2.2152752685546875
aF2.232584228515625
aF2.2309552001953126
aF2.2151760864257812
aF2.201291961669922
aF2.205306091308594
aF2.1942686462402343
aF2.1925376892089843
aF2.2070797729492186
aF2.1864108276367187
aF2.1841172790527343
aF2.1750856018066407
aF2.1608517456054686
aF2.1625437927246094
aF2.165946350097656
aF2.1699673461914064
aF2.1594721984863283
aF2.1705899047851562
aF2.1437049865722657
aF2.1659500122070314
aF2.1462823486328126
aF2.133619537353516
aF2.1390249633789065
aF2.1375749206542967
aF2.127907257080078
aF2.1283465576171876
aF2.1164169311523438
aF2.123026580810547
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.005543165836400919
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 15s'
p10
sS'final_test_loss'
p11
F2.123026580810547
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x18\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.