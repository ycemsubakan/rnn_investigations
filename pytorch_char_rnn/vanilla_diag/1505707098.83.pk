(dp0
S'train_loss'
p1
(lp2
F4.674473266601563
aF4.646144409179687
aF4.620367736816406
aF4.589187316894531
aF4.5651177978515625
aF4.534425048828125
aF4.509556579589844
aF4.4810498046875
aF4.4561328125
aF4.430072326660156
aF4.402977294921875
aF4.3762158203125
aF4.346297912597656
aF4.327855224609375
aF4.295496520996093
aF4.269862365722656
aF4.252565307617187
aF4.2224273681640625
aF4.190086975097656
aF4.180793151855469
aF4.145950012207031
aF4.12464599609375
aF4.091977844238281
aF4.060924072265625
aF4.028653259277344
aF4.0186636352539065
aF3.9853244018554688
aF3.970433349609375
aF3.932028503417969
aF3.9058453369140627
aF3.8816815185546876
aF3.8674951171875
aF3.8279339599609377
aF3.7805215454101564
aF3.7796456909179685
aF3.759516296386719
aF3.722384033203125
aF3.699778747558594
aF3.661144104003906
aF3.654002990722656
aF3.614227600097656
aF3.58447021484375
aF3.5767877197265623
aF3.5457427978515623
aF3.510186767578125
aF3.4915591430664064
aF3.4868679809570313
aF3.443112487792969
aF3.4138433837890627
aF3.3997958374023436
aF3.361324462890625
aF3.3630731201171873
aF3.322518310546875
aF3.296406555175781
aF3.2723486328125
aF3.2555059814453124
aF3.2067050170898437
aF3.2216256713867186
aF3.1812252807617187
aF3.193714294433594
aF3.1536050415039063
aF3.135428771972656
aF3.132337646484375
aF3.1066513061523438
aF3.0887911987304686
aF3.079179382324219
aF3.0774755859375
aF3.0436697387695313
aF3.0344302368164064
aF3.038187255859375
aF2.988316955566406
aF2.9811392211914063
aF2.968013916015625
aF2.9593551635742186
aF2.958152160644531
aF2.9626699829101564
aF2.93392578125
aF2.9269808959960937
aF2.906766052246094
aF2.902540283203125
aF2.9011822509765626
aF2.8814447021484373
aF2.8721710205078126
aF2.8957257080078125
aF2.852486572265625
aF2.8658261108398437
aF2.865135498046875
aF2.8513067626953124
aF2.8532669067382814
aF2.842317810058594
aF2.831509704589844
aF2.8028390502929685
aF2.830702819824219
aF2.8133038330078124
aF2.8119171142578123
aF2.7819671630859375
aF2.7769891357421876
aF2.770810852050781
aF2.7497381591796874
aF2.769372863769531
asS'test_loss'
p3
(lp4
F4.644000549316406
aF4.619120483398437
aF4.588825988769531
aF4.561063842773438
aF4.534356994628906
aF4.50638427734375
aF4.479704895019531
aF4.4579379272460935
aF4.426921081542969
aF4.401063842773437
aF4.379524841308593
aF4.347332458496094
aF4.323031005859375
aF4.3017205810546875
aF4.2743527221679685
aF4.2501220703125
aF4.2293087768554685
aF4.201147155761719
aF4.165819091796875
aF4.140353698730468
aF4.123120422363281
aF4.092917785644532
aF4.062329406738281
aF4.038694458007813
aF4.002893371582031
aF3.9916571044921874
aF3.949530029296875
aF3.9370947265625
aF3.909429626464844
aF3.86935546875
aF3.8448977661132813
aF3.8293878173828126
aF3.792398681640625
aF3.7838812255859375
aF3.746277160644531
aF3.7319381713867186
aF3.696589050292969
aF3.6699322509765624
aF3.649703369140625
aF3.628057556152344
aF3.5874459838867185
aF3.553705139160156
aF3.5294393920898437
aF3.522344970703125
aF3.475843505859375
aF3.471474609375
aF3.437496337890625
aF3.4203497314453126
aF3.3774139404296877
aF3.351929016113281
aF3.3420120239257813
aF3.315853271484375
aF3.2980157470703126
aF3.2882559204101565
aF3.268826904296875
aF3.2222323608398438
aF3.1996142578125
aF3.1953790283203123
aF3.16136474609375
aF3.1455206298828124
aF3.125723876953125
aF3.101429138183594
aF3.1082504272460936
aF3.086107177734375
aF3.058824462890625
aF3.054970397949219
aF3.0375762939453126
aF3.005920715332031
aF3.002755432128906
aF2.982321472167969
aF2.9811740112304688
aF2.9521969604492186
aF2.9545965576171875
aF2.96141357421875
aF2.9301577758789064
aF2.9316165161132814
aF2.9356231689453125
aF2.920247802734375
aF2.9009649658203127
aF2.8635528564453123
aF2.871382751464844
aF2.883497619628906
aF2.855202331542969
aF2.860795593261719
aF2.8333721923828126
aF2.8190243530273436
aF2.824149475097656
aF2.8266204833984374
aF2.8161349487304688
aF2.815306396484375
aF2.8153256225585936
aF2.785008850097656
aF2.805176086425781
aF2.808426513671875
aF2.8103558349609377
aF2.77524658203125
aF2.7702484130859375
aF2.7766683959960936
aF2.770572509765625
aF2.7816534423828125
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00028637759495539277
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 2s'
p10
sS'final_test_loss'
p11
F2.7816534423828125
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb7\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.