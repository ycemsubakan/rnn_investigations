(dp0
S'train_loss'
p1
(lp2
F4.646993103027344
aF4.627863159179688
aF4.60614013671875
aF4.595714416503906
aF4.574159545898437
aF4.561849975585938
aF4.540938720703125
aF4.52421875
aF4.50673095703125
aF4.4908074951171875
aF4.473300476074218
aF4.4589910888671875
aF4.439906921386719
aF4.417003784179688
aF4.400059509277344
aF4.385870056152344
aF4.369997253417969
aF4.350929260253906
aF4.337614135742188
aF4.316512756347656
aF4.302141723632812
aF4.281688537597656
aF4.2780999755859375
aF4.255762329101563
aF4.228164978027344
aF4.22468505859375
aF4.209737854003906
aF4.181052856445312
aF4.1703402709960935
aF4.159497375488281
aF4.145720520019531
aF4.121800231933594
aF4.10380126953125
aF4.107448425292969
aF4.081392822265625
aF4.0524813842773435
aF4.045971984863281
aF4.024809875488281
aF4.004429321289063
aF3.9911944580078127
aF3.9824871826171875
aF3.955244445800781
aF3.9423468017578127
aF3.924605407714844
aF3.9103753662109373
aF3.90262451171875
aF3.8769827270507813
aF3.8662869262695314
aF3.851499328613281
aF3.840841064453125
aF3.809595947265625
aF3.8035760498046876
aF3.779874572753906
aF3.775416259765625
aF3.745577697753906
aF3.7425244140625
aF3.70842041015625
aF3.6897271728515624
aF3.679197082519531
aF3.67484619140625
aF3.6532806396484374
aF3.6368954467773436
aF3.625687561035156
aF3.6040829467773436
aF3.59094482421875
aF3.5809146118164064
aF3.5620343017578127
aF3.5346743774414064
aF3.537847595214844
aF3.511708068847656
aF3.503599853515625
aF3.486686096191406
aF3.472867431640625
aF3.459780578613281
aF3.444258117675781
aF3.436917419433594
aF3.4191555786132812
aF3.401625061035156
aF3.386106262207031
aF3.376045837402344
aF3.3685711669921874
aF3.3409375
aF3.3385205078125
aF3.342776184082031
aF3.3357061767578124
aF3.3315774536132814
aF3.302049255371094
aF3.2822225952148436
aF3.26642822265625
aF3.261031494140625
aF3.2510507202148435
aF3.2449276733398436
aF3.2345211791992186
aF3.2197604370117188
aF3.2322686767578124
aF3.1750531005859375
aF3.18284423828125
aF3.173414306640625
aF3.156313781738281
aF3.180250244140625
asS'test_loss'
p3
(lp4
F4.627093200683594
aF4.6103564453125
aF4.58939453125
aF4.573892517089844
aF4.5575714111328125
aF4.539076538085937
aF4.525501403808594
aF4.500272216796875
aF4.486221008300781
aF4.4690115356445315
aF4.451600646972656
aF4.437375793457031
aF4.420415954589844
aF4.403463745117188
aF4.3839242553710935
aF4.367001953125
aF4.350591735839844
aF4.341371765136719
aF4.319098205566406
aF4.30652587890625
aF4.2844390869140625
aF4.271866455078125
aF4.254928894042969
aF4.238209838867188
aF4.219563293457031
aF4.1991488647460935
aF4.185455322265625
aF4.167829895019532
aF4.161943054199218
aF4.13916259765625
aF4.124421691894531
aF4.091522216796875
aF4.085926513671875
aF4.067947692871094
aF4.051478576660156
aF4.040831604003906
aF4.018387145996094
aF4.0026156616210935
aF3.991449890136719
aF3.9771234130859376
aF3.952173156738281
aF3.941278076171875
aF3.929803771972656
aF3.906748046875
aF3.89004150390625
aF3.874708557128906
aF3.8566848754882814
aF3.8441015625
aF3.8285171508789064
aF3.8309249877929688
aF3.800792541503906
aF3.784981384277344
aF3.7580194091796875
aF3.738591613769531
aF3.7404925537109377
aF3.714640808105469
aF3.686939697265625
aF3.7008590698242188
aF3.650902099609375
aF3.6427401733398437
aF3.628315734863281
aF3.621617431640625
aF3.622969665527344
aF3.584365234375
aF3.5539279174804688
aF3.564003601074219
aF3.5423809814453127
aF3.542420654296875
aF3.5232080078125
aF3.4999826049804685
aF3.490187072753906
aF3.4719134521484376
aF3.473834228515625
aF3.4331838989257815
aF3.429420166015625
aF3.4247412109375
aF3.386732482910156
aF3.393963623046875
aF3.382756652832031
aF3.3543606567382813
aF3.350084228515625
aF3.337108459472656
aF3.3324288940429687
aF3.31104736328125
aF3.304607238769531
aF3.297102966308594
aF3.2582659912109375
aF3.266515808105469
aF3.2572357177734377
aF3.2354974365234375
aF3.2256515502929686
aF3.217593078613281
aF3.22474365234375
aF3.20279052734375
aF3.1864276123046875
aF3.1672900390625
aF3.1859420776367187
aF3.149864501953125
aF3.1729986572265627
aF3.1338626098632814
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00011963334470849223
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 11s'
p10
sS'final_test_loss'
p11
F3.1338626098632814
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xeb\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.