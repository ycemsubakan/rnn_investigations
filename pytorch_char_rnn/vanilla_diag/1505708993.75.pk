(dp0
S'train_loss'
p1
(lp2
F4.647420959472656
aF4.642502746582031
aF4.637004089355469
aF4.629731140136719
aF4.623121948242187
aF4.617789916992187
aF4.609408874511718
aF4.601253356933594
aF4.598200378417968
aF4.588095092773438
aF4.585306701660156
aF4.575089111328125
aF4.56949462890625
aF4.566368408203125
aF4.557988891601562
aF4.551775512695312
aF4.542059020996094
aF4.537748413085938
aF4.532913818359375
aF4.520182495117187
aF4.520881958007813
aF4.510841674804688
aF4.5047216796875
aF4.498868103027344
aF4.489283447265625
aF4.484297180175782
aF4.479837646484375
aF4.469259338378906
aF4.461900634765625
aF4.457562561035156
aF4.4514596557617185
aF4.438673706054687
aF4.440411987304688
aF4.42537353515625
aF4.416454772949219
aF4.411494445800781
aF4.402372131347656
aF4.39564697265625
aF4.386716918945313
aF4.378784790039062
aF4.370406799316406
aF4.362422180175781
aF4.355350341796875
aF4.348610534667968
aF4.327256774902343
aF4.328171691894531
aF4.315325927734375
aF4.299380187988281
aF4.296189575195313
aF4.286162719726563
aF4.277787475585938
aF4.274718322753906
aF4.254946594238281
aF4.2457427978515625
aF4.231365356445313
aF4.22760986328125
aF4.2149136352539065
aF4.199508056640625
aF4.20121337890625
aF4.175096130371093
aF4.178238830566406
aF4.161182250976562
aF4.138488159179688
aF4.124051208496094
aF4.118348693847656
aF4.103852233886719
aF4.092985229492188
aF4.076371765136718
aF4.059515380859375
aF4.047093811035157
aF4.0336328125
aF4.03332275390625
aF4.01171142578125
aF3.992112121582031
aF3.9783590698242186
aF3.9538287353515624
aF3.956148376464844
aF3.9275106811523437
aF3.924325256347656
aF3.90626708984375
aF3.8835296630859375
aF3.8747830200195312
aF3.847933654785156
aF3.8362664794921875
aF3.833765563964844
aF3.827412109375
aF3.800922546386719
aF3.8015423583984376
aF3.7721652221679687
aF3.7603387451171875
aF3.746156921386719
aF3.728804016113281
aF3.7046890258789062
aF3.7106887817382814
aF3.6808126831054686
aF3.6905780029296875
aF3.663552551269531
aF3.648330078125
aF3.631241760253906
aF3.619130859375
asS'test_loss'
p3
(lp4
F4.6414797973632815
aF4.6358489990234375
aF4.629649353027344
aF4.621387329101562
aF4.6188116455078125
aF4.610061950683594
aF4.6013330078125
aF4.596929321289062
aF4.589435119628906
aF4.583238525390625
aF4.575947875976563
aF4.570814208984375
aF4.560687866210937
aF4.557804870605469
aF4.548764038085937
aF4.545393981933594
aF4.534895629882812
aF4.532127380371094
aF4.524151306152344
aF4.519448852539062
aF4.508205261230469
aF4.5089111328125
aF4.496436462402344
aF4.487917175292969
aF4.48351318359375
aF4.476944274902344
aF4.471645812988282
aF4.463816223144531
aF4.45466552734375
aF4.446581726074219
aF4.444229736328125
aF4.431946411132812
aF4.425503540039062
aF4.415262145996094
aF4.4086572265625
aF4.407589111328125
aF4.392217102050782
aF4.382543334960937
aF4.377711181640625
aF4.368038330078125
aF4.363794555664063
aF4.349854431152344
aF4.342974243164062
aF4.33526123046875
aF4.3227236938476565
aF4.325469055175781
aF4.305212707519531
aF4.294720458984375
aF4.279951782226562
aF4.278714599609375
aF4.261123962402344
aF4.260113525390625
aF4.242269287109375
aF4.232740783691407
aF4.218500366210938
aF4.2117831420898435
aF4.201851501464843
aF4.185072326660157
aF4.177987670898437
aF4.164562072753906
aF4.150536804199219
aF4.140455932617187
aF4.123735046386718
aF4.109168395996094
aF4.102689819335938
aF4.088929443359375
aF4.075526428222656
aF4.063545227050781
aF4.051354675292969
aF4.032956848144531
aF4.019146118164063
aF4.015633239746093
aF3.9900836181640624
aF3.973214111328125
aF3.953646545410156
aF3.941788330078125
aF3.9436331176757813
aF3.9243179321289063
aF3.8995785522460937
aF3.894864807128906
aF3.87056884765625
aF3.8629550170898437
aF3.8495913696289064
aF3.8381082153320314
aF3.803983154296875
aF3.807317199707031
aF3.7827178955078127
aF3.77535400390625
aF3.75468994140625
aF3.743602294921875
aF3.7236532592773437
aF3.711695556640625
aF3.685009765625
aF3.6840634155273437
aF3.6615200805664063
aF3.6369287109375
aF3.6119754028320314
aF3.617251281738281
aF3.604612121582031
aF3.602681884765625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00020827574817552615
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 5s'
p10
sS'final_test_loss'
p11
F3.602681884765625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'I\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.