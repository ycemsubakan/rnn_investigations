(dp0
S'train_loss'
p1
(lp2
F4.627509460449219
aF4.170025329589844
aF3.79801025390625
aF3.458272705078125
aF3.1853060913085938
aF3.02669189453125
aF2.885794982910156
aF2.821674499511719
aF2.8057916259765623
aF2.721392822265625
aF2.6771240234375
aF2.6524102783203123
aF2.648603515625
aF2.63127685546875
aF2.6091961669921875
aF2.6275369262695314
aF2.5859393310546874
aF2.57713134765625
aF2.5666839599609377
aF2.558119659423828
aF2.5546585083007813
aF2.5489633178710935
aF2.5480809020996094
aF2.5186174011230467
aF2.5356124877929687
aF2.524910430908203
aF2.5380699157714846
aF2.5251885986328126
aF2.508180847167969
aF2.5037782287597654
aF2.4963372802734374
aF2.477568817138672
aF2.502313232421875
aF2.4913980102539064
aF2.489623718261719
aF2.4735726928710937
aF2.4809393310546874
aF2.47068603515625
aF2.4626921081542967
aF2.45848388671875
aF2.466658935546875
aF2.4578314208984375
aF2.4582940673828126
aF2.463992156982422
aF2.460684967041016
aF2.444221649169922
aF2.4558087158203126
aF2.4456582641601563
aF2.442857513427734
aF2.4262904357910156
aF2.433592987060547
aF2.438729553222656
aF2.4258688354492186
aF2.4395458984375
aF2.434482421875
aF2.409764709472656
aF2.419935607910156
aF2.4089002990722657
aF2.4182275390625
aF2.4144752502441404
aF2.4159196472167968
aF2.410275573730469
aF2.41308349609375
aF2.3941000366210936
aF2.3806129455566407
aF2.3977703857421875
aF2.3856292724609376
aF2.3779783630371094
aF2.388689422607422
aF2.370652923583984
aF2.3548405456542967
aF2.3751982116699217
aF2.353746185302734
aF2.344116973876953
aF2.3364418029785154
aF2.3494334411621094
aF2.340606994628906
aF2.3403660583496095
aF2.358218231201172
aF2.336465148925781
aF2.3316949462890624
aF2.315601043701172
aF2.333875732421875
aF2.3197703552246094
aF2.320786437988281
aF2.3216188049316404
aF2.3061141967773438
aF2.3109393310546875
aF2.3115109252929686
aF2.301822509765625
aF2.3161473083496094
aF2.3018118286132814
aF2.300933837890625
aF2.2910475158691406
aF2.2905644226074218
aF2.285696563720703
aF2.2802020263671876
aF2.2560638427734374
aF2.286253662109375
aF2.266329040527344
asS'test_loss'
p3
(lp4
F4.172002563476562
aF3.7897296142578125
aF3.454445495605469
aF3.185128173828125
aF3.0179098510742186
aF2.891458740234375
aF2.8070529174804686
aF2.7736196899414063
aF2.7284661865234376
aF2.6951171875
aF2.6636843872070313
aF2.6527349853515627
aF2.6100387573242188
aF2.6225823974609375
aF2.5968829345703126
aF2.5878890991210937
aF2.5671395874023437
aF2.56122314453125
aF2.5442791748046876
aF2.5511845397949218
aF2.55002685546875
aF2.544519805908203
aF2.52257568359375
aF2.5101515197753907
aF2.503570098876953
aF2.5118968200683596
aF2.490625305175781
aF2.5007479858398436
aF2.496641845703125
aF2.4812355041503906
aF2.484039611816406
aF2.492920227050781
aF2.492467346191406
aF2.484335632324219
aF2.4835740661621095
aF2.466145324707031
aF2.4647091674804686
aF2.4561834716796875
aF2.4798548889160155
aF2.4566273498535156
aF2.4564312744140624
aF2.4565101623535157
aF2.4598902893066406
aF2.4435108947753905
aF2.4419427490234376
aF2.440574188232422
aF2.427449951171875
aF2.4384022521972657
aF2.4364796447753907
aF2.4301248168945313
aF2.4327452087402346
aF2.43384521484375
aF2.394503479003906
aF2.4339218139648438
aF2.4285038757324218
aF2.400751647949219
aF2.422330780029297
aF2.4052973937988282
aF2.3896052551269533
aF2.4089285278320314
aF2.402082824707031
aF2.3878462219238283
aF2.379153594970703
aF2.3820220947265627
aF2.38049560546875
aF2.3916395568847655
aF2.3665989685058593
aF2.3735626220703123
aF2.381253509521484
aF2.3689361572265626
aF2.3534596252441404
aF2.357884826660156
aF2.3675547790527345
aF2.35994140625
aF2.360012664794922
aF2.3485888671875
aF2.328366394042969
aF2.344306640625
aF2.330824432373047
aF2.315871124267578
aF2.3470167541503906
aF2.3177093505859374
aF2.326184844970703
aF2.328908233642578
aF2.318484344482422
aF2.3158865356445313
aF2.2958222961425783
aF2.312056579589844
aF2.310247039794922
aF2.2936534118652343
aF2.2832913208007812
aF2.2950457763671874
aF2.3037705993652344
aF2.291914825439453
aF2.2698178100585937
aF2.2647438049316406
aF2.263211212158203
aF2.272941131591797
aF2.268748931884766
aF2.258897552490234
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004669122517163877
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 6s'
p10
sS'final_test_loss'
p11
F2.258897552490234
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xbf\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.