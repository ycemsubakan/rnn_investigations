(dp0
S'train_loss'
p1
(lp2
F4.634298706054688
aF4.554562072753907
aF4.475630187988282
aF4.395215148925781
aF4.316166687011719
aF4.2272732543945315
aF4.135687255859375
aF4.040956420898437
aF3.927986755371094
aF3.8255441284179685
aF3.7057614135742187
aF3.5910614013671873
aF3.49048583984375
aF3.3654025268554686
aF3.3054937744140624
aF3.2196417236328125
aF3.134460144042969
aF3.1101593017578124
aF3.066805114746094
aF3.02260009765625
aF3.0164358520507815
aF2.9768026733398436
aF2.956542663574219
aF2.958160400390625
aF2.8938897705078124
aF2.8634347534179687
aF2.8914608764648437
aF2.8692587280273436
aF2.86489013671875
aF2.8041217041015627
aF2.8343017578125
aF2.7845327758789065
aF2.798728332519531
aF2.7808627319335937
aF2.7589382934570312
aF2.7433935546875
aF2.7293408203125
aF2.739500732421875
aF2.70953857421875
aF2.7337985229492188
aF2.733027038574219
aF2.717235412597656
aF2.697137756347656
aF2.6788668823242188
aF2.668465576171875
aF2.665682067871094
aF2.6756060791015623
aF2.6587176513671875
aF2.6465863037109374
aF2.6326083374023437
aF2.630368347167969
aF2.629307861328125
aF2.6200302124023436
aF2.6299749755859376
aF2.6238912963867187
aF2.599800720214844
aF2.6127505493164063
aF2.6137359619140623
aF2.607987365722656
aF2.591253356933594
aF2.5866229248046877
aF2.574542236328125
aF2.5742056274414065
aF2.5815545654296876
aF2.5702606201171876
aF2.5590972900390625
aF2.568768310546875
aF2.5865655517578126
aF2.5436329650878906
aF2.5484628295898437
aF2.5652041625976563
aF2.5537269592285154
aF2.5335023498535154
aF2.5507379150390626
aF2.545252380371094
aF2.5332870483398438
aF2.538779296875
aF2.5329580688476563
aF2.528340148925781
aF2.5235044860839846
aF2.5314056396484377
aF2.5238587951660154
aF2.5177537536621095
aF2.5278642272949217
aF2.513427734375
aF2.500598602294922
aF2.519022216796875
aF2.5209632873535157
aF2.492124328613281
aF2.500272674560547
aF2.498187713623047
aF2.496157531738281
aF2.49372802734375
aF2.474973602294922
aF2.4784815979003905
aF2.481900939941406
aF2.485459289550781
aF2.4789665222167967
aF2.486122589111328
aF2.465669860839844
asS'test_loss'
p3
(lp4
F4.55468505859375
aF4.476463317871094
aF4.3949673461914065
aF4.310785827636718
aF4.215019226074219
aF4.127798461914063
aF4.0310147094726565
aF3.9152401733398436
aF3.8275592041015627
aF3.696211853027344
aF3.615964660644531
aF3.4778903198242186
aF3.3866973876953126
aF3.289293212890625
aF3.215408935546875
aF3.157500915527344
aF3.1289266967773437
aF3.066999816894531
aF3.024375305175781
aF2.9994659423828125
aF2.9957525634765627
aF2.957225341796875
aF2.926341247558594
aF2.892413330078125
aF2.8790017700195314
aF2.8972952270507815
aF2.8659188842773435
aF2.8503103637695313
aF2.817327880859375
aF2.797388916015625
aF2.788968200683594
aF2.7908563232421875
aF2.7586785888671876
aF2.727542724609375
aF2.733265380859375
aF2.743599548339844
aF2.7538772583007813
aF2.7274298095703124
aF2.7151870727539062
aF2.676294250488281
aF2.6931341552734374
aF2.685316162109375
aF2.67936767578125
aF2.650679016113281
aF2.657229309082031
aF2.662534484863281
aF2.664632568359375
aF2.6421884155273436
aF2.6470867919921877
aF2.630009765625
aF2.6408502197265626
aF2.6112353515625
aF2.6295904541015624
aF2.6144174194335936
aF2.6063278198242186
aF2.609913330078125
aF2.594827880859375
aF2.584356689453125
aF2.583349609375
aF2.5865817260742188
aF2.577340087890625
aF2.583091125488281
aF2.5858084106445314
aF2.575107421875
aF2.564056396484375
aF2.553642578125
aF2.5652511596679686
aF2.5553944396972654
aF2.5445008850097657
aF2.550423889160156
aF2.5362612915039064
aF2.5428317260742186
aF2.5527964782714845
aF2.5128239440917968
aF2.5492962646484374
aF2.521241455078125
aF2.537933654785156
aF2.5228511047363282
aF2.5126467895507814
aF2.516401519775391
aF2.5316270446777343
aF2.5229981994628905
aF2.5053704833984374
aF2.5196099853515626
aF2.4895504760742186
aF2.4868318176269533
aF2.5046958923339844
aF2.474388427734375
aF2.503653106689453
aF2.4790533447265624
aF2.506017303466797
aF2.476513671875
aF2.4920419311523436
aF2.4603326416015623
aF2.481656951904297
aF2.4831001281738283
aF2.476389923095703
aF2.4603675842285155
aF2.464269561767578
aF2.4536477661132814
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0013417479524705134
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 19s'
p10
sS'final_test_loss'
p11
F2.4536477661132814
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'{\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.