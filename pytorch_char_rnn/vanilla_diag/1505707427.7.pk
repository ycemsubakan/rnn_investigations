(dp0
S'train_loss'
p1
(lp2
F4.624927062988281
aF4.589752197265625
aF4.554288635253906
aF4.522265319824219
aF4.487228088378906
aF4.455076599121094
aF4.418832702636719
aF4.387373046875
aF4.348069152832031
aF4.314771118164063
aF4.281038513183594
aF4.241941833496094
aF4.201121520996094
aF4.165999755859375
aF4.130546264648437
aF4.0919952392578125
aF4.0510122680664065
aF4.016926879882813
aF3.9767575073242187
aF3.9258602905273436
aF3.874722900390625
aF3.8296240234375
aF3.78572021484375
aF3.7405166625976562
aF3.708557434082031
aF3.6710137939453125
aF3.60981689453125
aF3.5643585205078123
aF3.5309567260742187
aF3.485018615722656
aF3.4443045043945313
aF3.39170654296875
aF3.3493878173828127
aF3.3196536254882814
aF3.28926025390625
aF3.251845703125
aF3.2276742553710935
aF3.2151449584960936
aF3.1669793701171876
aF3.1436871337890624
aF3.1046966552734374
aF3.11216796875
aF3.079991455078125
aF3.0333706665039064
aF3.014486999511719
aF3.0350152587890626
aF3.0069174194335937
aF2.9700567626953127
aF2.976337585449219
aF2.9576544189453124
aF2.9349322509765625
aF2.9416708374023437
aF2.929893798828125
aF2.9084091186523438
aF2.892720947265625
aF2.9000454711914063
aF2.89934814453125
aF2.8826873779296873
aF2.8532440185546877
aF2.874559020996094
aF2.8494839477539062
aF2.8341079711914063
aF2.814677734375
aF2.859988098144531
aF2.822204284667969
aF2.794718017578125
aF2.802140808105469
aF2.775567626953125
aF2.792447814941406
aF2.8011239624023436
aF2.7882321166992186
aF2.790045166015625
aF2.799290771484375
aF2.7812655639648436
aF2.7513177490234373
aF2.7680172729492187
aF2.78023193359375
aF2.7549285888671875
aF2.728188781738281
aF2.7697238159179687
aF2.7308251953125
aF2.725140380859375
aF2.746497802734375
aF2.728535461425781
aF2.704169921875
aF2.701554870605469
aF2.6959259033203127
aF2.7153704833984373
aF2.7301934814453124
aF2.7478073120117186
aF2.6676010131835937
aF2.6895465087890624
aF2.6909109497070314
aF2.7116207885742187
aF2.686515197753906
aF2.6820489501953126
aF2.6805551147460935
aF2.6654238891601563
aF2.67987548828125
aF2.677265930175781
asS'test_loss'
p3
(lp4
F4.5888623046875
aF4.552533569335938
aF4.521333923339844
aF4.48908447265625
aF4.453159484863281
aF4.420482482910156
aF4.385825500488282
aF4.348458862304687
aF4.311881103515625
aF4.276437683105469
aF4.23534423828125
aF4.206856689453125
aF4.1692025756835935
aF4.1284963989257815
aF4.090365600585938
aF4.050926818847656
aF3.995091857910156
aF3.9618463134765625
aF3.92890869140625
aF3.8689556884765626
aF3.8396783447265626
aF3.8008056640625
aF3.7348150634765624
aF3.68862548828125
aF3.6530804443359375
aF3.619376220703125
aF3.5669110107421873
aF3.526092834472656
aF3.474864196777344
aF3.434395751953125
aF3.399660339355469
aF3.351558837890625
aF3.314128112792969
aF3.2871548461914064
aF3.2573367309570314
aF3.2274212646484375
aF3.1970599365234373
aF3.1545648193359375
aF3.1552081298828125
aF3.112054443359375
aF3.09793212890625
aF3.0707467651367186
aF3.082298583984375
aF3.0377841186523438
aF2.9932369995117187
aF3.0112542724609375
aF2.9856646728515623
aF2.9850149536132813
aF2.9355349731445313
aF2.949706726074219
aF2.920079345703125
aF2.928497009277344
aF2.915150146484375
aF2.8922650146484377
aF2.9089837646484376
aF2.848671875
aF2.8789068603515626
aF2.866918640136719
aF2.8631610107421874
aF2.8221734619140624
aF2.8306674194335937
aF2.841009521484375
aF2.8274832153320313
aF2.8322628784179686
aF2.798331604003906
aF2.7956027221679687
aF2.7626165771484374
aF2.7726300048828123
aF2.8050469970703125
aF2.806080017089844
aF2.760855712890625
aF2.780003662109375
aF2.749808349609375
aF2.7867706298828123
aF2.7525125122070313
aF2.7490576171875
aF2.720308532714844
aF2.7475177001953126
aF2.7619747924804687
aF2.720530700683594
aF2.735044250488281
aF2.7151113891601564
aF2.720001525878906
aF2.708099060058594
aF2.703782653808594
aF2.713818359375
aF2.6907913208007814
aF2.695179443359375
aF2.6978118896484373
aF2.68193359375
aF2.686592712402344
aF2.697783203125
aF2.6760269165039063
aF2.687361755371094
aF2.6675103759765624
aF2.6701480102539064
aF2.6608895874023437
aF2.663951110839844
aF2.6572900390625
aF2.65775390625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00021604071675067286
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 7s'
p10
sS'final_test_loss'
p11
F2.65775390625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x0c\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.