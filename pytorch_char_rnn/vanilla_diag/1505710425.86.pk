(dp0
S'train_loss'
p1
(lp2
F4.649897766113281
aF4.171997680664062
aF3.757528991699219
aF3.392334289550781
aF3.18480224609375
aF3.0366961669921877
aF2.9596493530273436
aF2.865267333984375
aF2.800764465332031
aF2.8042892456054687
aF2.731094970703125
aF2.6986553955078123
aF2.683558349609375
aF2.6792636108398438
aF2.6488861083984374
aF2.6395233154296873
aF2.6114422607421877
aF2.6067764282226564
aF2.5929510498046877
aF2.590720520019531
aF2.5637677001953123
aF2.5716241455078124
aF2.554363555908203
aF2.544419708251953
aF2.535197296142578
aF2.5503790283203127
aF2.524084167480469
aF2.5210391235351564
aF2.508456115722656
aF2.5267147827148437
aF2.5002723693847657
aF2.5073516845703123
aF2.5029833984375
aF2.4960890197753907
aF2.4846974182128907
aF2.4928683471679687
aF2.486965789794922
aF2.4694564819335936
aF2.4839276123046874
aF2.4809918212890625
aF2.477577362060547
aF2.465096130371094
aF2.4635096740722657
aF2.468543701171875
aF2.4717767333984373
aF2.4428521728515626
aF2.454544677734375
aF2.452917175292969
aF2.437569122314453
aF2.4526028442382812
aF2.4470510864257813
aF2.4519097900390623
aF2.4315553283691407
aF2.4224610900878907
aF2.4232522583007814
aF2.4110151672363282
aF2.4198098754882813
aF2.413422393798828
aF2.4254600524902346
aF2.4159176635742186
aF2.4239015197753906
aF2.4153976440429688
aF2.410536651611328
aF2.392293243408203
aF2.4077017211914065
aF2.412567138671875
aF2.4133660888671873
aF2.4002201843261717
aF2.3898271179199218
aF2.3726194763183592
aF2.3840126037597655
aF2.370774230957031
aF2.3885791015625
aF2.3572715759277343
aF2.3691209411621093
aF2.3753596496582032
aF2.377333679199219
aF2.3555157470703123
aF2.3500131225585936
aF2.358900299072266
aF2.353950958251953
aF2.3458963012695313
aF2.3479669189453123
aF2.325753479003906
aF2.3382577514648437
aF2.3351429748535155
aF2.3338999938964844
aF2.3225218200683595
aF2.3291070556640623
aF2.316293182373047
aF2.3100514221191406
aF2.3205082702636717
aF2.30465087890625
aF2.3195765686035155
aF2.296364288330078
aF2.3203309631347655
aF2.302745819091797
aF2.3146231079101565
aF2.3090278625488283
aF2.3118051147460936
asS'test_loss'
p3
(lp4
F4.164529418945312
aF3.7535699462890624
aF3.419594421386719
aF3.15400390625
aF2.9971853637695314
aF2.92513427734375
aF2.851007995605469
aF2.8028955078125
aF2.7518515014648437
aF2.7429119873046877
aF2.681490783691406
aF2.6928668212890625
aF2.6453875732421874
aF2.6457778930664064
aF2.635320739746094
aF2.60840576171875
aF2.6129876708984376
aF2.59063720703125
aF2.565371398925781
aF2.561534729003906
aF2.5578982543945314
aF2.5273455810546874
aF2.533139953613281
aF2.535227966308594
aF2.529526672363281
aF2.517170104980469
aF2.5155242919921874
aF2.5251536560058594
aF2.491379852294922
aF2.5009413146972657
aF2.498721466064453
aF2.4891709899902343
aF2.478321990966797
aF2.4902110290527344
aF2.4827178955078124
aF2.4811183166503907
aF2.481869354248047
aF2.479970703125
aF2.4626895141601564
aF2.459202880859375
aF2.4497132873535157
aF2.4554373168945314
aF2.4492428588867186
aF2.467084503173828
aF2.4543276977539064
aF2.445161895751953
aF2.446319274902344
aF2.430574493408203
aF2.4431065368652343
aF2.434626159667969
aF2.424832611083984
aF2.4308782958984376
aF2.4322550964355467
aF2.4138226318359375
aF2.4245370483398436
aF2.4133531188964845
aF2.4288330078125
aF2.4056269836425783
aF2.4142503356933593
aF2.3995489501953124
aF2.400925598144531
aF2.398334655761719
aF2.388370361328125
aF2.3927850341796875
aF2.38980224609375
aF2.3887652587890624
aF2.395266876220703
aF2.37837158203125
aF2.3649754333496094
aF2.38489990234375
aF2.3698997497558594
aF2.3685667419433596
aF2.389890594482422
aF2.366965484619141
aF2.347410125732422
aF2.3529380798339843
aF2.354222412109375
aF2.3604580688476564
aF2.350474395751953
aF2.3562442016601564
aF2.3478407287597656
aF2.355307312011719
aF2.3313858032226564
aF2.328940124511719
aF2.332731018066406
aF2.3264027404785157
aF2.3157334899902344
aF2.3181581115722656
aF2.3253558349609373
aF2.31236083984375
aF2.293612823486328
aF2.314058990478516
aF2.3133091735839844
aF2.293612060546875
aF2.3000830078125
aF2.289384765625
aF2.297579803466797
aF2.293340148925781
aF2.3056568908691406
aF2.297151794433594
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0042990425304085525
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 7s'
p10
sS'final_test_loss'
p11
F2.297151794433594
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xc4\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.