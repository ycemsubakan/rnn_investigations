(dp0
S'train_loss'
p1
(lp2
F4.670877380371094
aF4.63966064453125
aF4.609082336425781
aF4.586782836914063
aF4.558378295898438
aF4.529197387695312
aF4.499812316894531
aF4.474627075195312
aF4.4441156005859375
aF4.426380615234375
aF4.39084716796875
aF4.371641540527344
aF4.3363778686523435
aF4.314173278808593
aF4.2927243041992185
aF4.2593603515625
aF4.244382629394531
aF4.2055902099609375
aF4.182091979980469
aF4.168050537109375
aF4.131712951660156
aF4.110179138183594
aF4.0890066528320315
aF4.057450256347656
aF4.033087463378906
aF4.004563903808593
aF3.9807034301757813
aF3.9631298828125
aF3.9164898681640623
aF3.9096356201171876
aF3.873651123046875
aF3.8579998779296876
aF3.835276794433594
aF3.8069424438476562
aF3.7872747802734374
aF3.7611529541015627
aF3.7279278564453127
aF3.703309326171875
aF3.6871722412109373
aF3.6595416259765625
aF3.6279415893554687
aF3.6390005493164064
aF3.578607482910156
aF3.5886553955078124
aF3.5393685913085937
aF3.513865966796875
aF3.5279510498046873
aF3.5089080810546873
aF3.468861083984375
aF3.45707275390625
aF3.404515380859375
aF3.4141006469726562
aF3.3960418701171875
aF3.3644305419921876
aF3.352244873046875
aF3.3147271728515624
aF3.3188137817382812
aF3.30990966796875
aF3.283780822753906
aF3.2531805419921875
aF3.2406240844726564
aF3.224053955078125
aF3.217834167480469
aF3.193052978515625
aF3.2006124877929687
aF3.1567611694335938
aF3.12739501953125
aF3.1451095581054687
aF3.142236633300781
aF3.126907958984375
aF3.105432434082031
aF3.0969598388671873
aF3.0960528564453127
aF3.06873046875
aF3.049268798828125
aF3.0626345825195314
aF3.0191195678710936
aF3.032509460449219
aF3.0206289672851563
aF2.98763916015625
aF2.9892257690429687
aF2.9742965698242188
aF2.9564950561523435
aF2.9412326049804687
aF2.9489398193359375
aF2.9356298828125
aF2.9272357177734376
aF2.9393792724609376
aF2.9117742919921876
aF2.952818603515625
aF2.8896527099609375
aF2.8977230834960936
aF2.9140310668945313
aF2.894797668457031
aF2.8695733642578123
aF2.87107421875
aF2.8834048461914064
aF2.862246398925781
aF2.8587417602539062
aF2.83944091796875
asS'test_loss'
p3
(lp4
F4.640192565917968
aF4.612512512207031
aF4.584315185546875
aF4.556134033203125
aF4.531396179199219
aF4.502337341308594
aF4.477931518554687
aF4.447976989746094
aF4.417416687011719
aF4.39697509765625
aF4.370134887695312
aF4.339070739746094
aF4.311197509765625
aF4.287321166992188
aF4.262356872558594
aF4.2288330078125
aF4.211872253417969
aF4.177638244628906
aF4.159740600585938
aF4.140849609375
aF4.099960632324219
aF4.083461303710937
aF4.050155334472656
aF4.025692443847657
aF3.997031555175781
aF3.9764199829101563
aF3.956351623535156
aF3.9169158935546875
aF3.89137451171875
aF3.8757818603515624
aF3.8554803466796876
aF3.837840576171875
aF3.796913757324219
aF3.7786041259765626
aF3.756714172363281
aF3.7338790893554688
aF3.712666015625
aF3.6936248779296874
aF3.6658596801757812
aF3.6222543334960937
aF3.6166851806640623
aF3.5826611328125
aF3.5729141235351562
aF3.5491943359375
aF3.513050842285156
aF3.500124206542969
aF3.4907666015625
aF3.4828060913085936
aF3.440279235839844
aF3.4218490600585936
aF3.3940554809570314
aF3.3947909545898436
aF3.3555865478515625
aF3.3414190673828124
aF3.3221624755859374
aF3.307830505371094
aF3.29371337890625
aF3.2648074340820314
aF3.2433035278320315
aF3.2560882568359375
aF3.204426574707031
aF3.2053680419921875
aF3.199296569824219
aF3.1722042846679686
aF3.1733407592773437
aF3.136710510253906
aF3.1399078369140625
aF3.1021417236328124
aF3.1212335205078126
aF3.082684326171875
aF3.0703829956054687
aF3.0716934204101562
aF3.0733993530273436
aF3.0577621459960938
aF3.0311273193359374
aF3.0108297729492186
aF3.025229187011719
aF3.00248779296875
aF3.003049011230469
aF2.9940533447265625
aF2.9910186767578124
aF2.9594320678710937
aF2.9548150634765626
aF2.944954528808594
aF2.927457275390625
aF2.9074017333984377
aF2.896506652832031
aF2.9023248291015626
aF2.898432312011719
aF2.8969500732421873
aF2.895765380859375
aF2.873651123046875
aF2.8733306884765626
aF2.869553527832031
aF2.870175476074219
aF2.852492370605469
aF2.8488198852539064
aF2.857821044921875
aF2.8161053466796875
aF2.8408660888671875
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00017833359640231854
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 13s'
p10
sS'final_test_loss'
p11
F2.8408660888671875
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xfb\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.