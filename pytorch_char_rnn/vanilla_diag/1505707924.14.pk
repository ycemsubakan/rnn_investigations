(dp0
S'train_loss'
p1
(lp2
F4.657481689453125
aF4.62125732421875
aF4.578545837402344
aF4.54363037109375
aF4.511920776367187
aF4.468499755859375
aF4.436199035644531
aF4.394666748046875
aF4.361033935546875
aF4.321504516601562
aF4.286968688964844
aF4.250267028808594
aF4.210666809082031
aF4.179700927734375
aF4.1432427978515625
aF4.106880798339843
aF4.071787109375
aF4.046466674804687
aF4.001694641113281
aF3.976029052734375
aF3.918484802246094
aF3.890417175292969
aF3.8613250732421873
aF3.8249847412109377
aF3.7999423217773436
aF3.751123352050781
aF3.725702819824219
aF3.6845648193359377
aF3.653858337402344
aF3.620677185058594
aF3.577085266113281
aF3.562188720703125
aF3.5204644775390626
aF3.491008605957031
aF3.4577972412109377
aF3.4230087280273436
aF3.388022766113281
aF3.374161682128906
aF3.3531103515625
aF3.311928405761719
aF3.30156005859375
aF3.248701477050781
aF3.2316690063476563
aF3.2198452758789062
aF3.182772521972656
aF3.17204833984375
aF3.1276815795898436
aF3.0972354125976564
aF3.0915512084960937
aF3.064050598144531
aF3.07487548828125
aF3.0490313720703126
aF3.045024108886719
aF3.0038275146484374
aF2.996937255859375
aF3.0030157470703127
aF2.987171630859375
aF2.964396667480469
aF2.95706298828125
aF2.9306683349609375
aF2.940360412597656
aF2.91256591796875
aF2.8972207641601564
aF2.8850115966796874
aF2.871953430175781
aF2.872053527832031
aF2.8833993530273436
aF2.8747769165039063
aF2.8450833129882813
aF2.8362969970703125
aF2.835804138183594
aF2.8375088500976564
aF2.799813537597656
aF2.8192132568359374
aF2.801982727050781
aF2.784755859375
aF2.7903616333007815
aF2.7741162109375
aF2.782048645019531
aF2.794004211425781
aF2.7648544311523438
aF2.769917297363281
aF2.767935791015625
aF2.763103332519531
aF2.745955505371094
aF2.7537579345703125
aF2.751770324707031
aF2.7404071044921876
aF2.7154135131835937
aF2.735252685546875
aF2.733331604003906
aF2.7434982299804687
aF2.7008197021484377
aF2.715055236816406
aF2.7258566284179686
aF2.700048522949219
aF2.702376708984375
aF2.68224609375
aF2.6955410766601564
aF2.7036517333984373
asS'test_loss'
p3
(lp4
F4.6180508422851565
aF4.580196228027344
aF4.543687133789063
aF4.509299621582032
aF4.469459228515625
aF4.43423828125
aF4.395274963378906
aF4.358133850097656
aF4.320023193359375
aF4.290989990234375
aF4.244936218261719
aF4.208928833007812
aF4.184254455566406
aF4.13992431640625
aF4.108410949707031
aF4.072210693359375
aF4.029395751953125
aF3.9925918579101562
aF3.9674041748046873
aF3.9243218994140623
aF3.8849102783203127
aF3.8609793090820315
aF3.812471618652344
aF3.790601806640625
aF3.7569009399414064
aF3.7127951049804686
aF3.6726010131835936
aF3.644899597167969
aF3.6134405517578125
aF3.566314697265625
aF3.533990783691406
aF3.512528991699219
aF3.485750427246094
aF3.471147155761719
aF3.407706298828125
aF3.3978955078125
aF3.3662734985351563
aF3.340655517578125
aF3.314242248535156
aF3.3000982666015624
aF3.2500213623046874
aF3.2238702392578125
aF3.1862701416015624
aF3.1845742797851564
aF3.161806640625
aF3.1535653686523437
aF3.130384826660156
aF3.09205322265625
aF3.0983126831054686
aF3.0553839111328127
aF3.0620928955078126
aF3.0345834350585936
aF3.005681457519531
aF2.989335021972656
aF2.97656982421875
aF2.953707580566406
aF2.946089782714844
aF2.9417193603515623
aF2.912086181640625
aF2.9256045532226564
aF2.8880276489257812
aF2.8791046142578125
aF2.8752865600585937
aF2.8763473510742186
aF2.873641357421875
aF2.84641357421875
aF2.846587829589844
aF2.849376220703125
aF2.837926025390625
aF2.828544921875
aF2.8004190063476564
aF2.7801141357421875
aF2.8061004638671876
aF2.81554931640625
aF2.7933721923828125
aF2.788389892578125
aF2.7666806030273436
aF2.7797775268554688
aF2.7800225830078125
aF2.744109802246094
aF2.757117919921875
aF2.753004150390625
aF2.7512551879882814
aF2.7409490966796874
aF2.7372952270507813
aF2.7408071899414064
aF2.7216900634765624
aF2.7384512329101565
aF2.7088668823242186
aF2.735304260253906
aF2.7368173217773437
aF2.7132867431640624
aF2.690829162597656
aF2.7085162353515626
aF2.719320068359375
aF2.683222351074219
aF2.692458190917969
aF2.690765075683594
aF2.662310791015625
aF2.6770120239257813
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0004180275071362277
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 58s'
p10
sS'final_test_loss'
p11
F2.6770120239257813
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xa1\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.