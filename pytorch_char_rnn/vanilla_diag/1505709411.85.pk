(dp0
S'train_loss'
p1
(lp2
F4.58097412109375
aF4.41244140625
aF4.237705993652344
aF4.0832693481445315
aF3.889518127441406
aF3.659102478027344
aF3.49906005859375
aF3.2845425415039062
aF3.1509158325195314
aF3.064224853515625
aF3.0112188720703124
aF2.936468505859375
aF2.9107086181640627
aF2.8813021850585936
aF2.8605081176757814
aF2.830220947265625
aF2.79880859375
aF2.7761581420898436
aF2.778583679199219
aF2.779627685546875
aF2.7429730224609377
aF2.686051025390625
aF2.673415222167969
aF2.7050466918945313
aF2.6656610107421876
aF2.649635925292969
aF2.6588018798828124
aF2.64940185546875
aF2.608645324707031
aF2.6198828125
aF2.6082861328125
aF2.6162838745117187
aF2.5814385986328126
aF2.603809509277344
aF2.570667419433594
aF2.57938720703125
aF2.5656494140625
aF2.568360595703125
aF2.5542799377441407
aF2.5609130859375
aF2.5454110717773437
aF2.5518861389160157
aF2.53982666015625
aF2.5218382263183594
aF2.5252651977539062
aF2.5086538696289065
aF2.5036611938476563
aF2.5172625732421876
aF2.4951902770996095
aF2.498011016845703
aF2.5055006408691405
aF2.499476013183594
aF2.4916917419433595
aF2.488100280761719
aF2.5073219299316407
aF2.4858848571777346
aF2.4784228515625
aF2.494227294921875
aF2.4574070739746094
aF2.491563262939453
aF2.466347198486328
aF2.4884619140625
aF2.4638262939453126
aF2.4582229614257813
aF2.4666514587402344
aF2.4522804260253905
aF2.4542156982421877
aF2.4430030822753905
aF2.451504974365234
aF2.434212493896484
aF2.441425476074219
aF2.4413133239746094
aF2.4259371948242188
aF2.45544677734375
aF2.4395533752441407
aF2.4297665405273436
aF2.40581787109375
aF2.408724060058594
aF2.4093173217773436
aF2.4176197814941407
aF2.4023655700683593
aF2.4121640014648436
aF2.4056082153320313
aF2.3981806945800783
aF2.388047637939453
aF2.4057025146484374
aF2.3983914184570314
aF2.372771453857422
aF2.3739999389648436
aF2.389757080078125
aF2.3866757202148436
aF2.378836212158203
aF2.3630406188964845
aF2.363234100341797
aF2.367190399169922
aF2.36964111328125
aF2.3652037048339842
aF2.3494305419921875
aF2.363122863769531
aF2.356880798339844
asS'test_loss'
p3
(lp4
F4.41204833984375
aF4.246354064941406
aF4.06459716796875
aF3.8789944458007812
aF3.655506286621094
aF3.4594854736328124
aF3.2991278076171877
aF3.1709420776367185
aF3.04599365234375
aF2.9679940795898436
aF2.9296603393554688
aF2.8798883056640623
aF2.860828857421875
aF2.8470828247070314
aF2.819583740234375
aF2.7986764526367187
aF2.7681210327148436
aF2.746559753417969
aF2.729444885253906
aF2.7182931518554687
aF2.6939459228515625
aF2.7097677612304687
aF2.6896160888671874
aF2.661127014160156
aF2.6474530029296877
aF2.643040771484375
aF2.62359619140625
aF2.6427047729492186
aF2.611395263671875
aF2.6019610595703124
aF2.6376507568359373
aF2.5745169067382814
aF2.58613525390625
aF2.567052917480469
aF2.566336975097656
aF2.562158508300781
aF2.563465881347656
aF2.554765319824219
aF2.567010498046875
aF2.554221954345703
aF2.534046325683594
aF2.5377265930175783
aF2.531473388671875
aF2.524783172607422
aF2.5061279296875
aF2.5070199584960937
aF2.497478332519531
aF2.487968292236328
aF2.522918243408203
aF2.4988699340820313
aF2.4861607360839844
aF2.488447418212891
aF2.507188720703125
aF2.482846374511719
aF2.492939453125
aF2.4739341735839844
aF2.4911534118652345
aF2.476636962890625
aF2.477644348144531
aF2.4538920593261717
aF2.4743161010742187
aF2.438173980712891
aF2.447168426513672
aF2.448343505859375
aF2.462449188232422
aF2.4616453552246096
aF2.445450439453125
aF2.438900909423828
aF2.417960662841797
aF2.437578582763672
aF2.4175643920898438
aF2.4311677551269533
aF2.4249946594238283
aF2.4214251708984373
aF2.4241725158691407
aF2.403775329589844
aF2.4166766357421876
aF2.3948703002929688
aF2.3996340942382814
aF2.408548583984375
aF2.400012969970703
aF2.391546630859375
aF2.388489990234375
aF2.3910450744628906
aF2.406796722412109
aF2.3869676208496093
aF2.390688629150391
aF2.380937042236328
aF2.379161529541016
aF2.3721444702148435
aF2.3707371520996094
aF2.3626145935058593
aF2.35282958984375
aF2.3513229370117186
aF2.3540626525878907
aF2.3656961059570314
aF2.3549085998535157
aF2.361524200439453
aF2.3566207885742188
aF2.362288055419922
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0019429450453239765
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 45s'
p10
sS'final_test_loss'
p11
F2.362288055419922
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb0\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.