(dp0
S'train_loss'
p1
(lp2
F4.609953308105469
aF4.22056640625
aF3.872231140136719
aF3.5802935791015624
aF3.3279052734375
aF3.1319549560546873
aF2.996040954589844
aF2.889698791503906
aF2.8192703247070314
aF2.7954986572265623
aF2.7537518310546876
aF2.7193399047851563
aF2.712012939453125
aF2.6713616943359373
aF2.6480523681640626
aF2.629249267578125
aF2.594896240234375
aF2.603148193359375
aF2.6200634765625
aF2.580597229003906
aF2.595614318847656
aF2.568192443847656
aF2.55235595703125
aF2.5620123291015626
aF2.5359619140625
aF2.5466136169433593
aF2.52552490234375
aF2.5157810974121095
aF2.504588165283203
aF2.4968698120117185
aF2.5017344665527346
aF2.513853759765625
aF2.5249790954589844
aF2.483590850830078
aF2.498490447998047
aF2.5120196533203125
aF2.484271240234375
aF2.509981842041016
aF2.480696105957031
aF2.4723136901855467
aF2.4831138610839845
aF2.4625918579101564
aF2.4741050720214846
aF2.4639813232421877
aF2.4763032531738283
aF2.4781838989257814
aF2.4594480895996096
aF2.4622068786621094
aF2.460732421875
aF2.4462701416015626
aF2.453615264892578
aF2.459608154296875
aF2.4454104614257814
aF2.4359083557128907
aF2.4419430541992186
aF2.4250616455078124
aF2.447342529296875
aF2.443156280517578
aF2.4149409484863282
aF2.4255599975585938
aF2.443708953857422
aF2.417514190673828
aF2.4190309143066404
aF2.427052459716797
aF2.418570098876953
aF2.4144483947753907
aF2.400609130859375
aF2.401293182373047
aF2.4192959594726564
aF2.387992706298828
aF2.404862518310547
aF2.4037319946289064
aF2.379982452392578
aF2.3952896118164064
aF2.3931149291992186
aF2.389180450439453
aF2.3762245178222656
aF2.365983581542969
aF2.3897061157226562
aF2.376195373535156
aF2.3784698486328124
aF2.379574890136719
aF2.372192687988281
aF2.3676211547851564
aF2.3739369201660154
aF2.360251922607422
aF2.347839660644531
aF2.352703857421875
aF2.3472055053710936
aF2.3519515991210938
aF2.351969757080078
aF2.347321014404297
aF2.3424424743652343
aF2.346194000244141
aF2.334822082519531
aF2.3312261962890624
aF2.3149397277832033
aF2.3218992614746092
aF2.3089894104003905
aF2.3273324584960937
asS'test_loss'
p3
(lp4
F4.224066467285156
aF3.8755593872070313
aF3.56393798828125
aF3.3071148681640623
aF3.1261215209960938
aF2.9881005859375
aF2.9021725463867187
aF2.8060342407226564
aF2.7835296630859374
aF2.752241516113281
aF2.708189697265625
aF2.6693551635742185
aF2.6918695068359373
aF2.665220031738281
aF2.6365057373046876
aF2.6154522705078125
aF2.5938494873046873
aF2.563748474121094
aF2.575639343261719
aF2.57088623046875
aF2.545651397705078
aF2.5545028686523437
aF2.5500901794433593
aF2.528910675048828
aF2.540082550048828
aF2.5239723205566404
aF2.5397930908203126
aF2.525551300048828
aF2.5221192932128904
aF2.5159751892089846
aF2.4996971130371093
aF2.4948095703125
aF2.4976141357421877
aF2.4819100952148436
aF2.501150817871094
aF2.4781138610839846
aF2.4832522583007814
aF2.4766641235351563
aF2.4732682800292967
aF2.4767945861816405
aF2.4808349609375
aF2.476556396484375
aF2.470894317626953
aF2.473134002685547
aF2.4588470458984375
aF2.4794329833984374
aF2.4534181213378905
aF2.4565963745117188
aF2.439949951171875
aF2.426818542480469
aF2.4441288757324218
aF2.429151611328125
aF2.4385948181152344
aF2.4362890625
aF2.4342884826660156
aF2.444619445800781
aF2.4322764587402346
aF2.432539520263672
aF2.4189181518554688
aF2.426943817138672
aF2.422679748535156
aF2.421988220214844
aF2.400671081542969
aF2.400667419433594
aF2.4168212890625
aF2.407848663330078
aF2.401099853515625
aF2.3812261962890626
aF2.4010934448242187
aF2.403461151123047
aF2.391645965576172
aF2.3968983459472657
aF2.3690614318847656
aF2.400461883544922
aF2.3942166137695313
aF2.372576141357422
aF2.3687860107421876
aF2.3613995361328124
aF2.374119873046875
aF2.3559893798828124
aF2.3416802978515623
aF2.3434307861328123
aF2.3708065795898436
aF2.3643447875976564
aF2.3408743286132814
aF2.3387623596191407
aF2.345747222900391
aF2.3377757263183594
aF2.343960723876953
aF2.3403993225097657
aF2.326834716796875
aF2.3282781982421876
aF2.335654296875
aF2.3324810791015627
aF2.3229331970214844
aF2.3055369567871096
aF2.327758026123047
aF2.3173446655273438
aF2.310413818359375
aF2.31198486328125
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004028762619946425
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 1s'
p10
sS'final_test_loss'
p11
F2.31198486328125
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb0\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.