(dp0
S'train_loss'
p1
(lp2
F4.641565551757813
aF4.4542041015625
aF4.294063415527344
aF4.123720092773437
aF3.960813293457031
aF3.8149288940429686
aF3.682979736328125
aF3.5647808837890627
aF3.44816650390625
aF3.3294888305664063
aF3.2564276123046874
aF3.1617752075195313
aF3.0959207153320314
aF3.0039437866210936
aF2.98359130859375
aF2.9234771728515625
aF2.879456787109375
aF2.852191162109375
aF2.8037628173828124
aF2.7739303588867186
aF2.7557275390625
aF2.740253601074219
aF2.7231924438476565
aF2.7191192626953127
aF2.674786376953125
aF2.6820086669921874
aF2.661044006347656
aF2.65397216796875
aF2.6633926391601563
aF2.6424850463867187
aF2.635061950683594
aF2.60779052734375
aF2.6093429565429687
aF2.5768911743164065
aF2.6086456298828127
aF2.5773239135742188
aF2.5736380004882813
aF2.5789556884765625
aF2.5691925048828126
aF2.54219482421875
aF2.5362089538574217
aF2.5525274658203125
aF2.5363783264160156
aF2.546170654296875
aF2.5229928588867185
aF2.5297012329101562
aF2.530931854248047
aF2.5062387084960935
aF2.500548858642578
aF2.518818054199219
aF2.5296701049804686
aF2.50928955078125
aF2.516726379394531
aF2.5173175048828127
aF2.5130128479003906
aF2.5057797241210937
aF2.515081024169922
aF2.5155154418945314
aF2.5106576538085936
aF2.519320526123047
aF2.4967816162109373
aF2.505462341308594
aF2.5027040100097655
aF2.499666748046875
aF2.4903173828125
aF2.487841796875
aF2.492362213134766
aF2.5094645690917967
aF2.4929934692382814
aF2.4813766479492188
aF2.472066650390625
aF2.4729183959960936
aF2.4731263732910156
aF2.484886474609375
aF2.4590165710449217
aF2.490795440673828
aF2.4871728515625
aF2.4757044982910155
aF2.4925054931640624
aF2.4788412475585937
aF2.461791229248047
aF2.4713543701171874
aF2.477364349365234
aF2.4735882568359373
aF2.467916259765625
aF2.4818988037109375
aF2.4634654235839846
aF2.4559310913085937
aF2.4789674377441404
aF2.4690858459472658
aF2.461555633544922
aF2.4659759521484377
aF2.4400650024414063
aF2.45782470703125
aF2.456053924560547
aF2.4477320861816407
aF2.455705413818359
aF2.4443357849121092
aF2.4590611267089844
aF2.4485382080078124
asS'test_loss'
p3
(lp4
F4.4560498046875
aF4.277946472167969
aF4.1131298828125
aF3.966727294921875
aF3.823804931640625
aF3.681951599121094
aF3.578829345703125
aF3.454950256347656
aF3.3435736083984375
aF3.2322113037109377
aF3.1689501953125
aF3.088544921875
aF3.033757629394531
aF2.9475091552734374
aF2.9114010620117186
aF2.859624328613281
aF2.82346923828125
aF2.7988436889648436
aF2.780550231933594
aF2.7302175903320314
aF2.728984680175781
aF2.712967834472656
aF2.699321594238281
aF2.6810311889648437
aF2.649833068847656
aF2.6637646484375
aF2.638052978515625
aF2.633761291503906
aF2.6236227416992186
aF2.615243225097656
aF2.59247314453125
aF2.592671203613281
aF2.5774700927734373
aF2.59387939453125
aF2.5621908569335936
aF2.5714849853515624
aF2.5696966552734377
aF2.5770523071289064
aF2.5432003784179686
aF2.5368019104003907
aF2.5280024719238283
aF2.5406178283691405
aF2.5339463806152343
aF2.5297090148925783
aF2.522768859863281
aF2.5269126892089844
aF2.520638275146484
aF2.5012667846679686
aF2.5292242431640624
aF2.5222935485839844
aF2.49267333984375
aF2.511638488769531
aF2.512978973388672
aF2.4978092956542968
aF2.4992799377441406
aF2.5044572448730467
aF2.484497375488281
aF2.5030778503417968
aF2.4889076232910154
aF2.489493560791016
aF2.500138397216797
aF2.5022816467285156
aF2.4877392578125
aF2.486410675048828
aF2.4899159240722657
aF2.483784027099609
aF2.48364990234375
aF2.4847474670410157
aF2.481070251464844
aF2.4736207580566405
aF2.4727313232421877
aF2.4542167663574217
aF2.473582458496094
aF2.4880836486816404
aF2.4754486083984375
aF2.493155517578125
aF2.4880935668945314
aF2.4933950805664065
aF2.4647030639648437
aF2.4713665771484377
aF2.474259796142578
aF2.468442687988281
aF2.4688011169433595
aF2.466985778808594
aF2.4532841491699218
aF2.4506959533691406
aF2.457154083251953
aF2.466250152587891
aF2.4685137939453123
aF2.4520162963867187
aF2.4512310791015626
aF2.453751983642578
aF2.4535928344726563
aF2.4619459533691406
aF2.4457969665527344
aF2.456606140136719
aF2.4442669677734377
aF2.4468568420410155
aF2.438883514404297
aF2.4492173767089844
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0013731512250153973
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 16s'
p10
sS'final_test_loss'
p11
F2.4492173767089844
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xeb\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.