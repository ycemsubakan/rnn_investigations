(dp0
S'train_loss'
p1
(lp2
F4.662584838867187
aF4.596763305664062
aF4.532750549316407
aF4.47026123046875
aF4.40358642578125
aF4.339372253417968
aF4.2782504272460935
aF4.2148193359375
aF4.153379821777344
aF4.094310913085938
aF4.035304260253906
aF3.980094299316406
aF3.9245864868164064
aF3.8572293090820313
aF3.808927307128906
aF3.750452880859375
aF3.696656494140625
aF3.6610708618164063
aF3.6027886962890623
aF3.569111022949219
aF3.506651611328125
aF3.461466979980469
aF3.4157000732421876
aF3.3653793334960938
aF3.3056915283203123
aF3.2938690185546875
aF3.2404550170898436
aF3.1884365844726563
aF3.167719421386719
aF3.119087829589844
aF3.0997018432617187
aF3.085817565917969
aF3.0538821411132813
aF3.006983337402344
aF3.0120620727539062
aF2.9617819213867187
aF2.95890380859375
aF2.9293682861328123
aF2.9160845947265623
aF2.8747817993164064
aF2.8872915649414064
aF2.8481280517578127
aF2.829468078613281
aF2.835529479980469
aF2.8275772094726563
aF2.8285586547851564
aF2.813251647949219
aF2.785711364746094
aF2.7821697998046875
aF2.77559326171875
aF2.7684930419921874
aF2.7579470825195314
aF2.7319796752929686
aF2.75467529296875
aF2.731120910644531
aF2.728757019042969
aF2.709309387207031
aF2.708695373535156
aF2.6914215087890625
aF2.6838107299804688
aF2.683462829589844
aF2.672785339355469
aF2.682540283203125
aF2.678070373535156
aF2.6766653442382813
aF2.65691162109375
aF2.6503680419921873
aF2.66894287109375
aF2.636090087890625
aF2.6289382934570313
aF2.6546401977539062
aF2.6537490844726563
aF2.63769287109375
aF2.6400848388671876
aF2.626383972167969
aF2.627779235839844
aF2.6062017822265626
aF2.6401983642578126
aF2.624978332519531
aF2.6041604614257814
aF2.619466552734375
aF2.6052059936523437
aF2.6250677490234375
aF2.609772033691406
aF2.595352783203125
aF2.6145965576171877
aF2.6150750732421875
aF2.586314392089844
aF2.5939205932617186
aF2.5899673461914063
aF2.6036309814453125
aF2.592923278808594
aF2.58028564453125
aF2.583724365234375
aF2.5874072265625
aF2.5594248962402344
aF2.5832308959960937
aF2.5629513549804686
aF2.5725286865234374
aF2.566872253417969
asS'test_loss'
p3
(lp4
F4.597727966308594
aF4.528348999023438
aF4.465943603515625
aF4.400509338378907
aF4.3398095703125
aF4.275086364746094
aF4.218187255859375
aF4.158507385253906
aF4.091091003417969
aF4.042743225097656
aF3.9759091186523436
aF3.914539794921875
aF3.8487506103515625
aF3.8143215942382813
aF3.7340130615234375
aF3.700497741699219
aF3.6549227905273436
aF3.598402099609375
aF3.5586322021484373
aF3.501214599609375
aF3.4478265380859376
aF3.4044369506835936
aF3.3370730590820314
aF3.306015625
aF3.2632916259765623
aF3.2391262817382813
aF3.1874435424804686
aF3.1588668823242188
aF3.126495666503906
aF3.079345397949219
aF3.072942810058594
aF3.0440139770507812
aF3.0181988525390624
aF2.9766448974609374
aF2.9697625732421873
aF2.9368609619140624
aF2.92562255859375
aF2.9161383056640626
aF2.8994436645507813
aF2.8783755493164063
aF2.8705010986328126
aF2.834913635253906
aF2.8198583984375
aF2.8293191528320314
aF2.7990066528320314
aF2.7968869018554687
aF2.773939514160156
aF2.7550949096679687
aF2.76644287109375
aF2.76189453125
aF2.7446798706054687
aF2.7471868896484377
aF2.7243280029296875
aF2.733826904296875
aF2.729617919921875
aF2.687762451171875
aF2.713553161621094
aF2.6862640380859375
aF2.6792047119140623
aF2.695897521972656
aF2.6654266357421874
aF2.703238525390625
aF2.676219177246094
aF2.6828399658203126
aF2.664598083496094
aF2.6703640747070314
aF2.656716003417969
aF2.6505670166015625
aF2.6353939819335936
aF2.628290710449219
aF2.6305072021484377
aF2.6321319580078124
aF2.6211767578125
aF2.611882629394531
aF2.63167236328125
aF2.620325012207031
aF2.615438232421875
aF2.6174945068359374
aF2.611861267089844
aF2.598216857910156
aF2.6180819702148437
aF2.605084228515625
aF2.61136474609375
aF2.6157977294921877
aF2.602248229980469
aF2.5818316650390627
aF2.5845077514648436
aF2.597942199707031
aF2.5732867431640627
aF2.6057208251953123
aF2.5889273071289063
aF2.5810247802734376
aF2.569427490234375
aF2.5816500854492186
aF2.56726806640625
aF2.5867523193359374
aF2.5588836669921875
aF2.5604263305664063
aF2.5677761840820312
aF2.5526312255859374
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005245353348648867
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 7s'
p10
sS'final_test_loss'
p11
F2.5526312255859374
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xd6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.