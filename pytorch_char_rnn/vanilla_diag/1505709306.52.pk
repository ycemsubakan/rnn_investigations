(dp0
S'train_loss'
p1
(lp2
F4.64286865234375
aF4.624931030273437
aF4.602775268554687
aF4.583006286621094
aF4.562888488769532
aF4.536873168945313
aF4.5186636352539065
aF4.502740783691406
aF4.481163024902344
aF4.458544311523437
aF4.443475341796875
aF4.42229736328125
aF4.394484252929687
aF4.379087829589844
aF4.354166564941406
aF4.335400390625
aF4.321158447265625
aF4.288256225585937
aF4.273682556152344
aF4.244581604003907
aF4.22639892578125
aF4.202223815917969
aF4.186610412597656
aF4.1614987182617185
aF4.147369995117187
aF4.125928039550781
aF4.098033752441406
aF4.081363525390625
aF4.059854125976562
aF4.040676574707032
aF4.016766052246094
aF3.986605529785156
aF3.9606582641601564
aF3.9402783203125
aF3.922588195800781
aF3.89384521484375
aF3.8771438598632812
aF3.8430084228515624
aF3.840779113769531
aF3.796099853515625
aF3.785037841796875
aF3.749375915527344
aF3.739501647949219
aF3.7021060180664063
aF3.6897213745117186
aF3.6646469116210936
aF3.6551470947265625
aF3.632588195800781
aF3.5915679931640625
aF3.596094665527344
aF3.5561181640625
aF3.5569857788085937
aF3.5155545043945313
aF3.489080810546875
aF3.4995281982421873
aF3.476133117675781
aF3.450339050292969
aF3.424866943359375
aF3.423259582519531
aF3.3728878784179686
aF3.3652838134765624
aF3.347248840332031
aF3.314296875
aF3.3299960327148437
aF3.3050747680664063
aF3.2924075317382813
aF3.2865914916992187
aF3.2574832153320314
aF3.2643890380859375
aF3.230849609375
aF3.2187548828125
aF3.1901654052734374
aF3.2124765014648435
aF3.1553781127929685
aF3.1553475952148435
aF3.1562811279296876
aF3.1593048095703127
aF3.12387939453125
aF3.1480548095703127
aF3.135300598144531
aF3.099940185546875
aF3.12169921875
aF3.1066348266601564
aF3.075950012207031
aF3.0798779296875
aF3.065057373046875
aF3.05334228515625
aF3.052650146484375
aF3.0297946166992187
aF3.0329415893554685
aF3.0136532592773437
aF2.994165344238281
aF3.0263107299804686
aF2.9913983154296875
aF2.9830972290039064
aF2.9908648681640626
aF2.9699560546875
aF2.957353210449219
aF2.9649337768554687
aF2.9823068237304686
asS'test_loss'
p3
(lp4
F4.624576721191406
aF4.5997445678710935
aF4.5838314819335935
aF4.558890380859375
aF4.540269775390625
aF4.520572204589843
aF4.501317749023437
aF4.483251647949219
aF4.455820617675781
aF4.4353421020507815
aF4.420413208007813
aF4.395441589355468
aF4.376252746582031
aF4.359085693359375
aF4.33919677734375
aF4.313871154785156
aF4.293244018554687
aF4.2742987060546875
aF4.256562194824219
aF4.224455871582031
aF4.209287719726563
aF4.187915954589844
aF4.158415832519531
aF4.138641967773437
aF4.119945678710938
aF4.097506408691406
aF4.074718933105469
aF4.045897827148438
aF4.023675842285156
aF3.9961163330078127
aF3.9877694702148436
aF3.970169677734375
aF3.928751220703125
aF3.909709777832031
aF3.8964288330078123
aF3.8770709228515625
aF3.8600927734375
aF3.8100967407226562
aF3.8009817504882815
aF3.7726791381835936
aF3.7638113403320315
aF3.7421429443359373
aF3.7183258056640627
aF3.6892385864257813
aF3.645591125488281
aF3.6358572387695314
aF3.6306948852539063
aF3.5950003051757813
aF3.5758328247070312
aF3.54746826171875
aF3.5308978271484377
aF3.522135925292969
aF3.4942633056640626
aF3.46575439453125
aF3.4654608154296875
aF3.4294863891601564
aF3.425945129394531
aF3.4062646484375
aF3.3843869018554686
aF3.3672836303710936
aF3.35248291015625
aF3.3375363159179687
aF3.318209533691406
aF3.294822998046875
aF3.2674102783203125
aF3.2788546752929686
aF3.255980224609375
aF3.2136688232421875
aF3.243209228515625
aF3.204886779785156
aF3.19473876953125
aF3.183976135253906
aF3.17360107421875
aF3.156097717285156
aF3.148068542480469
aF3.1313876342773437
aF3.1152249145507813
aF3.1148867797851563
aF3.1250347900390625
aF3.0949368286132812
aF3.07527099609375
aF3.10322021484375
aF3.0661972045898436
aF3.062185974121094
aF3.07722900390625
aF3.0113571166992186
aF3.0090121459960937
aF3.0278775024414064
aF2.997503356933594
aF3.0238613891601562
aF2.9969805908203124
aF3.01176513671875
aF3.00509765625
aF2.9664910888671874
aF2.988260498046875
aF2.973732604980469
aF2.9589114379882813
aF2.974112854003906
aF2.9380007934570314
aF2.9342938232421876
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0003621366856570712
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 52s'
p10
sS'final_test_loss'
p11
F2.9342938232421876
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'l\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.