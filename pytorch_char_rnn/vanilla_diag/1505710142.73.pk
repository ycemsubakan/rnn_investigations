(dp0
S'train_loss'
p1
(lp2
F4.616834716796875
aF3.7947146606445314
aF3.064014587402344
aF2.925415344238281
aF2.827004699707031
aF2.781211242675781
aF2.7385791015625
aF2.6820050048828126
aF2.643221740722656
aF2.6293130493164063
aF2.6081817626953123
aF2.5881402587890623
aF2.563590087890625
aF2.563695068359375
aF2.5561680603027344
aF2.529430236816406
aF2.534773712158203
aF2.519066162109375
aF2.533452606201172
aF2.5001046752929685
aF2.5009446716308594
aF2.5048231506347656
aF2.4898867797851563
aF2.4856088256835935
aF2.470597991943359
aF2.4576261901855467
aF2.474323425292969
aF2.4401820373535155
aF2.4366978454589843
aF2.4255575561523437
aF2.4165916442871094
aF2.418295440673828
aF2.419065246582031
aF2.4063990783691405
aF2.400221862792969
aF2.373529052734375
aF2.370067138671875
aF2.3749131774902343
aF2.371543426513672
aF2.3857379150390625
aF2.3666653442382812
aF2.3687672424316406
aF2.332549743652344
aF2.3639321899414063
aF2.339039611816406
aF2.3523822021484375
aF2.3205445861816405
aF2.314903564453125
aF2.3099736022949218
aF2.3056297302246094
aF2.304994812011719
aF2.2935975646972655
aF2.279107666015625
aF2.277841033935547
aF2.276622619628906
aF2.257601318359375
aF2.2603977966308593
aF2.250323944091797
aF2.260925598144531
aF2.23640869140625
aF2.2407081604003904
aF2.244503173828125
aF2.2259335327148437
aF2.213927459716797
aF2.202364654541016
aF2.220470733642578
aF2.201092071533203
aF2.1975599670410157
aF2.2008352661132813
aF2.185557556152344
aF2.180938720703125
aF2.1864772033691406
aF2.160675048828125
aF2.1654202270507814
aF2.1614642333984375
aF2.1634902954101562
aF2.1513563537597657
aF2.1485140991210936
aF2.1588679504394532
aF2.150869903564453
aF2.13083984375
aF2.1280523681640626
aF2.130748291015625
aF2.1320703125
aF2.1334989929199217
aF2.133586578369141
aF2.118175048828125
aF2.098635406494141
aF2.0976023864746094
aF2.0914552307128904
aF2.099700164794922
aF2.0690118408203126
aF2.0987060546875
aF2.1198565673828127
aF2.098843231201172
aF2.0799537658691407
aF2.0923582458496095
aF2.059551239013672
aF2.0865235900878907
aF2.0749627685546876
asS'test_loss'
p3
(lp4
F3.78779296875
aF3.0537835693359376
aF2.9166378784179687
aF2.809529113769531
aF2.7506033325195314
aF2.733414306640625
aF2.668173828125
aF2.619659729003906
aF2.6177182006835937
aF2.594016418457031
aF2.5781906127929686
aF2.575622863769531
aF2.568980712890625
aF2.5541664123535157
aF2.530611724853516
aF2.537201690673828
aF2.5315150451660156
aF2.5268284606933595
aF2.4935398864746094
aF2.4939251708984376
aF2.4699534606933593
aF2.4697874450683592
aF2.4604505920410156
aF2.482316741943359
aF2.441214294433594
aF2.4423876953125
aF2.4561611938476564
aF2.428740692138672
aF2.4292625427246093
aF2.41139404296875
aF2.4107424926757814
aF2.381100311279297
aF2.3744969177246094
aF2.383441925048828
aF2.3794683837890624
aF2.3711927795410155
aF2.364781951904297
aF2.373570861816406
aF2.3608172607421873
aF2.357076873779297
aF2.3646308898925783
aF2.3294331359863283
aF2.352286529541016
aF2.3453215026855467
aF2.3383128356933596
aF2.309261474609375
aF2.3270655822753907
aF2.2984352111816406
aF2.3060975646972657
aF2.298914031982422
aF2.2770201110839845
aF2.2643385314941407
aF2.2750975036621095
aF2.275734710693359
aF2.2595761108398436
aF2.26941650390625
aF2.2477427673339845
aF2.2459169006347657
aF2.2477783203125
aF2.2228541564941406
aF2.2325198364257814
aF2.2155801391601564
aF2.220386199951172
aF2.2005450439453127
aF2.202992248535156
aF2.204417877197266
aF2.1955938720703125
aF2.207674102783203
aF2.1951849365234377
aF2.1918617248535157
aF2.179461669921875
aF2.166900634765625
aF2.1776556396484374
aF2.166112823486328
aF2.1795355224609376
aF2.1571029663085937
aF2.1304229736328124
aF2.1370474243164064
aF2.1297491455078124
aF2.1377061462402343
aF2.140154266357422
aF2.1311355590820313
aF2.1300169372558595
aF2.1140170288085938
aF2.112359924316406
aF2.1196441650390625
aF2.1016522216796876
aF2.1056179809570312
aF2.0907221984863282
aF2.0741928100585936
aF2.096321563720703
aF2.0903665161132814
aF2.0769606018066407
aF2.058785858154297
aF2.077132263183594
aF2.0806007385253906
aF2.081728668212891
aF2.065315856933594
aF2.098797607421875
aF2.0732437133789063
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008740360504693224
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 2s'
p10
sS'final_test_loss'
p11
F2.0732437133789063
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf2\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.