(dp0
S'train_loss'
p1
(lp2
F4.616937866210938
aF4.410090942382812
aF4.215241394042969
aF4.039103088378906
aF3.8480648803710937
aF3.6902532958984375
aF3.546649169921875
aF3.39075439453125
aF3.2829498291015624
aF3.1609185791015624
aF3.0770944213867186
aF2.9789505004882812
aF2.9355517578125
aF2.86944580078125
aF2.82330322265625
aF2.799894104003906
aF2.7728237915039062
aF2.7498846435546875
aF2.7173501586914064
aF2.6742095947265625
aF2.687452697753906
aF2.6799993896484375
aF2.656006164550781
aF2.638738098144531
aF2.6074041748046874
aF2.6033370971679686
aF2.613614807128906
aF2.6200115966796873
aF2.5705316162109373
aF2.5981399536132814
aF2.5956649780273438
aF2.598994140625
aF2.5664251708984374
aF2.565022277832031
aF2.563167724609375
aF2.56476318359375
aF2.5694015502929686
aF2.547655029296875
aF2.5550607299804686
aF2.5467832946777342
aF2.5376129150390625
aF2.535155792236328
aF2.5158345031738283
aF2.5140641784667968
aF2.5014054870605467
aF2.5134156799316405
aF2.5151596069335938
aF2.5176126098632814
aF2.5094432067871093
aF2.5104930114746096
aF2.51875732421875
aF2.516753845214844
aF2.518265838623047
aF2.509842529296875
aF2.4924444580078124
aF2.5080441284179686
aF2.4779039001464844
aF2.4905166625976562
aF2.497034454345703
aF2.478036651611328
aF2.497314910888672
aF2.483709411621094
aF2.4819461059570314
aF2.480838623046875
aF2.471691436767578
aF2.475469970703125
aF2.481780548095703
aF2.4890092468261718
aF2.483345947265625
aF2.4701524353027344
aF2.464571228027344
aF2.466685791015625
aF2.4633279418945313
aF2.459951171875
aF2.4618646240234376
aF2.466176300048828
aF2.488931884765625
aF2.451945953369141
aF2.4636190795898436
aF2.471022644042969
aF2.465151672363281
aF2.4653500366210936
aF2.4663467407226562
aF2.456240234375
aF2.4691178894042967
aF2.4613636779785155
aF2.4709564208984376
aF2.4451805114746095
aF2.443776092529297
aF2.4472959899902342
aF2.442312469482422
aF2.436448059082031
aF2.444464111328125
aF2.4299665832519532
aF2.4469720458984376
aF2.42945556640625
aF2.428450927734375
aF2.4409976196289063
aF2.4256915283203124
aF2.422662658691406
asS'test_loss'
p3
(lp4
F4.414854125976563
aF4.207777709960937
aF4.026169738769531
aF3.8576190185546877
aF3.6900735473632813
aF3.5456118774414063
aF3.389093933105469
aF3.28110595703125
aF3.1596441650390625
aF3.0616546630859376
aF2.9754263305664064
aF2.9245236206054686
aF2.857602233886719
aF2.7955078125
aF2.7822308349609375
aF2.735196533203125
aF2.7455398559570314
aF2.70650634765625
aF2.702700500488281
aF2.6607635498046873
aF2.668984069824219
aF2.641956787109375
aF2.6431903076171874
aF2.625619201660156
aF2.613997802734375
aF2.599112548828125
aF2.6011090087890625
aF2.6039605712890626
aF2.5950628662109376
aF2.5855828857421876
aF2.5834503173828125
aF2.569360656738281
aF2.574715576171875
aF2.5581954956054687
aF2.5613583374023436
aF2.5451683044433593
aF2.5577537536621096
aF2.5376715087890624
aF2.5422268676757813
aF2.525694580078125
aF2.5165805053710937
aF2.5106077575683594
aF2.5110064697265626
aF2.51652099609375
aF2.508691864013672
aF2.5068804931640627
aF2.5104734802246096
aF2.520113220214844
aF2.5026596069335936
aF2.5032655334472658
aF2.5003826904296873
aF2.5030618286132813
aF2.5164779663085937
aF2.492747497558594
aF2.5135797119140624
aF2.50506103515625
aF2.477481231689453
aF2.470636901855469
aF2.4766778564453125
aF2.4753977966308596
aF2.49287841796875
aF2.4820603942871093
aF2.474714660644531
aF2.469638214111328
aF2.4732347106933594
aF2.477789001464844
aF2.4791033935546873
aF2.4750407409667967
aF2.4675634765625
aF2.4693023681640627
aF2.467981414794922
aF2.471624450683594
aF2.4645184326171874
aF2.4734967041015623
aF2.4485203552246095
aF2.464516296386719
aF2.4435160827636717
aF2.454126739501953
aF2.45583740234375
aF2.4638102722167967
aF2.4548951721191408
aF2.4466616821289064
aF2.4537892150878906
aF2.441564483642578
aF2.4331930541992186
aF2.4455610656738282
aF2.434957275390625
aF2.439788818359375
aF2.4379930114746093
aF2.4392767333984375
aF2.4482537841796876
aF2.4224000549316407
aF2.45359619140625
aF2.425894470214844
aF2.438138122558594
aF2.4292774963378907
aF2.426031188964844
aF2.4050958251953123
aF2.427820129394531
aF2.412775115966797
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0016819843476880323
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 11s'
p10
sS'final_test_loss'
p11
F2.412775115966797
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xde\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.