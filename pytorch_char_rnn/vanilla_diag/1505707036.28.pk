(dp0
S'train_loss'
p1
(lp2
F4.61835205078125
aF4.553203125
aF4.4872607421875
aF4.419735412597657
aF4.351318969726562
aF4.2734548950195315
aF4.193531799316406
aF4.110058288574219
aF4.012184753417968
aF3.9198428344726564
aF3.8290060424804686
aF3.7368734741210936
aF3.6385208129882813
aF3.541370544433594
aF3.4885345458984376
aF3.41677978515625
aF3.3514114379882813
aF3.2973532104492187
aF3.2661984252929686
aF3.204714660644531
aF3.1846139526367185
aF3.13
aF3.12271728515625
aF3.078796691894531
aF3.0715151977539064
aF3.040294189453125
aF3.038677062988281
aF3.0078366088867186
aF3.002413024902344
aF2.9716842651367186
aF2.9403582763671876
aF2.9571575927734375
aF2.921106872558594
aF2.924967041015625
aF2.8721267700195314
aF2.8651898193359373
aF2.860472412109375
aF2.8462124633789063
aF2.8349969482421873
aF2.832460632324219
aF2.7886532592773436
aF2.8126629638671874
aF2.785787353515625
aF2.7704827880859373
aF2.7764093017578126
aF2.7852774047851563
aF2.763673095703125
aF2.7392117309570314
aF2.7371755981445314
aF2.743261413574219
aF2.729527587890625
aF2.7110699462890624
aF2.704204406738281
aF2.700597229003906
aF2.697543029785156
aF2.6916375732421876
aF2.682442932128906
aF2.664886169433594
aF2.6799090576171873
aF2.6725799560546877
aF2.67593505859375
aF2.6398849487304688
aF2.6276519775390623
aF2.6603134155273436
aF2.640924377441406
aF2.6514797973632813
aF2.631180419921875
aF2.62334228515625
aF2.6307586669921874
aF2.604491882324219
aF2.6229013061523436
aF2.6405563354492188
aF2.622415771484375
aF2.6113250732421873
aF2.6030679321289063
aF2.590346374511719
aF2.6086740112304687
aF2.5808840942382814
aF2.5940176391601564
aF2.57700927734375
aF2.581764831542969
aF2.598746032714844
aF2.568525695800781
aF2.5726824951171876
aF2.5544094848632812
aF2.565616455078125
aF2.554861755371094
aF2.5735482788085937
aF2.540655059814453
aF2.5493516540527343
aF2.531760406494141
aF2.5475344848632813
aF2.548573455810547
aF2.5570147705078123
aF2.5406008911132814
aF2.5205223083496096
aF2.521181182861328
aF2.5242155456542967
aF2.5382354736328123
aF2.4997222900390623
asS'test_loss'
p3
(lp4
F4.551684265136719
aF4.486433715820312
aF4.424278564453125
aF4.346174926757812
aF4.273639526367187
aF4.188493041992188
aF4.107767028808594
aF4.012608032226563
aF3.9153811645507814
aF3.82696044921875
aF3.72912841796875
aF3.626089782714844
aF3.5441650390625
aF3.4531036376953126
aF3.40421142578125
aF3.3477056884765624
aF3.283627624511719
aF3.263335876464844
aF3.2144757080078126
aF3.1856143188476564
aF3.140535888671875
aF3.129430236816406
aF3.0998110961914063
aF3.0568063354492185
aF3.031069030761719
aF3.0383615112304687
aF3.006730651855469
aF2.9963671875
aF2.9522796630859376
aF2.9461624145507814
aF2.8859967041015624
aF2.919200744628906
aF2.9295147705078124
aF2.8929293823242186
aF2.85766845703125
aF2.8628558349609374
aF2.8292413330078126
aF2.8136309814453124
aF2.8081805419921877
aF2.806509704589844
aF2.7818524169921877
aF2.7885867309570314
aF2.7844451904296874
aF2.7774447631835937
aF2.7557257080078124
aF2.7443692016601564
aF2.755152587890625
aF2.71657470703125
aF2.7340753173828123
aF2.715234375
aF2.7402276611328125
aF2.69973876953125
aF2.7066171264648435
aF2.703394470214844
aF2.6925830078125
aF2.673739318847656
aF2.656983642578125
aF2.669924621582031
aF2.6646282958984373
aF2.6621807861328124
aF2.6635818481445312
aF2.65617919921875
aF2.6446878051757814
aF2.640244140625
aF2.6538079833984374
aF2.61228759765625
aF2.612784118652344
aF2.6287136840820313
aF2.6197390747070313
aF2.60128662109375
aF2.6098577880859377
aF2.5956271362304686
aF2.592030029296875
aF2.612041320800781
aF2.615386962890625
aF2.5903524780273437
aF2.57674072265625
aF2.5960540771484375
aF2.5790670776367186
aF2.5652197265625
aF2.5631179809570312
aF2.576601257324219
aF2.5694580078125
aF2.5662530517578124
aF2.536784973144531
aF2.538483428955078
aF2.549841003417969
aF2.5385516357421873
aF2.5330770874023436
aF2.5264265441894533
aF2.5218504333496092
aF2.523903350830078
aF2.533695068359375
aF2.5282090759277343
aF2.5089292907714844
aF2.513544616699219
aF2.511624755859375
aF2.5336595153808594
aF2.4971076965332033
aF2.514088897705078
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0016062476464148115
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 9s'
p10
sS'final_test_loss'
p11
F2.514088897705078
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'V\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.