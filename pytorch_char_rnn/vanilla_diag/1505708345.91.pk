(dp0
S'train_loss'
p1
(lp2
F4.614483337402344
aF4.565833435058594
aF4.518143615722656
aF4.467332153320313
aF4.421863403320312
aF4.370949096679688
aF4.327745971679687
aF4.27975830078125
aF4.225218811035156
aF4.178835144042969
aF4.111023864746094
aF4.064837341308594
aF4.017633972167968
aF3.9562283325195313
aF3.8999920654296876
aF3.8518707275390627
aF3.791021728515625
aF3.731228942871094
aF3.6657122802734374
aF3.608877868652344
aF3.5697772216796877
aF3.505140686035156
aF3.4371401977539064
aF3.3949539184570314
aF3.347153625488281
aF3.309027099609375
aF3.252130126953125
aF3.2100204467773437
aF3.1834365844726564
aF3.1415634155273438
aF3.1014105224609376
aF3.089560546875
aF3.0695904541015624
aF2.9913092041015625
aF2.9878506469726562
aF2.969130859375
aF2.9374935913085936
aF2.92374267578125
aF2.9239047241210936
aF2.902587890625
aF2.9126800537109374
aF2.8853643798828124
aF2.8582254028320313
aF2.86966064453125
aF2.836895751953125
aF2.8346099853515625
aF2.8225164794921875
aF2.82726806640625
aF2.7999392700195314
aF2.7887933349609373
aF2.798529052734375
aF2.7905044555664062
aF2.764775390625
aF2.7660867309570314
aF2.7582891845703124
aF2.74770751953125
aF2.76113525390625
aF2.741265869140625
aF2.722176818847656
aF2.700247497558594
aF2.7143408203125
aF2.718681640625
aF2.6965878295898436
aF2.714116516113281
aF2.694788818359375
aF2.6923709106445313
aF2.686729736328125
aF2.692279968261719
aF2.683529052734375
aF2.68808837890625
aF2.6705673217773436
aF2.657421875
aF2.6651312255859376
aF2.6468157958984375
aF2.6891909790039064
aF2.6572561645507813
aF2.6456182861328124
aF2.6700238037109374
aF2.6501739501953123
aF2.652047119140625
aF2.6361434936523436
aF2.6230157470703124
aF2.62827880859375
aF2.6396328735351564
aF2.6172885131835937
aF2.63093505859375
aF2.61545654296875
aF2.629289245605469
aF2.6140081787109377
aF2.6060601806640626
aF2.6188873291015624
aF2.6047857666015624
aF2.604847412109375
aF2.6163092041015625
aF2.5947430419921873
aF2.6062130737304687
aF2.5999267578125
aF2.5876690673828127
aF2.600896301269531
aF2.6286636352539063
asS'test_loss'
p3
(lp4
F4.5643463134765625
aF4.516421813964843
aF4.472675170898437
aF4.415555419921875
aF4.374315185546875
aF4.322097473144531
aF4.272930603027344
aF4.221344604492187
aF4.172602844238281
aF4.114535217285156
aF4.070159912109375
aF4.015873718261719
aF3.9523025512695313
aF3.895565185546875
aF3.8583209228515627
aF3.7714727783203124
aF3.7118145751953127
aF3.668559265136719
aF3.6121563720703125
aF3.5591436767578126
aF3.49369873046875
aF3.4543057250976563
aF3.395013732910156
aF3.3200918579101564
aF3.2928662109375
aF3.2530279541015625
aF3.1964373779296875
aF3.158875732421875
aF3.1093634033203124
aF3.0842181396484376
aF3.090740051269531
aF3.047362060546875
aF3.021792297363281
aF3.0149481201171877
aF2.9728518676757814
aF2.9684103393554686
aF2.9181503295898437
aF2.8907601928710935
aF2.902577209472656
aF2.8832443237304686
aF2.8699990844726564
aF2.846548767089844
aF2.8536029052734375
aF2.84898193359375
aF2.82383544921875
aF2.84238525390625
aF2.7812405395507813
aF2.791187744140625
aF2.7659857177734377
aF2.7750677490234374
aF2.7671197509765624
aF2.7643731689453124
aF2.751988830566406
aF2.7398971557617187
aF2.748951110839844
aF2.720281066894531
aF2.733441162109375
aF2.718291015625
aF2.70770751953125
aF2.7054095458984375
aF2.7289865112304685
aF2.7097183227539063
aF2.703768310546875
aF2.6920159912109374
aF2.691246337890625
aF2.682708740234375
aF2.66587890625
aF2.6707199096679686
aF2.6960012817382815
aF2.67136474609375
aF2.655810546875
aF2.6590957641601562
aF2.65599609375
aF2.6635671997070314
aF2.651524658203125
aF2.648299560546875
aF2.6461141967773436
aF2.664686279296875
aF2.6344558715820314
aF2.654139404296875
aF2.6289739990234375
aF2.6240835571289063
aF2.643018798828125
aF2.6210272216796877
aF2.6050204467773437
aF2.6166070556640624
aF2.6324368286132813
aF2.6126885986328126
aF2.607710876464844
aF2.6120950317382814
aF2.6145425415039063
aF2.6090731811523438
aF2.6264581298828125
aF2.591175537109375
aF2.6021636962890624
aF2.6089532470703123
aF2.6050515747070313
aF2.5785427856445313
aF2.591951904296875
aF2.580885925292969
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0003327204968018948
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 59s'
p10
sS'final_test_loss'
p11
F2.580885925292969
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf8\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.