(dp0
S'train_loss'
p1
(lp2
F4.6391229248046875
aF4.365882873535156
aF4.088530883789063
aF3.7847305297851563
aF3.4152215576171874
aF3.138563232421875
aF2.9799163818359373
aF2.8797366333007814
aF2.8941006469726562
aF2.840986633300781
aF2.82289306640625
aF2.793818054199219
aF2.7388619995117187
aF2.715374755859375
aF2.689786376953125
aF2.6675564575195314
aF2.665141296386719
aF2.638111267089844
aF2.610911865234375
aF2.618878173828125
aF2.586209411621094
aF2.5861111450195313
aF2.55735595703125
aF2.5530496215820313
aF2.5454449462890625
aF2.558479766845703
aF2.5266537475585937
aF2.539856719970703
aF2.5219956970214845
aF2.508170928955078
aF2.4963726806640625
aF2.4811019897460938
aF2.483185119628906
aF2.486705780029297
aF2.469657440185547
aF2.445792388916016
aF2.447787017822266
aF2.4649082946777345
aF2.426657867431641
aF2.4403285217285156
aF2.4230963134765626
aF2.421541442871094
aF2.432119903564453
aF2.4109646606445314
aF2.3936805725097656
aF2.392680206298828
aF2.3877903747558595
aF2.3797389221191407
aF2.3868768310546873
aF2.375264129638672
aF2.372427673339844
aF2.3549267578125
aF2.3511557006835937
aF2.3431317138671877
aF2.3425184631347657
aF2.3471070861816408
aF2.318309020996094
aF2.3300932312011717
aF2.3247425842285154
aF2.302648162841797
aF2.3129212951660154
aF2.320845489501953
aF2.307058868408203
aF2.286374206542969
aF2.2922972106933592
aF2.3080821228027344
aF2.2885563659667967
aF2.285850830078125
aF2.2755982971191404
aF2.265512237548828
aF2.266307525634766
aF2.25995361328125
aF2.260897521972656
aF2.2501734924316406
aF2.248128204345703
aF2.256542053222656
aF2.2516134643554686
aF2.238629913330078
aF2.252008361816406
aF2.2485063171386717
aF2.224861297607422
aF2.220420379638672
aF2.2294944763183593
aF2.210402526855469
aF2.2156785583496093
aF2.2012266540527343
aF2.2087271118164065
aF2.1994807434082033
aF2.202405548095703
aF2.223113708496094
aF2.188290710449219
aF2.1889334106445313
aF2.204456634521484
aF2.1896353149414063
aF2.1741552734375
aF2.172701416015625
aF2.184870147705078
aF2.1833340454101564
aF2.1581182861328125
aF2.18537109375
asS'test_loss'
p3
(lp4
F4.365782470703125
aF4.093635864257813
aF3.7612765502929686
aF3.418160095214844
aF3.154541015625
aF2.99192138671875
aF2.896880798339844
aF2.86422607421875
aF2.830308837890625
aF2.7973260498046875
aF2.7625637817382813
aF2.731099853515625
aF2.7022003173828124
aF2.6888092041015623
aF2.6699319458007813
aF2.660048828125
aF2.6314657592773436
aF2.623616943359375
aF2.6079025268554688
aF2.594845886230469
aF2.58348388671875
aF2.5581045532226563
aF2.5578607177734374
aF2.5527604675292968
aF2.5270039367675783
aF2.5327604675292967
aF2.5261187744140625
aF2.507674102783203
aF2.508046112060547
aF2.491365661621094
aF2.4840243530273436
aF2.496116027832031
aF2.474198303222656
aF2.4836470031738282
aF2.451453857421875
aF2.456778106689453
aF2.4409234619140623
aF2.439905242919922
aF2.4306373596191406
aF2.4169671630859373
aF2.4188668823242185
aF2.419945068359375
aF2.4099850463867187
aF2.3890557861328126
aF2.3958497619628907
aF2.370816650390625
aF2.358803405761719
aF2.367525329589844
aF2.3610858154296874
aF2.365165252685547
aF2.3558746337890626
aF2.3490940856933595
aF2.340177459716797
aF2.3363162231445314
aF2.3307919311523437
aF2.3141493225097656
aF2.3281407165527344
aF2.2999119567871094
aF2.3087367248535156
aF2.313892517089844
aF2.3020390319824218
aF2.3077520751953124
aF2.287375183105469
aF2.2787205505371095
aF2.2937748718261717
aF2.277676544189453
aF2.2715289306640627
aF2.280257263183594
aF2.268141784667969
aF2.271016845703125
aF2.271972351074219
aF2.252507629394531
aF2.2494430541992188
aF2.247548065185547
aF2.2374305725097656
aF2.2522828674316404
aF2.2450196838378904
aF2.2456816101074217
aF2.2331231689453124
aF2.22408935546875
aF2.223959197998047
aF2.2015814208984374
aF2.2053172302246096
aF2.221040802001953
aF2.2013339233398437
aF2.2172482299804686
aF2.209970703125
aF2.1886785888671874
aF2.216741180419922
aF2.182850189208984
aF2.181512451171875
aF2.1812132263183592
aF2.189910125732422
aF2.178566436767578
aF2.190201416015625
aF2.1831344604492187
aF2.171599578857422
aF2.171814422607422
aF2.1885028076171875
aF2.1641754150390624
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0069955599945530555
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 17s'
p10
sS'final_test_loss'
p11
F2.1641754150390624
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'e\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.