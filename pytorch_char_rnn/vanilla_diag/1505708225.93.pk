(dp0
S'train_loss'
p1
(lp2
F4.616571350097656
aF3.6881845092773435
aF3.1248291015625
aF2.8698239135742187
aF2.7512033081054685
aF2.716521911621094
aF2.673697814941406
aF2.650506896972656
aF2.628059997558594
aF2.59434814453125
aF2.5766036987304686
aF2.5532818603515626
aF2.5627847290039063
aF2.5336766052246094
aF2.523345184326172
aF2.533943634033203
aF2.522729797363281
aF2.520394287109375
aF2.5140794372558593
aF2.5296340942382813
aF2.515863800048828
aF2.501838836669922
aF2.4966796875
aF2.4909254455566407
aF2.484827575683594
aF2.4888214111328124
aF2.4594288635253907
aF2.465720977783203
aF2.476970977783203
aF2.452099609375
aF2.456998291015625
aF2.43998291015625
aF2.43701171875
aF2.4385452270507812
aF2.4267181396484374
aF2.431155090332031
aF2.4112693786621096
aF2.4113426208496094
aF2.3937750244140625
aF2.3849276733398437
aF2.40356689453125
aF2.3813401794433595
aF2.378575897216797
aF2.362532196044922
aF2.3797848510742186
aF2.36548583984375
aF2.3655665588378905
aF2.35087890625
aF2.3446067810058593
aF2.3552978515625
aF2.346871795654297
aF2.320665283203125
aF2.3271961975097657
aF2.3204347229003908
aF2.319613037109375
aF2.328968353271484
aF2.3098834228515623
aF2.284396209716797
aF2.298296356201172
aF2.315652160644531
aF2.293954315185547
aF2.2674981689453126
aF2.263869171142578
aF2.277155303955078
aF2.266224670410156
aF2.273741455078125
aF2.2500672912597657
aF2.2431623840332033
aF2.232477722167969
aF2.2466432189941408
aF2.238666839599609
aF2.244224853515625
aF2.230978698730469
aF2.2101069641113282
aF2.21755615234375
aF2.2034623718261717
aF2.2067674255371093
aF2.1916876220703125
aF2.191026458740234
aF2.2082244873046877
aF2.1944508361816406
aF2.1954090881347654
aF2.1528193664550783
aF2.1736419677734373
aF2.1639039611816404
aF2.150686492919922
aF2.168049163818359
aF2.158925323486328
aF2.1578155517578126
aF2.1605570983886717
aF2.140693817138672
aF2.1569024658203126
aF2.1194999694824217
aF2.1360099792480467
aF2.1400975036621093
aF2.130953063964844
aF2.125050354003906
aF2.150998840332031
aF2.128417510986328
aF2.1279637145996095
asS'test_loss'
p3
(lp4
F3.669780578613281
aF3.140366516113281
aF2.862882080078125
aF2.7726779174804688
aF2.712801818847656
aF2.6615582275390626
aF2.6244403076171876
aF2.602860107421875
aF2.567962951660156
aF2.568270568847656
aF2.5658602905273438
aF2.552823333740234
aF2.5345083618164064
aF2.504460754394531
aF2.5223074340820313
aF2.505185089111328
aF2.501439208984375
aF2.5117703247070313
aF2.5030169677734375
aF2.505745544433594
aF2.479652099609375
aF2.4762496948242188
aF2.4670469665527346
aF2.4637492370605467
aF2.473025054931641
aF2.441903076171875
aF2.4573399353027345
aF2.4604365539550783
aF2.4470150756835936
aF2.4390643310546873
aF2.426019744873047
aF2.446141357421875
aF2.421062774658203
aF2.4309764099121094
aF2.406591796875
aF2.4066490173339843
aF2.3901092529296877
aF2.388424072265625
aF2.3874276733398436
aF2.381915740966797
aF2.384157257080078
aF2.3750120544433595
aF2.3602503967285156
aF2.3691897583007813
aF2.3545721435546874
aF2.3372622680664064
aF2.3444898986816405
aF2.3381382751464845
aF2.3335220336914064
aF2.3277842712402346
aF2.3403623962402342
aF2.3249107360839845
aF2.3092344665527342
aF2.3187022399902344
aF2.296089782714844
aF2.2970599365234374
aF2.2806326293945314
aF2.284470672607422
aF2.279557189941406
aF2.2654629516601563
aF2.27412109375
aF2.2754745483398438
aF2.26426513671875
aF2.263205718994141
aF2.254419250488281
aF2.270052642822266
aF2.249575500488281
aF2.248177947998047
aF2.2283558654785156
aF2.227610778808594
aF2.227952117919922
aF2.2207122802734376
aF2.2162136840820312
aF2.2300204467773437
aF2.2127926635742186
aF2.2095440673828124
aF2.214101104736328
aF2.2073042297363283
aF2.189611053466797
aF2.1969314575195313
aF2.1714149475097657
aF2.1817726135253905
aF2.171377868652344
aF2.1522171020507814
aF2.1655094909667967
aF2.1585513305664064
aF2.1690380859375
aF2.167542724609375
aF2.139718017578125
aF2.13279296875
aF2.1386399841308594
aF2.1323837280273437
aF2.14525146484375
aF2.137975158691406
aF2.126459045410156
aF2.103635711669922
aF2.1546096801757812
aF2.129537353515625
aF2.124933776855469
aF2.1012722778320314
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.009846846032903712
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 9s'
p10
sS'final_test_loss'
p11
F2.1012722778320314
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe3\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.