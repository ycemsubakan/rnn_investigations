(dp0
S'train_loss'
p1
(lp2
F4.692116088867188
aF4.607694702148438
aF4.524891052246094
aF4.448739624023437
aF4.365156555175782
aF4.292574157714844
aF4.2144622802734375
aF4.137611694335938
aF4.04524169921875
aF3.9904351806640626
aF3.9208056640625
aF3.8294851684570315
aF3.786365661621094
aF3.709633483886719
aF3.6377130126953126
aF3.593116455078125
aF3.5173080444335936
aF3.4596112060546873
aF3.4005508422851562
aF3.3416897583007814
aF3.28841552734375
aF3.233477783203125
aF3.2168829345703127
aF3.1645477294921873
aF3.113133850097656
aF3.101029052734375
aF3.02705810546875
aF3.020670166015625
aF2.9775564575195315
aF2.939872131347656
aF2.9049530029296875
aF2.9135336303710937
aF2.8585830688476563
aF2.88231689453125
aF2.8401144409179686
aF2.838963928222656
aF2.8227084350585936
aF2.8042568969726562
aF2.777512512207031
aF2.7766748046875
aF2.760622863769531
aF2.760584716796875
aF2.7731842041015624
aF2.7296075439453125
aF2.72527099609375
aF2.7108135986328126
aF2.7302700805664064
aF2.731761474609375
aF2.7114694213867185
aF2.69714599609375
aF2.691358642578125
aF2.687705078125
aF2.6758236694335937
aF2.6602447509765623
aF2.6509246826171875
aF2.6526620483398435
aF2.6570700073242186
aF2.6390261840820313
aF2.63608154296875
aF2.6271438598632812
aF2.6332623291015627
aF2.6447955322265626
aF2.626754150390625
aF2.6103060913085936
aF2.6115399169921876
aF2.6110198974609373
aF2.6325750732421875
aF2.60222900390625
aF2.6007476806640626
aF2.612701110839844
aF2.5744970703125
aF2.5800909423828124
aF2.5877456665039062
aF2.593236999511719
aF2.5582720947265627
aF2.6018524169921875
aF2.599283447265625
aF2.5787554931640626
aF2.5714932250976563
aF2.574151611328125
aF2.5524700927734374
aF2.5633782958984375
aF2.5480657958984376
aF2.5550471496582032
aF2.5630218505859377
aF2.565570068359375
aF2.56077880859375
aF2.5638497924804686
aF2.5608160400390627
aF2.533792266845703
aF2.5567552185058595
aF2.55319580078125
aF2.5567387390136718
aF2.5332608032226562
aF2.5332249450683593
aF2.537306671142578
aF2.534212646484375
aF2.54991455078125
aF2.5474032592773437
aF2.5316229248046875
asS'test_loss'
p3
(lp4
F4.611698913574219
aF4.525502624511719
aF4.442264099121093
aF4.370689086914062
aF4.283505249023437
aF4.2171826171875
aF4.140613403320312
aF4.063096923828125
aF3.985536804199219
aF3.9222793579101562
aF3.8506292724609374
aF3.7758450317382812
aF3.7304623413085936
aF3.6409381103515623
aF3.578424377441406
aF3.512054748535156
aF3.430787353515625
aF3.4020068359375
aF3.3396707153320313
aF3.2745401000976564
aF3.229685974121094
aF3.176735534667969
aF3.136751403808594
aF3.0917254638671876
aF3.0447222900390627
aF3.0370404052734377
aF3.0101971435546875
aF2.9651373291015624
aF2.9324658203125
aF2.906593017578125
aF2.8871414184570314
aF2.874494934082031
aF2.842967224121094
aF2.8361199951171874
aF2.8085723876953126
aF2.810758361816406
aF2.8045669555664063
aF2.7763638305664062
aF2.7610910034179685
aF2.7594830322265627
aF2.746348876953125
aF2.76598876953125
aF2.7487411499023438
aF2.7310720825195314
aF2.7471054077148436
aF2.7224600219726565
aF2.721544189453125
aF2.6963922119140626
aF2.681884765625
aF2.67666259765625
aF2.661355285644531
aF2.662850036621094
aF2.668507385253906
aF2.665654602050781
aF2.657872314453125
aF2.6545361328125
aF2.6324362182617187
aF2.6351629638671876
aF2.6359088134765627
aF2.633819580078125
aF2.6287725830078124
aF2.5994921875
aF2.609468994140625
aF2.619146423339844
aF2.6107333374023436
aF2.5993157958984376
aF2.5970858764648437
aF2.5816806030273436
aF2.5686569213867188
aF2.5893316650390625
aF2.6036065673828124
aF2.5759759521484376
aF2.5599159240722655
aF2.57138427734375
aF2.5695037841796875
aF2.584153137207031
aF2.5598985290527345
aF2.5642129516601564
aF2.5636386108398437
aF2.5571656799316407
aF2.5535504150390627
aF2.557767181396484
aF2.5311846923828125
aF2.557425231933594
aF2.5548883056640626
aF2.551427764892578
aF2.560037841796875
aF2.5398451232910157
aF2.5489801025390624
aF2.5404583740234377
aF2.5491835021972657
aF2.543323669433594
aF2.54498291015625
aF2.5537924194335937
aF2.521554412841797
aF2.536923828125
aF2.5205747985839846
aF2.5324105834960937
aF2.5325794982910157
aF2.511376953125
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00067258272733367
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 8s'
p10
sS'final_test_loss'
p11
F2.511376953125
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xd2\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.