(dp0
S'train_loss'
p1
(lp2
F4.6372982788085935
aF4.628933410644532
aF4.620552978515625
aF4.609714965820313
aF4.603567199707031
aF4.592659606933593
aF4.584155578613281
aF4.574345092773438
aF4.564608154296875
aF4.55661376953125
aF4.551287231445312
aF4.533452758789062
aF4.527706298828125
aF4.516205749511719
aF4.513598022460937
aF4.502522277832031
aF4.488817138671875
aF4.4778530883789065
aF4.474180603027344
aF4.465310974121094
aF4.453147888183594
aF4.452022094726562
aF4.441336975097657
aF4.4321597290039065
aF4.4199221801757815
aF4.409521789550781
aF4.396444091796875
aF4.397980651855469
aF4.390488586425781
aF4.384666442871094
aF4.365394592285156
aF4.3584423828125
aF4.347005615234375
aF4.341502685546875
aF4.324902038574219
aF4.318003540039062
aF4.3216073608398435
aF4.3072747802734375
aF4.295588073730468
aF4.286195068359375
aF4.276019287109375
aF4.262310485839844
aF4.248704833984375
aF4.239962768554688
aF4.237307739257813
aF4.225759887695313
aF4.215955810546875
aF4.210180969238281
aF4.198878479003906
aF4.179532775878906
aF4.175686340332032
aF4.1646435546875
aF4.156682739257812
aF4.144777526855469
aF4.142848815917969
aF4.136233825683593
aF4.113084106445313
aF4.102046813964844
aF4.102781677246094
aF4.082962951660156
aF4.069147033691406
aF4.062589111328125
aF4.054827880859375
aF4.041212463378907
aF4.031337890625
aF4.02442626953125
aF4.008282470703125
aF3.997183532714844
aF3.9943621826171873
aF3.978228759765625
aF3.968154296875
aF3.96515380859375
aF3.95199951171875
aF3.924769287109375
aF3.9282217407226563
aF3.912892150878906
aF3.9037875366210937
aF3.904693908691406
aF3.8875970458984375
aF3.879516906738281
aF3.8626739501953127
aF3.864335632324219
aF3.839120178222656
aF3.830740966796875
aF3.8109951782226563
aF3.808269348144531
aF3.7994088745117187
aF3.8059429931640625
aF3.7702435302734374
aF3.7795254516601564
aF3.7476272583007812
aF3.735562438964844
aF3.7347540283203124
aF3.724299011230469
aF3.7171511840820313
aF3.694189147949219
aF3.68424560546875
aF3.67130126953125
aF3.664129333496094
aF3.673666687011719
asS'test_loss'
p3
(lp4
F4.628757629394531
aF4.6141964721679685
aF4.6093313598632815
aF4.600745544433594
aF4.590711975097657
aF4.581013793945313
aF4.574546813964844
aF4.564154968261719
aF4.5542819213867185
aF4.549414367675781
aF4.535171203613281
aF4.52473876953125
aF4.513104858398438
aF4.515608520507812
aF4.50562744140625
aF4.4921871948242185
aF4.480802001953125
aF4.475833740234375
aF4.460218505859375
aF4.455668640136719
aF4.446636047363281
aF4.439735412597656
aF4.426378784179687
aF4.423799438476562
aF4.408375244140625
aF4.399311828613281
aF4.391543884277343
aF4.384158935546875
aF4.374857177734375
aF4.358031311035156
aF4.360169982910156
aF4.343474426269531
aF4.338597412109375
aF4.320550231933594
aF4.313949890136719
aF4.309637756347656
aF4.298472290039062
aF4.296043395996094
aF4.278453674316406
aF4.276642456054687
aF4.263128967285156
aF4.254441833496093
aF4.247276000976562
aF4.233195190429687
aF4.221121826171875
aF4.224929504394531
aF4.201167297363281
aF4.189205017089844
aF4.177309265136719
aF4.178447265625
aF4.162074890136719
aF4.151941528320313
aF4.148038940429688
aF4.127828979492188
aF4.1179507446289065
aF4.1052734375
aF4.106679077148438
aF4.084429931640625
aF4.067432250976562
aF4.066982421875
aF4.061820373535157
aF4.054676208496094
aF4.0328759765625
aF4.036073913574219
aF4.0214541625976565
aF4.0063650512695315
aF3.996895446777344
aF3.9840509033203126
aF3.9828759765625
aF3.9665103149414063
aF3.956501770019531
aF3.940119323730469
aF3.926697998046875
aF3.915177001953125
aF3.910142822265625
aF3.8946380615234375
aF3.8954904174804685
aF3.8894268798828127
aF3.880421447753906
aF3.846826477050781
aF3.8501556396484373
aF3.8376235961914062
aF3.825021057128906
aF3.8048269653320315
aF3.796734619140625
aF3.7859808349609376
aF3.7901885986328123
aF3.7699658203125
aF3.761240234375
aF3.7414285278320314
aF3.7393173217773437
aF3.7342755126953127
aF3.710382080078125
aF3.718952941894531
aF3.6944854736328123
aF3.6850759887695315
aF3.669927978515625
aF3.650582580566406
aF3.672831726074219
aF3.647420654296875
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00017826726371433145
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 50s'
p10
sS'final_test_loss'
p11
F3.647420654296875
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'i\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.