(dp0
S'train_loss'
p1
(lp2
F4.6538217163085935
aF4.285076599121094
aF3.9575628662109374
aF3.6783148193359376
aF3.4498977661132812
aF3.2426666259765624
aF3.086356201171875
aF2.94095458984375
aF2.8682562255859376
aF2.8407852172851564
aF2.7964266967773437
aF2.7626934814453126
aF2.720559387207031
aF2.7043280029296874
aF2.6796533203125
aF2.6614129638671873
aF2.6589779663085937
aF2.6441275024414064
aF2.5983486938476563
aF2.638222351074219
aF2.611630554199219
aF2.5977304077148435
aF2.5586126708984374
aF2.5691842651367187
aF2.546153106689453
aF2.5541883850097657
aF2.53151123046875
aF2.5251158142089842
aF2.540160827636719
aF2.547019805908203
aF2.533819580078125
aF2.5237808227539062
aF2.503473358154297
aF2.518427429199219
aF2.5037074279785156
aF2.5077670288085936
aF2.5146823120117188
aF2.5042620849609376
aF2.505428466796875
aF2.503345947265625
aF2.4871286010742186
aF2.5043045043945313
aF2.4866551208496093
aF2.4996376037597656
aF2.498116302490234
aF2.477034912109375
aF2.480990447998047
aF2.4900340270996093
aF2.487975769042969
aF2.4807676696777343
aF2.4572552490234374
aF2.467647705078125
aF2.4747389221191405
aF2.4749993896484375
aF2.465428771972656
aF2.4816554260253905
aF2.468545379638672
aF2.47097900390625
aF2.452259368896484
aF2.4564573669433596
aF2.4542344665527343
aF2.4513534545898437
aF2.451479949951172
aF2.4502252197265624
aF2.447920227050781
aF2.442768402099609
aF2.429803771972656
aF2.43865234375
aF2.4266787719726564
aF2.439729766845703
aF2.4413633728027344
aF2.418134918212891
aF2.4226873779296874
aF2.4029362487792967
aF2.4177627563476562
aF2.420518646240234
aF2.4016908264160155
aF2.4042352294921874
aF2.410359649658203
aF2.398510284423828
aF2.400798797607422
aF2.396182098388672
aF2.395369415283203
aF2.394438934326172
aF2.4040895080566407
aF2.3834765625
aF2.3929063415527345
aF2.3834263610839845
aF2.3942425537109373
aF2.390067596435547
aF2.410713806152344
aF2.3639581298828123
aF2.3767239379882814
aF2.3886508178710937
aF2.3814817810058595
aF2.361521453857422
aF2.373262634277344
aF2.3628584289550782
aF2.3748573303222655
aF2.349056854248047
asS'test_loss'
p3
(lp4
F4.275152282714844
aF3.9538009643554686
aF3.66094482421875
aF3.414048767089844
aF3.2563702392578127
aF3.0902215576171876
aF2.9513424682617186
aF2.887617492675781
aF2.824158020019531
aF2.769428405761719
aF2.7352871704101562
aF2.6877291870117186
aF2.686872863769531
aF2.686188049316406
aF2.6661798095703126
aF2.6394989013671877
aF2.61606201171875
aF2.617276611328125
aF2.594505615234375
aF2.583699035644531
aF2.57721435546875
aF2.5532266235351564
aF2.5773760986328127
aF2.568531799316406
aF2.5497015380859374
aF2.539390411376953
aF2.528672790527344
aF2.542214813232422
aF2.523123016357422
aF2.525318450927734
aF2.5080801391601564
aF2.498382110595703
aF2.505843048095703
aF2.51796875
aF2.507142333984375
aF2.4871951293945314
aF2.4892709350585935
aF2.5105531311035154
aF2.498290252685547
aF2.491022491455078
aF2.4798841857910157
aF2.479906463623047
aF2.4869140625
aF2.4857208251953127
aF2.4675035095214843
aF2.4826040649414063
aF2.468433837890625
aF2.4754833984375
aF2.4770802307128905
aF2.4747331237792967
aF2.4561448669433594
aF2.4596173095703127
aF2.4532119750976564
aF2.459871826171875
aF2.4657252502441405
aF2.447426452636719
aF2.461064453125
aF2.4552548217773436
aF2.453060302734375
aF2.4372097778320314
aF2.4517161560058596
aF2.4401902770996093
aF2.445257110595703
aF2.4191485595703126
aF2.440604553222656
aF2.432646484375
aF2.4294395446777344
aF2.4363468933105468
aF2.4055992126464845
aF2.4377520751953123
aF2.4100567626953127
aF2.4146527099609374
aF2.4116397094726563
aF2.41887939453125
aF2.4111280822753907
aF2.4210302734375
aF2.4040457153320314
aF2.3879345703125
aF2.4110267639160154
aF2.3964547729492187
aF2.431190948486328
aF2.391336212158203
aF2.386477355957031
aF2.408057861328125
aF2.3994522094726562
aF2.3880247497558593
aF2.3923710632324218
aF2.377316741943359
aF2.400712432861328
aF2.3644790649414062
aF2.3769525146484374
aF2.361059112548828
aF2.3698739624023437
aF2.3610955810546876
aF2.3639485168457033
aF2.3606256103515624
aF2.3690695190429687
aF2.3611163330078124
aF2.353315582275391
aF2.350302429199219
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0027298503512319726
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 17s'
p10
sS'final_test_loss'
p11
F2.350302429199219
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe8\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.