(dp0
S'train_loss'
p1
(lp2
F4.597694396972656
aF4.306194458007813
aF4.011314086914062
aF3.6954464721679687
aF3.3798687744140623
aF3.147450256347656
aF2.993077697753906
aF2.902193908691406
aF2.863944091796875
aF2.8157247924804687
aF2.8127862548828126
aF2.7682571411132812
aF2.734394226074219
aF2.709798583984375
aF2.70436767578125
aF2.692052307128906
aF2.6599737548828126
aF2.6641836547851563
aF2.6218386840820314
aF2.6242581176757813
aF2.5934182739257814
aF2.6003591918945315
aF2.5770147705078124
aF2.5783966064453123
aF2.5620556640625
aF2.56387451171875
aF2.5509109497070312
aF2.5454188537597657
aF2.546966857910156
aF2.529397430419922
aF2.5335877990722655
aF2.521993408203125
aF2.5165921020507813
aF2.524222412109375
aF2.5010162353515626
aF2.503077392578125
aF2.48906005859375
aF2.50050048828125
aF2.4817153930664064
aF2.47294677734375
aF2.474111785888672
aF2.471080627441406
aF2.4650590515136717
aF2.4644464111328124
aF2.45916259765625
aF2.448396759033203
aF2.4415959167480468
aF2.4329209899902344
aF2.44027099609375
aF2.445784149169922
aF2.4301251220703124
aF2.4288087463378907
aF2.4123629760742187
aF2.4225770568847658
aF2.4124732971191407
aF2.4049087524414063
aF2.3857212829589844
aF2.4131509399414064
aF2.370655364990234
aF2.392939453125
aF2.373828887939453
aF2.377816619873047
aF2.378579559326172
aF2.375632019042969
aF2.3708035278320314
aF2.3784034729003904
aF2.3729345703125
aF2.3661671447753907
aF2.354525299072266
aF2.3487985229492185
aF2.3465200805664064
aF2.3445896911621094
aF2.339743804931641
aF2.337521209716797
aF2.3467390441894533
aF2.327680816650391
aF2.301102752685547
aF2.316053466796875
aF2.300842590332031
aF2.305045318603516
aF2.281027526855469
aF2.2976914978027345
aF2.3034295654296875
aF2.29032470703125
aF2.2854261779785157
aF2.279896697998047
aF2.279103240966797
aF2.262021789550781
aF2.2583769226074217
aF2.2674575805664063
aF2.257753448486328
aF2.2709500122070314
aF2.2452777099609373
aF2.2542344665527345
aF2.243822021484375
aF2.259485168457031
aF2.2562734985351565
aF2.234358215332031
aF2.2361271667480467
aF2.2442585754394533
asS'test_loss'
p3
(lp4
F4.300894775390625
aF4.0137460327148435
aF3.7057989501953124
aF3.3717706298828123
aF3.139970397949219
aF2.9671502685546876
aF2.921164855957031
aF2.8425192260742187
aF2.8265625
aF2.799222412109375
aF2.7440032958984375
aF2.70074462890625
aF2.706529541015625
aF2.6699853515625
aF2.6632199096679687
aF2.6692245483398436
aF2.635914001464844
aF2.6094342041015626
aF2.612762451171875
aF2.6141976928710937
aF2.576024475097656
aF2.591996154785156
aF2.571433410644531
aF2.5755886840820312
aF2.5557933044433594
aF2.5615411376953126
aF2.5207012939453124
aF2.5373272705078125
aF2.5313560485839846
aF2.5069189453125
aF2.5089154052734375
aF2.5022332763671873
aF2.512758483886719
aF2.5088958740234375
aF2.4964805603027345
aF2.485761871337891
aF2.476451416015625
aF2.492867126464844
aF2.4747422790527343
aF2.4567462158203126
aF2.469801483154297
aF2.4561097717285154
aF2.4634580993652344
aF2.435918426513672
aF2.441714324951172
aF2.4403532409667967
aF2.445500946044922
aF2.42813720703125
aF2.4321360778808594
aF2.4231216430664064
aF2.4095610046386717
aF2.4100233459472657
aF2.4097422790527343
aF2.398114776611328
aF2.404355926513672
aF2.388173370361328
aF2.376110076904297
aF2.3794508361816407
aF2.3690115356445314
aF2.393663787841797
aF2.3608441162109375
aF2.356907958984375
aF2.3696034240722654
aF2.349681701660156
aF2.356492919921875
aF2.3423527526855468
aF2.3414683532714844
aF2.353790588378906
aF2.3359910583496095
aF2.345760498046875
aF2.3287754821777344
aF2.3098298645019533
aF2.3412129211425783
aF2.309084014892578
aF2.318678131103516
aF2.3080455017089845
aF2.3174948120117187
aF2.30552978515625
aF2.287917175292969
aF2.2754591369628905
aF2.294566345214844
aF2.285603485107422
aF2.286221771240234
aF2.297729949951172
aF2.2687889099121095
aF2.2751513671875
aF2.263752136230469
aF2.2687196350097656
aF2.269664611816406
aF2.2771870422363283
aF2.2430349731445314
aF2.2486769104003907
aF2.24631103515625
aF2.252248992919922
aF2.266462554931641
aF2.2454046630859374
aF2.217460174560547
aF2.22938232421875
aF2.2343841552734376
aF2.2317134094238282
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.003581169861365736
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 39s'
p10
sS'final_test_loss'
p11
F2.2317134094238282
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xab\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.