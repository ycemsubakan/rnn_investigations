(dp0
S'train_loss'
p1
(lp2
F4.604136352539062
aF4.535049743652344
aF4.466680603027344
aF4.398731384277344
aF4.328158569335938
aF4.255151062011719
aF4.1862945556640625
aF4.108875427246094
aF4.040881652832031
aF3.9585552978515626
aF3.885267333984375
aF3.8026858520507814
aF3.7385208129882814
aF3.6671356201171874
aF3.567290954589844
aF3.4939013671875
aF3.4374365234375
aF3.354717102050781
aF3.3042572021484373
aF3.2749444580078126
aF3.1933251953125
aF3.165424499511719
aF3.0992083740234375
aF3.0707125854492188
aF3.0540542602539062
aF3.000957336425781
aF2.964100036621094
aF2.9377166748046877
aF2.906717529296875
aF2.9134585571289064
aF2.8801202392578125
aF2.845704345703125
aF2.853787841796875
aF2.8507904052734374
aF2.800728759765625
aF2.7918035888671877
aF2.8010098266601564
aF2.7880511474609375
aF2.7541241455078125
aF2.764497985839844
aF2.7660296630859373
aF2.754102783203125
aF2.74635009765625
aF2.743871765136719
aF2.705519104003906
aF2.7168963623046873
aF2.695581970214844
aF2.6976962280273438
aF2.6797369384765624
aF2.6916732788085938
aF2.6995980834960935
aF2.667781982421875
aF2.6887936401367187
aF2.642986145019531
aF2.6750692749023437
aF2.6709933471679688
aF2.6441299438476564
aF2.6181414794921873
aF2.6371383666992188
aF2.619320068359375
aF2.652626953125
aF2.6237841796875
aF2.614793395996094
aF2.634031982421875
aF2.6153125
aF2.622680969238281
aF2.60418701171875
aF2.5887664794921874
aF2.5859759521484373
aF2.6007147216796875
aF2.600830383300781
aF2.609990234375
aF2.5936203002929688
aF2.5922747802734376
aF2.5927169799804686
aF2.5893826293945312
aF2.5849905395507813
aF2.5647027587890623
aF2.580479736328125
aF2.576011962890625
aF2.5700411987304688
aF2.560516052246094
aF2.5606759643554686
aF2.5604898071289064
aF2.546153869628906
aF2.564764099121094
aF2.567662353515625
aF2.5560113525390626
aF2.5513066101074218
aF2.542130889892578
aF2.561160888671875
aF2.552986755371094
aF2.5430279541015626
aF2.545570526123047
aF2.5256607055664064
aF2.526846466064453
aF2.54921142578125
aF2.5383351135253904
aF2.532877960205078
aF2.529841003417969
asS'test_loss'
p3
(lp4
F4.534885864257813
aF4.4666070556640625
aF4.398789367675781
aF4.3247988891601565
aF4.266991882324219
aF4.188686828613282
aF4.11295654296875
aF4.033779907226562
aF3.9486090087890626
aF3.8868426513671874
aF3.810040283203125
aF3.736134338378906
aF3.6535418701171873
aF3.5651919555664064
aF3.4912893676757815
aF3.4290399169921875
aF3.375699157714844
aF3.331401672363281
aF3.2278826904296873
aF3.193795471191406
aF3.1523333740234376
aF3.102056884765625
aF3.05974609375
aF2.9993731689453127
aF2.9848361206054688
aF2.958415832519531
aF2.9519699096679686
aF2.9081439208984374
aF2.895316467285156
aF2.876181335449219
aF2.8198483276367186
aF2.8557363891601564
aF2.838878479003906
aF2.801715087890625
aF2.7820529174804687
aF2.7890164184570314
aF2.7531903076171873
aF2.748765563964844
aF2.744437255859375
aF2.7482730102539064
aF2.740223693847656
aF2.7129791259765623
aF2.7343096923828125
aF2.723086242675781
aF2.7084832763671876
aF2.6876092529296876
aF2.681825866699219
aF2.687422180175781
aF2.671774597167969
aF2.670582275390625
aF2.669753112792969
aF2.656618347167969
aF2.680422058105469
aF2.6339599609375
aF2.6738787841796876
aF2.634266662597656
aF2.65782958984375
aF2.6457644653320314
aF2.645387878417969
aF2.6304391479492186
aF2.6330081176757814
aF2.631277770996094
aF2.608634033203125
aF2.605854797363281
aF2.6163784790039064
aF2.58506103515625
aF2.5925424194335935
aF2.5894876098632813
aF2.595595703125
aF2.590387268066406
aF2.581609802246094
aF2.58011962890625
aF2.5723779296875
aF2.575956115722656
aF2.57810791015625
aF2.57990478515625
aF2.571683044433594
aF2.56673583984375
aF2.559062652587891
aF2.575101013183594
aF2.5489939880371093
aF2.5773971557617186
aF2.538060607910156
aF2.5535952758789064
aF2.5534486389160156
aF2.569462890625
aF2.5678884887695315
aF2.540657806396484
aF2.5318258666992186
aF2.5309803771972654
aF2.5414111328125
aF2.5388005065917967
aF2.5427203369140625
aF2.520760192871094
aF2.5416566467285158
aF2.5321331787109376
aF2.5098463439941407
aF2.5389895629882813
aF2.520928955078125
aF2.5215878295898437
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005261132023560857
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 57s'
p10
sS'final_test_loss'
p11
F2.5215878295898437
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe4\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.