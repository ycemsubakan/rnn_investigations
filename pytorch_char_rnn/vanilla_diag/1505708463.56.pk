(dp0
S'train_loss'
p1
(lp2
F4.673899230957031
aF4.438998107910156
aF4.2168798828125
aF4.002314453125
aF3.7756582641601564
aF3.5524111938476564
aF3.3477993774414063
aF3.2132278442382813
aF3.0834524536132815
aF2.9724459838867188
aF2.9295449829101563
aF2.882016906738281
aF2.8505068969726564
aF2.8190676879882814
aF2.8014306640625
aF2.773526306152344
aF2.741868896484375
aF2.7323208618164063
aF2.719560546875
aF2.684553527832031
aF2.6564846801757813
aF2.655858154296875
aF2.657169189453125
aF2.637151184082031
aF2.6057510375976562
aF2.6021524047851563
aF2.598235778808594
aF2.5808099365234374
aF2.583665466308594
aF2.586393127441406
aF2.5514486694335936
aF2.56906982421875
aF2.5484773254394533
aF2.541514587402344
aF2.533995819091797
aF2.522300262451172
aF2.534260559082031
aF2.515359344482422
aF2.5118646240234375
aF2.502969512939453
aF2.5120440673828126
aF2.5102362060546874
aF2.4936811828613283
aF2.4968463134765626
aF2.482124481201172
aF2.4876139831542967
aF2.486829986572266
aF2.4683262634277345
aF2.479257354736328
aF2.464529571533203
aF2.455801544189453
aF2.443179473876953
aF2.4580010986328125
aF2.459944152832031
aF2.447972869873047
aF2.4360577392578127
aF2.440363464355469
aF2.434407043457031
aF2.4338858032226565
aF2.412142791748047
aF2.4057856750488282
aF2.4159164428710938
aF2.4169154357910156
aF2.3989736938476565
aF2.3924365234375
aF2.3807249450683594
aF2.399009094238281
aF2.4075238037109377
aF2.3758311462402344
aF2.415140838623047
aF2.3690940856933596
aF2.377730712890625
aF2.3764884948730467
aF2.3725413513183593
aF2.370838317871094
aF2.3623797607421877
aF2.360012664794922
aF2.3744129943847656
aF2.3612139892578123
aF2.3568551635742185
aF2.3455239868164064
aF2.3666677856445313
aF2.3494732666015623
aF2.346217041015625
aF2.3541328430175783
aF2.3513262939453123
aF2.3362554931640624
aF2.3255683898925783
aF2.321650390625
aF2.3276585388183593
aF2.318343048095703
aF2.324809875488281
aF2.3145445251464842
aF2.317628173828125
aF2.3053587341308592
aF2.3299501037597654
aF2.328824920654297
aF2.3065200805664063
aF2.2843499755859376
aF2.3019035339355467
asS'test_loss'
p3
(lp4
F4.436418151855468
aF4.218403930664063
aF3.990938415527344
aF3.7533154296875
aF3.5394696044921874
aF3.358009948730469
aF3.167301025390625
aF3.0730010986328127
aF3.0013333129882813
aF2.90903076171875
aF2.859031066894531
aF2.836282043457031
aF2.806638488769531
aF2.7909371948242185
aF2.7545510864257814
aF2.7397894287109374
aF2.702467956542969
aF2.695317687988281
aF2.6702630615234373
aF2.6617742919921876
aF2.6490554809570312
aF2.6316937255859374
aF2.62407958984375
aF2.604350280761719
aF2.5751763916015626
aF2.605380859375
aF2.569880065917969
aF2.576000061035156
aF2.558157501220703
aF2.546669769287109
aF2.549032440185547
aF2.5359075927734374
aF2.545568084716797
aF2.5378462219238282
aF2.5188038635253904
aF2.525659484863281
aF2.520218963623047
aF2.506884765625
aF2.506460876464844
aF2.516282043457031
aF2.4856033325195312
aF2.5057221984863283
aF2.4885467529296874
aF2.481728363037109
aF2.470677947998047
aF2.4749658203125
aF2.4628781127929686
aF2.440440673828125
aF2.45571044921875
aF2.4514118957519533
aF2.460491180419922
aF2.435902404785156
aF2.436395721435547
aF2.4433413696289064
aF2.4246307373046876
aF2.4255609130859375
aF2.402901458740234
aF2.4081794738769533
aF2.427256317138672
aF2.40048095703125
aF2.4216986083984375
aF2.4110453796386717
aF2.394868927001953
aF2.3978680419921874
aF2.3954119873046875
aF2.37753662109375
aF2.3989523315429686
aF2.379253082275391
aF2.395097351074219
aF2.3945243835449217
aF2.3642823791503904
aF2.3811491394042967
aF2.3568190002441405
aF2.363189697265625
aF2.37398681640625
aF2.3753034973144533
aF2.3553543090820312
aF2.3500711059570314
aF2.3662229919433595
aF2.3632440185546875
aF2.348427734375
aF2.3372921752929687
aF2.3298416137695312
aF2.3338673400878904
aF2.330024871826172
aF2.3180393981933594
aF2.3228550720214844
aF2.331457061767578
aF2.3303791809082033
aF2.335979461669922
aF2.3161651611328127
aF2.3140093994140627
aF2.3050553894042967
aF2.315035705566406
aF2.3140025329589844
aF2.3149114990234376
aF2.2998226928710936
aF2.2965679931640626
aF2.290019989013672
aF2.2735244750976564
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008461122626262422
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 43s'
p10
sS'final_test_loss'
p11
F2.2735244750976564
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'?\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.