(dp0
S'train_loss'
p1
(lp2
F4.6704248046875
aF4.638035888671875
aF4.609855651855469
aF4.585910949707031
aF4.558664245605469
aF4.530599365234375
aF4.504428405761718
aF4.473030090332031
aF4.445249328613281
aF4.419406433105468
aF4.39479736328125
aF4.3639208984375
aF4.340928039550781
aF4.312950439453125
aF4.284036254882812
aF4.249838256835938
aF4.228971557617188
aF4.189457397460938
aF4.167570190429688
aF4.145588073730469
aF4.110853271484375
aF4.077446594238281
aF4.0522158813476565
aF4.02820556640625
aF4.0054779052734375
aF3.9776556396484377
aF3.94222412109375
aF3.915527038574219
aF3.8839599609375
aF3.8382598876953127
aF3.8229046630859376
aF3.8021759033203124
aF3.768333740234375
aF3.7406060791015623
aF3.6966192626953127
aF3.669432373046875
aF3.6438836669921875
aF3.6147052001953126
aF3.60888916015625
aF3.572168273925781
aF3.548316650390625
aF3.5154217529296874
aF3.48896728515625
aF3.473446044921875
aF3.441608581542969
aF3.4052420043945313
aF3.390382995605469
aF3.356962890625
aF3.339538269042969
aF3.3120162963867186
aF3.2831771850585936
aF3.2539608764648436
aF3.270401611328125
aF3.220179443359375
aF3.211392822265625
aF3.1936685180664064
aF3.183274230957031
aF3.1608370971679687
aF3.1341015625
aF3.107937927246094
aF3.0983285522460937
aF3.0946945190429687
aF3.0687551879882813
aF3.044180908203125
aF3.06593017578125
aF3.0239080810546874
aF3.0016384887695313
aF2.990414733886719
aF2.9869476318359376
aF2.9740866088867186
aF2.9735043334960936
aF2.9703533935546873
aF2.9396328735351562
aF2.935650634765625
aF2.918095703125
aF2.932255859375
aF2.910833740234375
aF2.9131680297851563
aF2.8718222045898436
aF2.8944525146484374
aF2.887453308105469
aF2.862581787109375
aF2.851888427734375
aF2.852628479003906
aF2.834792175292969
aF2.827952575683594
aF2.8259146118164065
aF2.8322955322265626
aF2.8154721069335937
aF2.821451416015625
aF2.8123370361328126
aF2.803414611816406
aF2.782188720703125
aF2.7862652587890624
aF2.7830010986328126
aF2.7887887573242187
aF2.7831011962890626
aF2.795796203613281
aF2.768757629394531
aF2.7766659545898436
asS'test_loss'
p3
(lp4
F4.635828247070313
aF4.611726989746094
aF4.583904113769531
aF4.559357604980469
aF4.529462280273438
aF4.501358642578125
aF4.474256896972657
aF4.444990539550782
aF4.4158984375
aF4.3932952880859375
aF4.357023315429688
aF4.338571472167969
aF4.307966613769532
aF4.278434753417969
aF4.253544311523438
aF4.224337158203125
aF4.194155578613281
aF4.160208129882813
aF4.135551147460937
aF4.1139132690429685
aF4.071946716308593
aF4.053568115234375
aF4.023972473144531
aF3.992174072265625
aF3.966755676269531
aF3.9301943969726563
aF3.8900375366210938
aF3.879768981933594
aF3.8570016479492186
aF3.8120745849609374
aF3.7971881103515623
aF3.7666259765625
aF3.714798583984375
aF3.7141232299804687
aF3.666011962890625
aF3.6502755737304686
aF3.616266174316406
aF3.5843374633789065
aF3.5713665771484373
aF3.5268267822265624
aF3.4944277954101564
aF3.4706866455078127
aF3.4645208740234374
aF3.4283676147460938
aF3.3992633056640624
aF3.3716387939453125
aF3.3508834838867188
aF3.339396667480469
aF3.3228936767578126
aF3.288246765136719
aF3.2671282958984373
aF3.245114440917969
aF3.249833679199219
aF3.199439697265625
aF3.18315185546875
aF3.179215393066406
aF3.145075378417969
aF3.1244244384765625
aF3.1100729370117186
aF3.104725646972656
aF3.0686376953125
aF3.074399719238281
aF3.0593817138671877
aF3.031651611328125
aF3.0270315551757814
aF3.01198974609375
aF2.996138000488281
aF2.995777282714844
aF2.970567626953125
aF2.9644293212890624
aF2.9370074462890625
aF2.945453796386719
aF2.904934997558594
aF2.935435485839844
aF2.9086672973632814
aF2.9070318603515624
aF2.9068731689453124
aF2.8854638671875
aF2.8682125854492186
aF2.86717529296875
aF2.8679345703125
aF2.8493533325195313
aF2.8363043212890626
aF2.8422259521484374
aF2.84319580078125
aF2.840133056640625
aF2.812555236816406
aF2.818060302734375
aF2.8115802001953125
aF2.8002734375
aF2.79554443359375
aF2.8055374145507814
aF2.7906320190429685
aF2.8001446533203125
aF2.7791583251953127
aF2.782978515625
aF2.775307922363281
aF2.7466909790039065
aF2.7662930297851562
aF2.763367919921875
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00034358795157593503
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 0s'
p10
sS'final_test_loss'
p11
F2.763367919921875
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x9d\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.