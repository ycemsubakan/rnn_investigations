(dp0
S'train_loss'
p1
(lp2
F4.63917724609375
aF4.54842041015625
aF4.451085205078125
aF4.35913818359375
aF4.2613217163085935
aF4.173611755371094
aF4.079545288085938
aF3.988603210449219
aF3.8857708740234376
aF3.7936865234375
aF3.70203125
aF3.623956604003906
aF3.5530426025390627
aF3.477256164550781
aF3.3980853271484377
aF3.3234881591796874
aF3.2622845458984373
aF3.2132296752929688
aF3.164407043457031
aF3.102733459472656
aF3.085928955078125
aF3.0312399291992187
aF3.026754150390625
aF2.9925546264648437
aF2.9524005126953123
aF2.933814697265625
aF2.90435791015625
aF2.8834292602539064
aF2.8487014770507812
aF2.8282452392578126
aF2.829038391113281
aF2.829759521484375
aF2.844606628417969
aF2.7979827880859376
aF2.7978826904296876
aF2.7933734130859373
aF2.758316650390625
aF2.781680908203125
aF2.7349807739257814
aF2.721419677734375
aF2.7223394775390624
aF2.7138522338867186
aF2.7060086059570314
aF2.6798367309570312
aF2.6968756103515625
aF2.6866363525390624
aF2.688302307128906
aF2.6836309814453125
aF2.66322509765625
aF2.6584909057617185
aF2.6432919311523437
aF2.637154541015625
aF2.6399755859375
aF2.65298095703125
aF2.6307159423828126
aF2.64783203125
aF2.618616943359375
aF2.6180072021484375
aF2.6237258911132812
aF2.626180419921875
aF2.5996212768554687
aF2.5820330810546874
aF2.5970635986328126
aF2.590276184082031
aF2.5936090087890626
aF2.5844329833984374
aF2.5931552124023436
aF2.5782736206054686
aF2.5761203002929687
aF2.5739321899414063
aF2.569686279296875
aF2.582454833984375
aF2.575722961425781
aF2.554670715332031
aF2.565474853515625
aF2.5508609008789063
aF2.549948425292969
aF2.539774169921875
aF2.560037841796875
aF2.56840087890625
aF2.5573268127441406
aF2.534937744140625
aF2.540180206298828
aF2.537945556640625
aF2.5388365173339844
aF2.537606506347656
aF2.5382957458496094
aF2.5187339782714844
aF2.5199420166015627
aF2.53031005859375
aF2.546256866455078
aF2.519087066650391
aF2.524532928466797
aF2.511904449462891
aF2.509103088378906
aF2.513115234375
aF2.508933258056641
aF2.4955929565429686
aF2.507977294921875
aF2.511850128173828
asS'test_loss'
p3
(lp4
F4.542233581542969
aF4.446163330078125
aF4.3585498046875
aF4.270205383300781
aF4.178788146972656
aF4.0800238037109375
aF3.9837606811523436
aF3.903001403808594
aF3.807259521484375
aF3.7077496337890623
aF3.609793395996094
aF3.5503887939453125
aF3.441239013671875
aF3.382013244628906
aF3.306890869140625
aF3.261929626464844
aF3.207232666015625
aF3.1571420288085936
aF3.0979244995117186
aF3.049783935546875
aF3.0298727416992186
aF3.02307861328125
aF2.956152038574219
aF2.9636419677734374
aF2.904591064453125
aF2.8995294189453125
aF2.881145324707031
aF2.8803036499023436
aF2.862463073730469
aF2.8170361328125
aF2.8261676025390625
aF2.7843447875976564
aF2.804927978515625
aF2.7844192504882814
aF2.7608322143554687
aF2.7357281494140624
aF2.7449188232421875
aF2.739354553222656
aF2.7139157104492186
aF2.7178555297851563
aF2.71936767578125
aF2.722378845214844
aF2.6852407836914063
aF2.7035232543945313
aF2.664400634765625
aF2.6583966064453124
aF2.6372265625
aF2.6490167236328124
aF2.6425775146484374
aF2.6527560424804686
aF2.6696377563476563
aF2.627904052734375
aF2.6452691650390623
aF2.625567626953125
aF2.6289263916015626
aF2.5987985229492185
aF2.6277511596679686
aF2.6242547607421876
aF2.5960198974609376
aF2.5927008056640624
aF2.6014553833007814
aF2.5833914184570315
aF2.576885070800781
aF2.5850619506835937
aF2.57905029296875
aF2.595215148925781
aF2.5737103271484374
aF2.5691683959960936
aF2.5671621704101564
aF2.578355407714844
aF2.5669403076171875
aF2.5730731201171877
aF2.554956512451172
aF2.5558351135253905
aF2.5435508728027343
aF2.550438232421875
aF2.5383091735839844
aF2.537742004394531
aF2.533091735839844
aF2.5329217529296875
aF2.535770263671875
aF2.531026153564453
aF2.5183396911621094
aF2.519783477783203
aF2.5227479553222656
aF2.528974609375
aF2.5416697692871093
aF2.522725830078125
aF2.518422088623047
aF2.515877380371094
aF2.5199931335449217
aF2.4998583984375
aF2.5084014892578126
aF2.505402374267578
aF2.4994747924804686
aF2.5130662536621093
aF2.522509002685547
aF2.494044189453125
aF2.4896372985839843
aF2.514739990234375
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0016913909384398988
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 50s'
p10
sS'final_test_loss'
p11
F2.514739990234375
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'g\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.