(dp0
S'train_loss'
p1
(lp2
F4.6621551513671875
aF4.630429382324219
aF4.602648315429687
aF4.574612731933594
aF4.545260620117188
aF4.51651611328125
aF4.4883120727539065
aF4.461140747070313
aF4.432069396972656
aF4.404804992675781
aF4.3835302734375
aF4.355133056640625
aF4.325648498535156
aF4.298262329101562
aF4.276002807617187
aF4.2403411865234375
aF4.224951171875
aF4.195299072265625
aF4.174243774414062
aF4.139595336914063
aF4.110355834960938
aF4.077708740234375
aF4.07353271484375
aF4.033069763183594
aF4.004124755859375
aF3.98510986328125
aF3.9504299926757813
aF3.9356307983398438
aF3.9145242309570314
aF3.8900723266601562
aF3.858577880859375
aF3.828887634277344
aF3.813382568359375
aF3.801763000488281
aF3.757718811035156
aF3.7245849609375
aF3.7107342529296874
aF3.688923034667969
aF3.6534979248046877
aF3.637700500488281
aF3.613280029296875
aF3.6011993408203127
aF3.579644775390625
aF3.570082092285156
aF3.522740478515625
aF3.5248382568359373
aF3.4608584594726564
aF3.474923400878906
aF3.4310406494140624
aF3.435643310546875
aF3.3885821533203124
aF3.365751037597656
aF3.3700296020507814
aF3.330767822265625
aF3.33193359375
aF3.323724670410156
aF3.2793853759765623
aF3.271045837402344
aF3.2593109130859377
aF3.2305841064453125
aF3.2350802612304688
aF3.2001727294921873
aF3.193456115722656
aF3.2019314575195312
aF3.1595248413085937
aF3.1515072631835936
aF3.142705383300781
aF3.1204074096679686
aF3.11522705078125
aF3.0808560180664064
aF3.0969342041015624
aF3.053336486816406
aF3.0656747436523437
aF3.0510693359375
aF3.0471978759765626
aF3.023557434082031
aF3.018671875
aF2.9923526000976564
aF2.9815664672851563
aF2.972950439453125
aF2.9858499145507813
aF2.960141296386719
aF2.9552578735351562
aF2.9314117431640625
aF2.9492483520507813
aF2.911163024902344
aF2.904286193847656
aF2.895767822265625
aF2.928292236328125
aF2.9230496215820314
aF2.8848272705078126
aF2.8838214111328124
aF2.856175537109375
aF2.8799227905273437
aF2.849226989746094
aF2.8506585693359376
aF2.8464825439453123
aF2.842119140625
aF2.85059326171875
aF2.838406677246094
asS'test_loss'
p3
(lp4
F4.632630004882812
aF4.599222412109375
aF4.575179138183594
aF4.546954956054687
aF4.52082275390625
aF4.48708740234375
aF4.46345458984375
aF4.435795593261719
aF4.410470886230469
aF4.378592834472657
aF4.355264587402344
aF4.329405517578125
aF4.297425842285156
aF4.267261047363281
aF4.243104248046875
aF4.216952514648438
aF4.201328735351563
aF4.171040649414063
aF4.141141052246094
aF4.115574951171875
aF4.093149719238281
aF4.0517340087890625
aF4.029770202636719
aF4.014720458984375
aF3.98312744140625
aF3.9629806518554687
aF3.9296270751953126
aF3.896778259277344
aF3.875906982421875
aF3.8581768798828127
aF3.831196594238281
aF3.814985656738281
aF3.782804260253906
aF3.756103515625
aF3.731622619628906
aF3.719712219238281
aF3.6858120727539063
aF3.6706536865234374
aF3.6232830810546877
aF3.611802978515625
aF3.590052490234375
aF3.567038879394531
aF3.5402081298828123
aF3.535352783203125
aF3.509179992675781
aF3.490362243652344
aF3.4573382568359374
aF3.4308892822265626
aF3.4089947509765626
aF3.39199951171875
aF3.3775592041015625
aF3.3547817993164064
aF3.340953063964844
aF3.3000222778320314
aF3.3138824462890626
aF3.2872378540039064
aF3.268815002441406
aF3.230945739746094
aF3.2199606323242187
aF3.207175598144531
aF3.1877294921875
aF3.18605224609375
aF3.13990234375
aF3.145960693359375
aF3.129298095703125
aF3.1405963134765624
aF3.1193533325195313
aF3.095118408203125
aF3.0883734130859377
aF3.082996826171875
aF3.050367431640625
aF3.0533416748046873
aF3.0396575927734375
aF3.022415771484375
aF3.0314947509765626
aF3.0145458984375
aF3.005967102050781
aF2.997706298828125
aF2.9637109375
aF2.940815734863281
aF2.9390859985351563
aF2.9470388793945315
aF2.921631164550781
aF2.896251220703125
aF2.911779479980469
aF2.9028936767578126
aF2.8918777465820313
aF2.9210739135742188
aF2.8978509521484375
aF2.874700927734375
aF2.878067626953125
aF2.8900689697265625
aF2.8672659301757815
aF2.8649481201171874
aF2.853829650878906
aF2.847747802734375
aF2.8503448486328127
aF2.83260009765625
aF2.8355343627929686
aF2.8172735595703124
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0001810221492977254
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 13s'
p10
sS'final_test_loss'
p11
F2.8172735595703124
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xff\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.