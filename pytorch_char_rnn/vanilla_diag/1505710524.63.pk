(dp0
S'train_loss'
p1
(lp2
F4.610792846679687
aF4.561251220703125
aF4.5136502075195315
aF4.46473876953125
aF4.415361022949218
aF4.368555908203125
aF4.317729187011719
aF4.264906311035157
aF4.2138845825195315
aF4.155828247070312
aF4.114029541015625
aF4.0570156860351565
aF3.9921194458007814
aF3.9353396606445314
aF3.868018798828125
aF3.8012957763671875
aF3.7368435668945312
aF3.6757098388671876
aF3.620943603515625
aF3.5679766845703127
aF3.47716064453125
aF3.4271734619140624
aF3.4022598266601562
aF3.328382568359375
aF3.2797119140625
aF3.2141842651367187
aF3.193960876464844
aF3.1592169189453125
aF3.134949951171875
aF3.1083734130859373
aF3.071962585449219
aF3.0449237060546874
aF3.006671142578125
aF2.995269470214844
aF2.965226745605469
aF2.9288543701171874
aF2.9305517578125
aF2.943153991699219
aF2.887989807128906
aF2.89486083984375
aF2.8789877319335937
aF2.8767547607421875
aF2.8145977783203127
aF2.822309875488281
aF2.8218508911132814
aF2.834772033691406
aF2.785639343261719
aF2.7777725219726563
aF2.767075500488281
aF2.762503662109375
aF2.7749188232421873
aF2.7874679565429688
aF2.748668212890625
aF2.7406103515625
aF2.7337680053710938
aF2.7304812622070314
aF2.723529968261719
aF2.714815673828125
aF2.717377014160156
aF2.7258370971679686
aF2.7190347290039063
aF2.6780239868164064
aF2.6976248168945314
aF2.6889764404296876
aF2.6905462646484377
aF2.676652526855469
aF2.677596130371094
aF2.673978271484375
aF2.65734619140625
aF2.6649359130859374
aF2.6712664794921874
aF2.6657958984375
aF2.639443359375
aF2.6368829345703126
aF2.6529879760742188
aF2.633935546875
aF2.6391278076171876
aF2.6352963256835937
aF2.6386148071289064
aF2.6464138793945313
aF2.6115841674804687
aF2.596732482910156
aF2.6179928588867187
aF2.623637390136719
aF2.605629577636719
aF2.6326254272460936
aF2.5905032348632813
aF2.601062927246094
aF2.5995269775390626
aF2.6096261596679686
aF2.59455810546875
aF2.597587890625
aF2.582769470214844
aF2.5917425537109375
aF2.571996765136719
aF2.585168762207031
aF2.5887271118164064
aF2.5711044311523437
aF2.579232482910156
aF2.5704266357421877
asS'test_loss'
p3
(lp4
F4.561764221191407
aF4.511984252929688
aF4.463568420410156
aF4.416847839355468
aF4.366050109863282
aF4.318340454101563
aF4.269168701171875
aF4.217109680175781
aF4.1628875732421875
aF4.108294067382812
aF4.033868408203125
aF3.9861376953125
aF3.9199383544921873
aF3.860516357421875
aF3.8071337890625
aF3.7265570068359377
aF3.6594747924804687
aF3.6052365112304687
aF3.5309271240234374
aF3.479222412109375
aF3.4179974365234376
aF3.3795550537109373
aF3.3306109619140627
aF3.2707122802734374
aF3.20453857421875
aF3.1806686401367186
aF3.143057861328125
aF3.1206573486328124
aF3.0623138427734373
aF3.0547140502929686
aF3.02833251953125
aF2.999978332519531
aF2.984346618652344
aF2.9422003173828126
aF2.9192230224609377
aF2.92877197265625
aF2.9051968383789064
aF2.8845919799804687
aF2.8594000244140627
aF2.8632879638671875
aF2.847886962890625
aF2.8297628784179687
aF2.85098388671875
aF2.823827819824219
aF2.7931063842773436
aF2.7938720703125
aF2.7901687622070312
aF2.7766815185546876
aF2.741046142578125
aF2.7402081298828125
aF2.7609896850585938
aF2.7532223510742186
aF2.727523193359375
aF2.7393618774414064
aF2.723367004394531
aF2.73394775390625
aF2.7093966674804686
aF2.689020080566406
aF2.7063714599609376
aF2.7123468017578123
aF2.7062786865234374
aF2.663942565917969
aF2.691381530761719
aF2.679920349121094
aF2.6751248168945314
aF2.657434387207031
aF2.6595440673828126
aF2.672095947265625
aF2.64151123046875
aF2.6525848388671873
aF2.6408148193359375
aF2.6494992065429686
aF2.62955322265625
aF2.644033203125
aF2.6292071533203125
aF2.6284335327148436
aF2.618275451660156
aF2.6379458618164064
aF2.629388427734375
aF2.6204571533203125
aF2.6134930419921876
aF2.607854309082031
aF2.598089294433594
aF2.610837097167969
aF2.604560546875
aF2.584231262207031
aF2.5959573364257813
aF2.590013122558594
aF2.594676513671875
aF2.5728204345703123
aF2.5690399169921876
aF2.588173828125
aF2.590143737792969
aF2.5604727172851565
aF2.580489501953125
aF2.5542872619628905
aF2.5895843505859375
aF2.57739013671875
aF2.5662380981445314
aF2.576580810546875
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.000526825280054156
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 38s'
p10
sS'final_test_loss'
p11
F2.576580810546875
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb4\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.