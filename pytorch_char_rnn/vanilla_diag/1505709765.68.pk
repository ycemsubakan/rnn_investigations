(dp0
S'train_loss'
p1
(lp2
F4.617506713867187
aF4.465767211914063
aF4.314524536132812
aF4.175123596191407
aF4.053954162597656
aF3.925924072265625
aF3.7875772094726563
aF3.6703335571289064
aF3.5503878784179688
aF3.4365875244140627
aF3.352911376953125
aF3.2735986328125
aF3.1988385009765623
aF3.1251019287109374
aF3.0429168701171876
aF3.0344049072265626
aF2.9680722045898436
aF2.925337829589844
aF2.8873553466796875
aF2.856592102050781
aF2.8379852294921877
aF2.7963189697265625
aF2.7536907958984376
aF2.736328430175781
aF2.7260968017578127
aF2.7217938232421877
aF2.6992092895507813
aF2.6777374267578127
aF2.6814093017578124
aF2.6684869384765624
aF2.672510681152344
aF2.65060791015625
aF2.6342279052734376
aF2.6387930297851563
aF2.64033935546875
aF2.6196859741210936
aF2.6323004150390625
aF2.604602966308594
aF2.6124603271484377
aF2.6023788452148438
aF2.5959487915039063
aF2.5916131591796874
aF2.577131652832031
aF2.5662411499023436
aF2.5831658935546873
aF2.5623233032226564
aF2.5469798278808593
aF2.556802520751953
aF2.548994598388672
aF2.5484779357910154
aF2.550403594970703
aF2.5506309509277343
aF2.5321641540527344
aF2.5391070556640627
aF2.5331517028808594
aF2.5374104309082033
aF2.5328045654296876
aF2.5247019958496093
aF2.524550476074219
aF2.5362063598632814
aF2.51790771484375
aF2.5155755615234376
aF2.5090867614746095
aF2.5041302490234374
aF2.503861083984375
aF2.498746337890625
aF2.509498748779297
aF2.5140422058105467
aF2.5022352600097655
aF2.5105854797363283
aF2.4992723083496093
aF2.523946075439453
aF2.505529022216797
aF2.4942999267578125
aF2.5022605895996093
aF2.477513427734375
aF2.4942454528808593
aF2.487002105712891
aF2.4938897705078125
aF2.4958892822265626
aF2.496759338378906
aF2.4847976684570314
aF2.4838882446289063
aF2.4856190490722656
aF2.471585235595703
aF2.487121124267578
aF2.480815124511719
aF2.485434112548828
aF2.4856532287597655
aF2.497076110839844
aF2.48728515625
aF2.4859429931640626
aF2.4804296875
aF2.4723529052734374
aF2.4719247436523437
aF2.488433074951172
aF2.467406463623047
aF2.4633059692382813
aF2.461273498535156
aF2.4640374755859376
asS'test_loss'
p3
(lp4
F4.464801940917969
aF4.314150390625
aF4.174393310546875
aF4.0281069946289065
aF3.9057318115234376
aF3.7800802612304687
aF3.651756591796875
aF3.5549884033203125
aF3.4586285400390624
aF3.3568804931640623
aF3.267481384277344
aF3.185086364746094
aF3.1258358764648437
aF3.074160461425781
aF3.0377651977539064
aF2.9507040405273437
aF2.9405972290039064
aF2.8637811279296876
aF2.8594149780273437
aF2.8277972412109373
aF2.8172113037109376
aF2.7578839111328124
aF2.731297912597656
aF2.71228271484375
aF2.7104837036132814
aF2.721929016113281
aF2.683714599609375
aF2.689480895996094
aF2.63547607421875
aF2.6395465087890626
aF2.6396612548828124
aF2.6156646728515627
aF2.620736083984375
aF2.617862548828125
aF2.6172500610351563
aF2.6069210815429686
aF2.6273284912109376
aF2.5886849975585937
aF2.584192199707031
aF2.5937533569335938
aF2.5690093994140626
aF2.5663461303710937
aF2.5685198974609373
aF2.5656341552734374
aF2.5842715454101564
aF2.5419696044921873
aF2.5557916259765623
aF2.526738586425781
aF2.5323980712890624
aF2.5511540222167968
aF2.5351414489746094
aF2.52281494140625
aF2.5445448303222657
aF2.524582824707031
aF2.520322570800781
aF2.510278625488281
aF2.4961663818359376
aF2.51437255859375
aF2.5092790222167967
aF2.528097686767578
aF2.517789611816406
aF2.5078578186035156
aF2.5145246887207033
aF2.500088653564453
aF2.507103271484375
aF2.4781480407714844
aF2.484181213378906
aF2.5019679260253906
aF2.5019131469726563
aF2.498623962402344
aF2.506300811767578
aF2.4981817626953124
aF2.490407257080078
aF2.4892611694335938
aF2.4943363952636717
aF2.490448455810547
aF2.4890286254882814
aF2.494858703613281
aF2.469907531738281
aF2.485782470703125
aF2.493938140869141
aF2.482657012939453
aF2.482030029296875
aF2.4793855285644533
aF2.4785232543945312
aF2.4681898498535157
aF2.476661834716797
aF2.470846710205078
aF2.4802044677734374
aF2.473897705078125
aF2.46888916015625
aF2.4617822265625
aF2.463714294433594
aF2.4627684020996092
aF2.4622317504882814
aF2.4676031494140624
aF2.4634901428222657
aF2.464479675292969
aF2.449859161376953
aF2.456329803466797
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0011537066668006303
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 10s'
p10
sS'final_test_loss'
p11
F2.456329803466797
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe3\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.