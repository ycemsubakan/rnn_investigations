(dp0
S'train_loss'
p1
(lp2
F4.5792608642578125
aF4.568669738769532
aF4.554065246582031
aF4.540547485351563
aF4.528080139160156
aF4.513637084960937
aF4.504624328613281
aF4.490201721191406
aF4.478276062011719
aF4.459440002441406
aF4.449217224121094
aF4.437929382324219
aF4.423252868652344
aF4.413650817871094
aF4.396329345703125
aF4.381747741699218
aF4.371449279785156
aF4.353129577636719
aF4.346234741210938
aF4.330580139160157
aF4.3193838500976565
aF4.297666931152344
aF4.286524353027343
aF4.2697686767578125
aF4.2563040161132815
aF4.23292724609375
aF4.219906311035157
aF4.207031860351562
aF4.19114990234375
aF4.180730895996094
aF4.165956726074219
aF4.140895080566406
aF4.1219720458984375
aF4.113497314453125
aF4.0902880859375
aF4.06826171875
aF4.049035339355469
aF4.032967224121093
aF4.015556030273437
aF3.9997149658203126
aF3.9765283203125
aF3.9498846435546877
aF3.932601318359375
aF3.917744445800781
aF3.9029736328125
aF3.8729193115234377
aF3.867398376464844
aF3.8403494262695315
aF3.80822021484375
aF3.7886996459960938
aF3.7673291015625
aF3.7337982177734377
aF3.729298095703125
aF3.7092376708984376
aF3.6976107788085937
aF3.6602093505859377
aF3.64996826171875
aF3.6272467041015624
aF3.5890478515625
aF3.5992742919921876
aF3.5493682861328124
aF3.5351138305664063
aF3.5458560180664063
aF3.486733093261719
aF3.48419677734375
aF3.478228759765625
aF3.4599560546875
aF3.4302603149414064
aF3.4182608032226565
aF3.3932293701171874
aF3.3810958862304688
aF3.3694488525390627
aF3.370162353515625
aF3.334232177734375
aF3.3240582275390627
aF3.3237045288085936
aF3.2854766845703125
aF3.2751043701171874
aF3.26146240234375
aF3.24379150390625
aF3.2393212890625
aF3.2159469604492186
aF3.2185897827148438
aF3.202917175292969
aF3.1771112060546876
aF3.19470458984375
aF3.1636297607421877
aF3.1734857177734375
aF3.1469024658203124
aF3.1311517333984376
aF3.144861755371094
aF3.1214999389648437
aF3.132210388183594
aF3.0960708618164063
aF3.1172329711914064
aF3.0960479736328126
aF3.0885589599609373
aF3.092181396484375
aF3.0675204467773436
aF3.0735040283203126
asS'test_loss'
p3
(lp4
F4.565905151367187
aF4.553465576171875
aF4.538290405273438
aF4.5271694946289065
aF4.5130746459960935
aF4.5031396484375
aF4.4877996826171875
aF4.474312438964843
aF4.464515991210938
aF4.452821044921875
aF4.436879272460938
aF4.4230477905273435
aF4.40986083984375
aF4.400648803710937
aF4.381835327148438
aF4.370238342285156
aF4.362964782714844
aF4.3405368041992185
aF4.328080749511718
aF4.3084130859375
aF4.298030090332031
aF4.285820007324219
aF4.2718984985351565
aF4.253224487304688
aF4.237684326171875
aF4.223255920410156
aF4.199330444335938
aF4.18908447265625
aF4.173948669433594
aF4.1451025390625
aF4.138367919921875
aF4.125562744140625
aF4.111324768066407
aF4.076914367675781
aF4.068038330078125
aF4.041257019042969
aF4.0276272583007815
aF4.0048370361328125
aF3.991799621582031
aF3.963070983886719
aF3.9498391723632813
aF3.928795166015625
aF3.9056591796875
aF3.8814892578125
aF3.8585028076171874
aF3.8461874389648436
aF3.835945739746094
aF3.813648681640625
aF3.7780227661132812
aF3.7573483276367186
aF3.741778564453125
aF3.7212744140625
aF3.7033761596679686
aF3.67968994140625
aF3.65935302734375
aF3.6360159301757813
aF3.6164785766601564
aF3.5920986938476562
aF3.576357421875
aF3.5609259033203124
aF3.533299560546875
aF3.5292794799804685
aF3.513250732421875
aF3.481120300292969
aF3.4726702880859377
aF3.459211730957031
aF3.432906494140625
aF3.411379699707031
aF3.399725341796875
aF3.3908685302734374
aF3.358101501464844
aF3.364010925292969
aF3.331876220703125
aF3.313824462890625
aF3.2859283447265626
aF3.2910366821289063
aF3.2689794921875
aF3.249158935546875
aF3.252955017089844
aF3.2402877807617188
aF3.210687255859375
aF3.216546936035156
aF3.1771505737304686
aF3.1935369873046877
aF3.1532232666015627
aF3.187384033203125
aF3.160926513671875
aF3.135892639160156
aF3.1417108154296876
aF3.11455322265625
aF3.0991305541992187
aF3.1044296264648437
aF3.128437805175781
aF3.0982049560546874
aF3.0803182983398436
aF3.0740017700195312
aF3.0617169189453124
aF3.066195373535156
aF3.068800964355469
aF3.042987060546875
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0001816556280079609
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 25s'
p10
sS'final_test_loss'
p11
F3.042987060546875
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x90\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.