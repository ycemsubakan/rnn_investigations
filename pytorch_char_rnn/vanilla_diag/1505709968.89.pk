(dp0
S'train_loss'
p1
(lp2
F4.636349487304687
aF4.170487365722656
aF3.5720794677734373
aF3.1197268676757814
aF2.9319137573242187
aF2.8933599853515624
aF2.876358642578125
aF2.841843566894531
aF2.7639785766601563
aF2.754871826171875
aF2.707092590332031
aF2.7069677734375
aF2.6900149536132814
aF2.634463195800781
aF2.6076809692382814
aF2.5938351440429686
aF2.6015866088867186
aF2.5818447875976562
aF2.5655691528320315
aF2.5496615600585937
aF2.536971130371094
aF2.522195129394531
aF2.5242340087890627
aF2.5022349548339844
aF2.5099815368652343
aF2.4763389587402345
aF2.482816467285156
aF2.473000793457031
aF2.4665371704101564
aF2.4515582275390626
aF2.4579533386230468
aF2.445699462890625
aF2.4268943786621096
aF2.424363861083984
aF2.401552276611328
aF2.4076768493652345
aF2.4126431274414064
aF2.376545867919922
aF2.3700135803222655
aF2.379131622314453
aF2.365631561279297
aF2.364967041015625
aF2.3543106079101563
aF2.344349822998047
aF2.339399719238281
aF2.340484619140625
aF2.3427912902832033
aF2.305622863769531
aF2.300575714111328
aF2.293412628173828
aF2.298164520263672
aF2.298584747314453
aF2.3030384826660155
aF2.2669973754882813
aF2.2735630798339845
aF2.2758021545410156
aF2.2800958251953123
aF2.268571624755859
aF2.270718994140625
aF2.265899200439453
aF2.2572068786621093
aF2.2403858947753905
aF2.2426799011230467
aF2.232129669189453
aF2.209433898925781
aF2.2384564208984377
aF2.2138385009765624
aF2.22533935546875
aF2.2284783935546875
aF2.205406494140625
aF2.208359375
aF2.1839024353027345
aF2.191900329589844
aF2.2035760498046875
aF2.1681475830078125
aF2.1696112060546877
aF2.178835754394531
aF2.1620433044433596
aF2.1831350708007813
aF2.1737921142578127
aF2.1771951293945313
aF2.1562924194335937
aF2.1623126220703126
aF2.1615728759765624
aF2.131026611328125
aF2.1564027404785158
aF2.140911102294922
aF2.134233093261719
aF2.118627624511719
aF2.131772918701172
aF2.1320896911621094
aF2.124827575683594
aF2.1166836547851564
aF2.133531341552734
aF2.1217269897460938
aF2.091496887207031
aF2.1187429809570313
aF2.104531555175781
aF2.099897766113281
aF2.0900199890136717
asS'test_loss'
p3
(lp4
F4.1651068115234375
aF3.5870623779296875
aF3.0970114135742186
aF2.9443624877929686
aF2.9099044799804688
aF2.823830261230469
aF2.765635986328125
aF2.754775390625
aF2.7305538940429686
aF2.712611083984375
aF2.6740872192382814
aF2.6406951904296876
aF2.6129824829101564
aF2.6264056396484374
aF2.5796807861328124
aF2.5623211669921875
aF2.573178405761719
aF2.5711709594726564
aF2.5203515625
aF2.528234100341797
aF2.511919403076172
aF2.508435821533203
aF2.498865203857422
aF2.4901185607910157
aF2.4796014404296876
aF2.4853570556640623
aF2.461777801513672
aF2.4617359924316404
aF2.4504740905761717
aF2.463223419189453
aF2.4396380615234374
aF2.4076852416992187
aF2.411976165771484
aF2.4050309753417967
aF2.395157928466797
aF2.395940856933594
aF2.387445983886719
aF2.378371276855469
aF2.3615687561035155
aF2.3635023498535155
aF2.3600535583496094
aF2.3558961486816408
aF2.3603590393066405
aF2.318808898925781
aF2.3275584411621093
aF2.329934844970703
aF2.2981866455078124
aF2.3041864013671876
aF2.270829315185547
aF2.302287292480469
aF2.2875765991210937
aF2.2863313293457033
aF2.2810304260253904
aF2.2752618408203125
aF2.2782720947265624
aF2.271741485595703
aF2.2511920166015624
aF2.2669906616210938
aF2.244893341064453
aF2.2385270690917967
aF2.2291799926757814
aF2.24132568359375
aF2.2331468200683595
aF2.2403939819335936
aF2.225155792236328
aF2.2223968505859375
aF2.212696838378906
aF2.218518829345703
aF2.190667724609375
aF2.195986328125
aF2.177068176269531
aF2.1945155334472655
aF2.1715184020996094
aF2.187438201904297
aF2.181956787109375
aF2.178721160888672
aF2.196529541015625
aF2.184776153564453
aF2.1698193359375
aF2.171204833984375
aF2.1730996704101564
aF2.1294085693359377
aF2.1556245422363283
aF2.1432861328125
aF2.1246882629394532
aF2.116535339355469
aF2.1395248413085937
aF2.121438293457031
aF2.1263389587402344
aF2.130868682861328
aF2.111194915771484
aF2.1038471984863283
aF2.1143495178222658
aF2.1175172424316404
aF2.1059765625
aF2.098710784912109
aF2.109316711425781
aF2.0960684204101563
aF2.103762359619141
aF2.096261291503906
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008728979363318118
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 26s'
p10
sS'final_test_loss'
p11
F2.096261291503906
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x88\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.