(dp0
S'train_loss'
p1
(lp2
F4.679920349121094
aF4.619201049804688
aF4.554616394042969
aF4.495689697265625
aF4.433572692871094
aF4.367683715820313
aF4.314273681640625
aF4.2472900390625
aF4.185395202636719
aF4.126820068359375
aF4.069368286132812
aF4.013609008789063
aF3.955845947265625
aF3.901431884765625
aF3.8385995483398436
aF3.7682232666015625
aF3.7290216064453126
aF3.670625
aF3.613553771972656
aF3.5696478271484375
aF3.5003411865234373
aF3.463868408203125
aF3.3994281005859377
aF3.344016418457031
aF3.334233703613281
aF3.2666424560546874
aF3.2470477294921873
aF3.206639404296875
aF3.163832702636719
aF3.1218603515625
aF3.099304504394531
aF3.0585003662109376
aF3.0373501586914062
aF2.9926531982421873
aF2.9771038818359377
aF2.9444598388671874
aF2.9487139892578127
aF2.9124371337890627
aF2.9069671630859375
aF2.898437194824219
aF2.8763906860351565
aF2.8925595092773437
aF2.8170095825195314
aF2.847928466796875
aF2.8260287475585937
aF2.8138885498046875
aF2.790294494628906
aF2.753733825683594
aF2.767760925292969
aF2.755526428222656
aF2.7690335083007813
aF2.7381417846679685
aF2.774208679199219
aF2.7357415771484375
aF2.7245794677734376
aF2.7221990966796876
aF2.7132464599609376
aF2.7198248291015625
aF2.7100234985351563
aF2.68280517578125
aF2.654930419921875
aF2.7046868896484373
aF2.686470642089844
aF2.6777572631835938
aF2.6774737548828127
aF2.670968017578125
aF2.6562899780273437
aF2.669483337402344
aF2.6515994262695313
aF2.641556396484375
aF2.6573419189453125
aF2.6524667358398437
aF2.6326348876953123
aF2.609837951660156
aF2.644517517089844
aF2.6151553344726564
aF2.615053405761719
aF2.5887026977539063
aF2.6118399047851564
aF2.62030029296875
aF2.589124755859375
aF2.6042489624023437
aF2.6042166137695313
aF2.584778137207031
aF2.588334655761719
aF2.595583801269531
aF2.59912109375
aF2.5854367065429686
aF2.606656188964844
aF2.564041748046875
aF2.5716925048828125
aF2.590367736816406
aF2.5915325927734374
aF2.5565052795410157
aF2.5765615844726564
aF2.5576231384277346
aF2.5823193359375
aF2.5708807373046874
aF2.5603216552734374
aF2.562850341796875
asS'test_loss'
p3
(lp4
F4.621033935546875
aF4.557013244628906
aF4.495098876953125
aF4.430614624023438
aF4.373594970703125
aF4.310498657226563
aF4.249297180175781
aF4.192479858398437
aF4.130169982910156
aF4.070808715820313
aF4.020274047851562
aF3.957677917480469
aF3.8907904052734374
aF3.835079345703125
aF3.7745797729492185
aF3.717043151855469
aF3.6712445068359374
aF3.613866882324219
aF3.558052978515625
aF3.5013333129882813
aF3.4573068237304687
aF3.3944912719726563
aF3.3613516235351564
aF3.316448059082031
aF3.271556091308594
aF3.2236920166015626
aF3.2039639282226564
aF3.1440704345703123
aF3.1269631958007813
aF3.060331115722656
aF3.053910217285156
aF3.0329299926757813
aF3.0096337890625
aF2.976181640625
aF2.959796142578125
aF2.9390139770507813
aF2.89519287109375
aF2.9001171875
aF2.9033139038085936
aF2.860162658691406
aF2.83639404296875
aF2.8442007446289064
aF2.8339276123046875
aF2.8134027099609376
aF2.801960144042969
aF2.7749478149414064
aF2.764332275390625
aF2.7723553466796873
aF2.7395648193359374
aF2.735617980957031
aF2.7419146728515624
aF2.715406494140625
aF2.730843505859375
aF2.7353280639648436
aF2.703721618652344
aF2.7108944702148436
aF2.689716491699219
aF2.6928610229492187
aF2.681077880859375
aF2.6730029296875
aF2.675062255859375
aF2.6514132690429686
aF2.661076965332031
aF2.667958068847656
aF2.6472039794921876
aF2.6503396606445313
aF2.6515707397460937
aF2.6488641357421874
aF2.6352304077148436
aF2.6396994018554687
aF2.621842346191406
aF2.6306271362304687
aF2.6158737182617187
aF2.6246539306640626
aF2.623885498046875
aF2.6160739135742186
aF2.626038818359375
aF2.610077209472656
aF2.612281494140625
aF2.607138977050781
aF2.6019171142578124
aF2.5755718994140624
aF2.590433349609375
aF2.58436767578125
aF2.5947393798828124
aF2.583955078125
aF2.5806048583984373
aF2.577953186035156
aF2.573128967285156
aF2.5803952026367187
aF2.577181091308594
aF2.550264587402344
aF2.561869812011719
aF2.5922943115234376
aF2.5577638244628904
aF2.5595928955078127
aF2.5721734619140624
aF2.5800857543945312
aF2.5462957763671876
aF2.5472032165527345
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0006294689715275107
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 2s'
p10
sS'final_test_loss'
p11
F2.5472032165527345
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb8\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.