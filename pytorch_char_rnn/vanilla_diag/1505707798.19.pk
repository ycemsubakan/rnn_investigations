(dp0
S'train_loss'
p1
(lp2
F4.638208618164063
aF4.123342895507813
aF3.6108795166015626
aF3.2124807739257815
aF2.9551126098632814
aF2.8415966796875
aF2.7968218994140623
aF2.7839764404296874
aF2.7334515380859377
aF2.716436462402344
aF2.707541198730469
aF2.6790652465820313
aF2.6403823852539063
aF2.6125372314453124
aF2.5915963745117185
aF2.6008291625976563
aF2.58920166015625
aF2.537669677734375
aF2.5468621826171876
aF2.5460675048828123
aF2.5476310729980467
aF2.5514767456054686
aF2.5320863342285156
aF2.5192910766601564
aF2.508362579345703
aF2.504613037109375
aF2.503204650878906
aF2.48606689453125
aF2.4753733825683595
aF2.500232391357422
aF2.490727844238281
aF2.488783111572266
aF2.4882768249511718
aF2.4752142333984377
aF2.462999267578125
aF2.4580062866210937
aF2.463520355224609
aF2.4401690673828127
aF2.4521110534667967
aF2.4345442199707032
aF2.4280491638183594
aF2.4278805541992186
aF2.4146299743652344
aF2.4116639709472656
aF2.391650848388672
aF2.4066455078125
aF2.399547271728516
aF2.3923130798339844
aF2.374866485595703
aF2.3990664672851563
aF2.3698805236816405
aF2.379793853759766
aF2.368515930175781
aF2.377999267578125
aF2.359216766357422
aF2.3497940063476563
aF2.3419837951660156
aF2.3368553161621093
aF2.3440122985839844
aF2.3410943603515624
aF2.3329428100585936
aF2.3393045043945313
aF2.3270089721679685
aF2.3361552429199217
aF2.3288832092285157
aF2.3013760375976564
aF2.314421844482422
aF2.296761932373047
aF2.2938499450683594
aF2.30208984375
aF2.2859040832519533
aF2.293369140625
aF2.296142120361328
aF2.2763404846191406
aF2.2743759155273438
aF2.280780944824219
aF2.2616368103027344
aF2.2596444702148437
aF2.2694940185546875
aF2.25921142578125
aF2.2451319885253906
aF2.2531285095214844
aF2.2391343688964844
aF2.2425473022460936
aF2.224736785888672
aF2.236478576660156
aF2.2241358947753906
aF2.2137106323242186
aF2.224270935058594
aF2.2162075805664063
aF2.2385614013671873
aF2.204532012939453
aF2.2126814270019532
aF2.206718292236328
aF2.1903761291503905
aF2.2012249755859377
aF2.2067361450195313
aF2.1754074096679688
aF2.178023376464844
aF2.19909912109375
asS'test_loss'
p3
(lp4
F4.125166015625
aF3.611114501953125
aF3.20630126953125
aF2.921883850097656
aF2.8395147705078125
aF2.802964782714844
aF2.7520486450195314
aF2.7227438354492186
aF2.7247137451171874
aF2.6723812866210936
aF2.6615606689453126
aF2.6237188720703126
aF2.593363952636719
aF2.5948355102539065
aF2.5829891967773437
aF2.5716781616210938
aF2.5737646484375
aF2.5483056640625
aF2.5522091674804686
aF2.5328633117675783
aF2.5284219360351563
aF2.5357852172851563
aF2.5189669799804686
aF2.5068630981445312
aF2.501378631591797
aF2.496701354980469
aF2.4813050842285156
aF2.4798033142089846
aF2.4859190368652344
aF2.4717475891113283
aF2.4670806884765626
aF2.4626966857910157
aF2.4641683959960936
aF2.463575897216797
aF2.4502098083496096
aF2.4391973876953124
aF2.426934814453125
aF2.426619567871094
aF2.4248704528808593
aF2.4200727844238283
aF2.4218617248535157
aF2.4074530029296874
aF2.4065953063964844
aF2.4054107666015625
aF2.398798370361328
aF2.3805551147460937
aF2.3721275329589844
aF2.375671844482422
aF2.366715545654297
aF2.373543701171875
aF2.374509735107422
aF2.3622137451171876
aF2.3545266723632814
aF2.3440428161621094
aF2.3408641052246093
aF2.3402520751953126
aF2.3535780334472656
aF2.338509979248047
aF2.331229248046875
aF2.3180809020996094
aF2.3013755798339846
aF2.313614959716797
aF2.3273423767089843
aF2.3015342712402345
aF2.305296630859375
aF2.2922393798828127
aF2.30874755859375
aF2.288557586669922
aF2.280767364501953
aF2.2727426147460936
aF2.2851266479492187
aF2.283634033203125
aF2.2793431091308594
aF2.267763214111328
aF2.2568699645996095
aF2.260992126464844
aF2.251751708984375
aF2.2436177062988283
aF2.269814910888672
aF2.237776794433594
aF2.2346432495117186
aF2.227850036621094
aF2.233458251953125
aF2.229051971435547
aF2.2334162902832033
aF2.2232740783691405
aF2.2232958984375
aF2.2123338317871095
aF2.2085693359375
aF2.2075682067871094
aF2.2116627502441406
aF2.207857360839844
aF2.2073020935058594
aF2.2106150817871093
aF2.205199737548828
aF2.1892613220214843
aF2.1932891845703124
aF2.1919801330566404
aF2.1845169067382812
aF2.1616624450683593
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004113699401639728
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 5s'
p10
sS'final_test_loss'
p11
F2.1616624450683593
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf5\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.