(dp0
S'train_loss'
p1
(lp2
F4.636521911621093
aF4.59457275390625
aF4.560619506835938
aF4.527470092773438
aF4.486902465820313
aF4.457328796386719
aF4.42153564453125
aF4.388378295898438
aF4.342757568359375
aF4.317327575683594
aF4.286026611328125
aF4.25275146484375
aF4.218299560546875
aF4.188395080566406
aF4.1444549560546875
aF4.108658447265625
aF4.078934020996094
aF4.037620239257812
aF4.005214233398437
aF3.9867840576171876
aF3.9434268188476564
aF3.9008297729492187
aF3.877808837890625
aF3.8442156982421873
aF3.800481262207031
aF3.764967346191406
aF3.742442932128906
aF3.7144073486328124
aF3.6752719116210937
aF3.643105163574219
aF3.6057952880859374
aF3.5891693115234373
aF3.5500701904296874
aF3.5294403076171874
aF3.4988967895507814
aF3.451968994140625
aF3.4345547485351564
aF3.409277038574219
aF3.359585876464844
aF3.357801818847656
aF3.3365972900390624
aF3.2827572631835937
aF3.273573913574219
aF3.268363342285156
aF3.2396304321289064
aF3.1954034423828124
aF3.173818359375
aF3.151916809082031
aF3.166628112792969
aF3.1480804443359376
aF3.0983251953125
aF3.0801016235351564
aF3.0555270385742186
aF3.0313330078125
aF3.0333050537109374
aF3.0259423828125
aF3.0238027954101563
aF2.995616455078125
aF2.9975735473632814
aF2.963461608886719
aF2.9566696166992186
aF2.9473284912109374
aF2.9151934814453124
aF2.912903137207031
aF2.906804504394531
aF2.9159970092773437
aF2.875205078125
aF2.8596112060546877
aF2.8706594848632814
aF2.8564303588867186
aF2.850322265625
aF2.8507720947265627
aF2.827247314453125
aF2.831815490722656
aF2.825794982910156
aF2.8259292602539063
aF2.803821716308594
aF2.8028524780273436
aF2.7992459106445313
aF2.7973666381835938
aF2.77531982421875
aF2.771340026855469
aF2.7515945434570312
aF2.751329040527344
aF2.750960693359375
aF2.7641192626953126
aF2.7425808715820312
aF2.7355999755859375
aF2.7380355834960937
aF2.7444796752929688
aF2.7357568359375
aF2.723008728027344
aF2.7086306762695314
aF2.723878173828125
aF2.7009768676757813
aF2.7100570678710936
aF2.7101458740234374
aF2.675531921386719
aF2.693268127441406
aF2.691572265625
asS'test_loss'
p3
(lp4
F4.596885070800782
aF4.559660949707031
aF4.52139404296875
aF4.4881680297851565
aF4.458257751464844
aF4.4208682250976565
aF4.384478759765625
aF4.350819091796875
aF4.315282287597657
aF4.274555358886719
aF4.249110412597656
aF4.214117736816406
aF4.173157653808594
aF4.1469140625
aF4.111297607421875
aF4.063907775878906
aF4.039140014648438
aF3.995821228027344
aF3.9764303588867187
aF3.9396481323242187
aF3.9124465942382813
aF3.87290771484375
aF3.8414947509765627
aF3.820359802246094
aF3.7790255737304688
aF3.7389450073242188
aF3.7112942504882813
aF3.6839129638671877
aF3.6421548461914064
aF3.6205645751953126
aF3.5770993041992187
aF3.5621218872070313
aF3.5106631469726564
aF3.4936260986328125
aF3.475250244140625
aF3.4335861206054688
aF3.4005966186523438
aF3.357943115234375
aF3.34362060546875
aF3.3047967529296876
aF3.3036669921875
aF3.2461123657226563
aF3.2495379638671875
aF3.213056640625
aF3.1829177856445314
aF3.181248779296875
aF3.140441589355469
aF3.1316848754882813
aF3.1257443237304687
aF3.079062805175781
aF3.074338684082031
aF3.0619134521484375
aF3.062576904296875
aF3.0454205322265624
aF3.0053118896484374
aF3.02208740234375
aF2.9874667358398437
aF2.963543701171875
aF2.9722195434570313
aF2.9381781005859375
aF2.921752624511719
aF2.9089923095703125
aF2.9070989990234377
aF2.8888201904296875
aF2.9005755615234374
aF2.872214050292969
aF2.8330047607421873
aF2.8609774780273436
aF2.8315835571289063
aF2.8470919799804686
aF2.8170266723632813
aF2.825142822265625
aF2.81485595703125
aF2.8094781494140624
aF2.8100384521484374
aF2.7672628784179687
aF2.786477355957031
aF2.7614144897460937
aF2.7858514404296875
aF2.761433410644531
aF2.73965087890625
aF2.7475982666015626
aF2.7599945068359375
aF2.7371246337890627
aF2.7550839233398436
aF2.7271652221679688
aF2.730376892089844
aF2.743532409667969
aF2.726176452636719
aF2.7298599243164063
aF2.7100433349609374
aF2.722392578125
aF2.709344787597656
aF2.7059335327148437
aF2.7020220947265625
aF2.7077291870117186
aF2.674702453613281
aF2.6817312622070313
aF2.6724972534179687
aF2.6847415161132813
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.000439498106105268
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 59s'
p10
sS'final_test_loss'
p11
F2.6847415161132813
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x9b\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.