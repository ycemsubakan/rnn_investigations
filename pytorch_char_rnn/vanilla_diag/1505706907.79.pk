(dp0
S'train_loss'
p1
(lp2
F4.607798156738281
aF4.5837945556640625
aF4.560464782714844
aF4.540474243164063
aF4.513829956054687
aF4.4920565795898435
aF4.468640747070313
aF4.446470947265625
aF4.424075927734375
aF4.401150512695312
aF4.3755929565429685
aF4.354498596191406
aF4.330308227539063
aF4.306604919433593
aF4.284749450683594
aF4.260348815917968
aF4.232066955566406
aF4.220440368652344
aF4.166719665527344
aF4.162590026855469
aF4.131596984863282
aF4.097508850097657
aF4.0772552490234375
aF4.045988159179688
aF4.02329345703125
aF4.000480346679687
aF3.9624703979492186
aF3.9276690673828125
aF3.9016363525390627
aF3.866629333496094
aF3.840364990234375
aF3.8105850219726562
aF3.7747979736328126
aF3.734252624511719
aF3.7187844848632814
aF3.681097412109375
aF3.669291687011719
aF3.6217681884765627
aF3.597337341308594
aF3.564729919433594
aF3.5263912963867186
aF3.5011309814453124
aF3.4715185546875
aF3.4634109497070313
aF3.4187283325195312
aF3.3989312744140623
aF3.378412170410156
aF3.365325927734375
aF3.347276306152344
aF3.3055984497070314
aF3.2669073486328126
aF3.2785845947265626
aF3.232623291015625
aF3.241029052734375
aF3.2087631225585938
aF3.1879046630859373
aF3.1678927612304686
aF3.143824462890625
aF3.1418145751953124
aF3.12459716796875
aF3.1026846313476564
aF3.1008126831054685
aF3.0951107788085936
aF3.0621511840820315
aF3.0659796142578126
aF3.0447659301757812
aF3.032309875488281
aF3.051555480957031
aF3.026717224121094
aF3.00431640625
aF2.9960678100585936
aF2.9860150146484377
aF2.9857333374023436
aF2.957855224609375
aF2.9595657348632813
aF2.9502606201171875
aF2.9278961181640626
aF2.9537063598632813
aF2.9258697509765623
aF2.9255471801757813
aF2.91819580078125
aF2.9017672729492188
aF2.903519287109375
aF2.8825344848632812
aF2.8885748291015627
aF2.888173828125
aF2.871124572753906
aF2.8781869506835935
aF2.890028076171875
aF2.868541564941406
aF2.8645401000976562
aF2.866263732910156
aF2.8578326416015627
aF2.8326406860351563
aF2.8334286499023436
aF2.8620635986328127
aF2.821754150390625
aF2.817650146484375
aF2.802853698730469
aF2.813715515136719
asS'test_loss'
p3
(lp4
F4.582965087890625
aF4.5601434326171875
aF4.538877868652344
aF4.512994384765625
aF4.493931274414063
aF4.468814086914063
aF4.449621276855469
aF4.423802795410157
aF4.399119262695312
aF4.378943176269531
aF4.347987670898437
aF4.329089965820312
aF4.305927734375
aF4.274668884277344
aF4.26218994140625
aF4.237008666992187
aF4.200975036621093
aF4.179265441894532
aF4.148650207519531
aF4.138354187011719
aF4.090628662109375
aF4.0780728149414065
aF4.047684936523438
aF4.016986389160156
aF3.98294677734375
aF3.9543084716796875
aF3.9269683837890623
aF3.903902587890625
aF3.861942138671875
aF3.835443420410156
aF3.8011795043945313
aF3.7803216552734376
aF3.7429766845703125
aF3.7234222412109377
aF3.6953726196289063
aF3.653620300292969
aF3.6292404174804687
aF3.5803659057617185
aF3.5530108642578124
aF3.5478399658203124
aF3.5035809326171874
aF3.467974853515625
aF3.4544168090820313
aF3.4295599365234377
aF3.3737646484375
aF3.367864685058594
aF3.3535205078125
aF3.3321542358398437
aF3.3026419067382813
aF3.2746405029296874
aF3.2791815185546875
aF3.2533026123046875
aF3.210602111816406
aF3.2078811645507814
aF3.1878567504882813
aF3.1742767333984374
aF3.141251525878906
aF3.1397320556640627
aF3.1207510375976564
aF3.1075949096679687
aF3.093324890136719
aF3.115006408691406
aF3.0840411376953125
aF3.0629364013671876
aF3.0544091796875
aF3.0204351806640624
aF3.001650695800781
aF3.0125177001953123
aF2.9903216552734375
aF3.012164306640625
aF2.960455017089844
aF2.9953302001953124
aF2.9743728637695312
aF2.961400451660156
aF2.9777566528320314
aF2.942721252441406
aF2.943631591796875
aF2.9273382568359376
aF2.9088824462890623
aF2.9207916259765625
aF2.92693603515625
aF2.860598449707031
aF2.892278137207031
aF2.8767105102539063
aF2.8757980346679686
aF2.8536376953125
aF2.87376708984375
aF2.8697994995117186
aF2.8479769897460936
aF2.856495056152344
aF2.8396002197265626
aF2.85333740234375
aF2.8409054565429686
aF2.8159442138671875
aF2.830285949707031
aF2.8236929321289064
aF2.804234924316406
aF2.818007507324219
aF2.80821533203125
aF2.809842224121094
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00018846958820701283
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 46s'
p10
sS'final_test_loss'
p11
F2.809842224121094
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xd0\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.