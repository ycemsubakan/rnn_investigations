(dp0
S'train_loss'
p1
(lp2
F4.633990478515625
aF4.560872802734375
aF4.485256042480469
aF4.410401916503906
aF4.329046020507812
aF4.26227783203125
aF4.197866516113281
aF4.127952575683594
aF4.06494873046875
aF3.991485900878906
aF3.9299078369140625
aF3.8610714721679686
aF3.7926422119140626
aF3.72208251953125
aF3.665332946777344
aF3.5958633422851562
aF3.544572448730469
aF3.4914410400390623
aF3.4267227172851564
aF3.370885009765625
aF3.341177978515625
aF3.2547113037109376
aF3.2386373901367187
aF3.1653497314453123
aF3.1534503173828123
aF3.1201513671875
aF3.074996337890625
aF3.0513275146484373
aF2.994044189453125
aF2.984488220214844
aF2.9825830078125
aF2.9072647094726562
aF2.8912152099609374
aF2.8916384887695314
aF2.866397705078125
aF2.8509567260742186
aF2.8298745727539063
aF2.801204833984375
aF2.801483154296875
aF2.7964501953125
aF2.7878121948242187
aF2.7425765991210938
aF2.757764892578125
aF2.7409124755859375
aF2.7164065551757814
aF2.69626220703125
aF2.6984124755859376
aF2.713465576171875
aF2.702575988769531
aF2.6739266967773436
aF2.6789361572265626
aF2.679471435546875
aF2.683629455566406
aF2.660227966308594
aF2.670601501464844
aF2.6549554443359376
aF2.6437933349609377
aF2.649813232421875
aF2.6295257568359376
aF2.6243942260742186
aF2.6247714233398436
aF2.6217837524414063
aF2.6072906494140624
aF2.62166748046875
aF2.604867248535156
aF2.607327880859375
aF2.609822998046875
aF2.6095477294921876
aF2.589296569824219
aF2.5813702392578124
aF2.5933377075195314
aF2.587279052734375
aF2.575556640625
aF2.5701632690429688
aF2.563199462890625
aF2.572218017578125
aF2.576127624511719
aF2.583843078613281
aF2.576124572753906
aF2.5786892700195314
aF2.5567640686035156
aF2.5525172424316405
aF2.5842111206054685
aF2.5778994750976563
aF2.5421434020996094
aF2.5361309814453126
aF2.546800537109375
aF2.5441473388671874
aF2.57477294921875
aF2.5660302734375
aF2.5650625610351563
aF2.555005645751953
aF2.528639678955078
aF2.527171783447266
aF2.546281433105469
aF2.543301239013672
aF2.5216519165039064
aF2.530911865234375
aF2.5321333312988283
aF2.523759613037109
asS'test_loss'
p3
(lp4
F4.557017822265625
aF4.48250732421875
aF4.4093624877929685
aF4.33412353515625
aF4.260917358398437
aF4.204494323730469
aF4.11541748046875
aF4.050340270996093
aF3.9977301025390624
aF3.911463317871094
aF3.852635498046875
aF3.78851806640625
aF3.715106201171875
aF3.651881103515625
aF3.5930142211914062
aF3.52884033203125
aF3.4552447509765627
aF3.4095391845703125
aF3.36657470703125
aF3.3238714599609374
aF3.2684527587890626
aF3.232393493652344
aF3.1722262573242186
aF3.1282666015625
aF3.1274642944335938
aF3.0510848999023437
aF3.0425933837890624
aF2.9946588134765624
aF2.9606814575195313
aF2.94417236328125
aF2.911370849609375
aF2.8901632690429686
aF2.885142517089844
aF2.8310662841796876
aF2.8456552124023435
aF2.825464782714844
aF2.8001412963867187
aF2.81607666015625
aF2.776622314453125
aF2.7746408081054685
aF2.7263006591796874
aF2.7318426513671876
aF2.739769287109375
aF2.7538320922851565
aF2.71759521484375
aF2.6767779541015626
aF2.6942047119140624
aF2.711324462890625
aF2.677590637207031
aF2.65642822265625
aF2.657620544433594
aF2.6449710083007814
aF2.6524111938476564
aF2.640483093261719
aF2.6437448120117186
aF2.6379473876953123
aF2.6166094970703124
aF2.642904968261719
aF2.6589779663085937
aF2.615350341796875
aF2.617995300292969
aF2.6269271850585936
aF2.6258694458007814
aF2.5902420043945313
aF2.5871487426757813
aF2.614724426269531
aF2.6148605346679688
aF2.5784646606445314
aF2.574239501953125
aF2.5877197265625
aF2.606778259277344
aF2.5795687866210937
aF2.5781658935546874
aF2.5711785888671876
aF2.578675537109375
aF2.5736932373046875
aF2.5753860473632812
aF2.5722122192382812
aF2.5575883483886717
aF2.5742022705078127
aF2.556771087646484
aF2.5525604248046876
aF2.5508694458007812
aF2.5544528198242187
aF2.5429306030273438
aF2.5486907958984375
aF2.5299591064453124
aF2.531233062744141
aF2.5625347900390625
aF2.5445681762695314
aF2.537047882080078
aF2.557472381591797
aF2.5322305297851564
aF2.528706817626953
aF2.535645294189453
aF2.5310699462890627
aF2.518326873779297
aF2.5132908630371094
aF2.5207086181640626
aF2.524858245849609
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0009289622227963304
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 59s'
p10
sS'final_test_loss'
p11
F2.524858245849609
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x97\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.