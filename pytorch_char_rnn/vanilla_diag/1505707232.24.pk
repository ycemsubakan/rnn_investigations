(dp0
S'train_loss'
p1
(lp2
F4.620614929199219
aF4.284788513183594
aF3.89016357421875
aF3.45892578125
aF3.1857537841796875
aF3.026461486816406
aF2.9822711181640624
aF2.926560974121094
aF2.8341799926757814
aF2.81140869140625
aF2.79430419921875
aF2.765975036621094
aF2.7326651000976563
aF2.6957626342773438
aF2.661435241699219
aF2.68390625
aF2.6359573364257813
aF2.6208145141601564
aF2.597911682128906
aF2.592212829589844
aF2.5846566772460937
aF2.554972686767578
aF2.556415252685547
aF2.558358917236328
aF2.5230999755859376
aF2.5550631713867187
aF2.5185447692871095
aF2.5247027587890627
aF2.499208526611328
aF2.5019570922851564
aF2.4966656494140627
aF2.487997741699219
aF2.4826931762695312
aF2.469776458740234
aF2.472922058105469
aF2.4835858154296875
aF2.4508033752441407
aF2.4484793090820314
aF2.4150636291503904
aF2.4351844787597656
aF2.4084027099609373
aF2.412738800048828
aF2.4019979858398437
aF2.411074676513672
aF2.38499267578125
aF2.3728668212890627
aF2.374134521484375
aF2.359624938964844
aF2.3626171875
aF2.337965393066406
aF2.35461181640625
aF2.3431101989746095
aF2.334874572753906
aF2.3320075988769533
aF2.312685852050781
aF2.3142160034179686
aF2.3370774841308593
aF2.302703552246094
aF2.3019671630859375
aF2.3029490661621095
aF2.280393371582031
aF2.308505401611328
aF2.2841752624511718
aF2.254569854736328
aF2.2879679870605467
aF2.2601580810546875
aF2.2505502319335937
aF2.265386199951172
aF2.2469050598144533
aF2.2640248107910157
aF2.2179563903808592
aF2.2391650390625
aF2.2294361877441404
aF2.2455516052246094
aF2.2432635498046873
aF2.210856628417969
aF2.201374969482422
aF2.21033935546875
aF2.2173721313476564
aF2.2041612243652344
aF2.206759033203125
aF2.1770875549316404
aF2.1975807189941405
aF2.201282196044922
aF2.184287414550781
aF2.2035403442382813
aF2.183360137939453
aF2.186014862060547
aF2.1762069702148437
aF2.164198303222656
aF2.169938659667969
aF2.162284240722656
aF2.1622914123535155
aF2.1525035095214844
aF2.15226318359375
aF2.160875701904297
aF2.1490852355957033
aF2.1513401794433595
aF2.14128173828125
aF2.128384704589844
asS'test_loss'
p3
(lp4
F4.281700744628906
aF3.8725009155273438
aF3.4546954345703127
aF3.1668972778320312
aF3.037095031738281
aF2.9440121459960937
aF2.929936828613281
aF2.8685113525390626
aF2.7969241333007813
aF2.7686666870117187
aF2.7351303100585938
aF2.716877136230469
aF2.6829962158203124
aF2.650818176269531
aF2.6555667114257813
aF2.630950927734375
aF2.6202984619140626
aF2.5958428955078126
aF2.5839593505859373
aF2.5842416381835935
aF2.5679425048828124
aF2.5459356689453125
aF2.536986083984375
aF2.5218333435058593
aF2.523480987548828
aF2.5035856628417967
aF2.5147671508789062
aF2.5052861022949218
aF2.496253662109375
aF2.491907043457031
aF2.475700225830078
aF2.445217742919922
aF2.464678955078125
aF2.454725799560547
aF2.4461605834960936
aF2.4164613342285155
aF2.424880065917969
aF2.42360107421875
aF2.4357203674316406
aF2.400709533691406
aF2.395299835205078
aF2.3861184692382813
aF2.3873193359375
aF2.3778924560546875
aF2.364879608154297
aF2.377912139892578
aF2.369284210205078
aF2.3612286376953127
aF2.3435763549804687
aF2.340887145996094
aF2.3479769897460936
aF2.3214981079101564
aF2.3047105407714845
aF2.3240957641601563
aF2.333185577392578
aF2.294761962890625
aF2.322162322998047
aF2.283059387207031
aF2.2985260009765627
aF2.2974703979492186
aF2.2705953979492186
aF2.284701690673828
aF2.262635498046875
aF2.2650625610351565
aF2.2665631103515627
aF2.2517520141601564
aF2.2515318298339846
aF2.242370910644531
aF2.241197967529297
aF2.2471354675292967
aF2.235229187011719
aF2.223006286621094
aF2.233668212890625
aF2.219205627441406
aF2.2370970153808596
aF2.2327203369140625
aF2.204999542236328
aF2.1987001037597658
aF2.2163514709472656
aF2.2038763427734374
aF2.2024708557128907
aF2.190638427734375
aF2.1929885864257814
aF2.1998916625976563
aF2.1864140319824217
aF2.191794128417969
aF2.1756947326660154
aF2.1866290283203127
aF2.165462646484375
aF2.170301971435547
aF2.1629248046875
aF2.1536129760742186
aF2.147115936279297
aF2.149092254638672
aF2.150859832763672
aF2.156454620361328
aF2.1459552001953126
aF2.136466064453125
aF2.115082092285156
aF2.137473907470703
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008746113542171476
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 12s'
p10
sS'final_test_loss'
p11
F2.137473907470703
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'[\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.