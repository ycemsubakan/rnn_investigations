(dp0
S'train_loss'
p1
(lp2
F4.627333984375
aF4.175752868652344
aF3.681496276855469
aF3.2278985595703125
aF2.984522399902344
aF2.87827880859375
aF2.81157958984375
aF2.775709228515625
aF2.7487777709960937
aF2.7082818603515624
aF2.7121957397460936
aF2.649294738769531
aF2.6422564697265627
aF2.611707763671875
aF2.582178955078125
aF2.5854144287109375
aF2.5883303833007814
aF2.54873046875
aF2.5624234008789064
aF2.5379153442382814
aF2.543701477050781
aF2.5331581115722654
aF2.5243408203125
aF2.504276123046875
aF2.5163633728027346
aF2.5112599182128905
aF2.510907745361328
aF2.4991018676757815
aF2.5030783081054686
aF2.477047576904297
aF2.4835050964355467
aF2.466690673828125
aF2.461700897216797
aF2.4659136962890624
aF2.4480659484863283
aF2.461749725341797
aF2.4530926513671876
aF2.4239323425292967
aF2.4462399291992187
aF2.4204150390625
aF2.412406005859375
aF2.417003173828125
aF2.412953186035156
aF2.4063243103027343
aF2.4074392700195313
aF2.406296844482422
aF2.387895965576172
aF2.3944435119628906
aF2.382102813720703
aF2.3912278747558595
aF2.3852398681640623
aF2.388599090576172
aF2.3444097900390624
aF2.3713282775878906
aF2.3561555480957033
aF2.3542442321777344
aF2.3546095275878907
aF2.3416484069824217
aF2.337643890380859
aF2.3250035095214843
aF2.327861480712891
aF2.3166818237304687
aF2.3067852783203127
aF2.321584930419922
aF2.3048658752441407
aF2.308518371582031
aF2.293651580810547
aF2.2912255859375
aF2.2834101867675782
aF2.2802342224121093
aF2.288453369140625
aF2.2750396728515625
aF2.2550813293457033
aF2.284762725830078
aF2.26078125
aF2.253487548828125
aF2.240625
aF2.2559600830078126
aF2.2481349182128905
aF2.261083984375
aF2.2501690673828123
aF2.2318150329589845
aF2.2241995239257815
aF2.2413259887695314
aF2.223302459716797
aF2.2118992614746094
aF2.2181546020507814
aF2.2174716186523438
aF2.193858184814453
aF2.1901010131835936
aF2.196142578125
aF2.19199951171875
aF2.213298645019531
aF2.1850741577148436
aF2.1994801330566407
aF2.1661122131347654
aF2.206939697265625
aF2.19463623046875
aF2.1781997680664062
aF2.17969970703125
asS'test_loss'
p3
(lp4
F4.163095703125
aF3.681966552734375
aF3.2178335571289063
aF2.9702334594726563
aF2.862786865234375
aF2.823658752441406
aF2.759299621582031
aF2.72802490234375
aF2.701582946777344
aF2.671510925292969
aF2.63155029296875
aF2.653120422363281
aF2.606163330078125
aF2.5874862670898438
aF2.5649142456054688
aF2.5820022583007813
aF2.5481573486328126
aF2.5524658203125
aF2.5369033813476562
aF2.551577606201172
aF2.5194610595703124
aF2.519214630126953
aF2.505115509033203
aF2.4934039306640625
aF2.5128944396972654
aF2.478681793212891
aF2.4957035827636718
aF2.4812017822265626
aF2.487781982421875
aF2.4811688232421876
aF2.476639404296875
aF2.4652610778808595
aF2.4586634826660156
aF2.4505511474609376
aF2.438698425292969
aF2.4440521240234374
aF2.4290615844726564
aF2.418539276123047
aF2.42360107421875
aF2.4167059326171874
aF2.4022250366210938
aF2.415933837890625
aF2.384034423828125
aF2.377840881347656
aF2.385042419433594
aF2.393982238769531
aF2.3866029357910157
aF2.3728895568847657
aF2.3691412353515626
aF2.3669424438476563
aF2.3804522705078126
aF2.359292755126953
aF2.355631561279297
aF2.3584974670410155
aF2.3397540283203124
aF2.3373974609375
aF2.331559906005859
aF2.3212873840332033
aF2.3059152221679686
aF2.301623992919922
aF2.311465606689453
aF2.316925048828125
aF2.3000489807128908
aF2.297144317626953
aF2.2792207336425783
aF2.2995704650878905
aF2.2833930969238283
aF2.2740557861328123
aF2.2721484375
aF2.279940948486328
aF2.2783265686035157
aF2.272000579833984
aF2.269270324707031
aF2.254597015380859
aF2.2512693786621094
aF2.236250152587891
aF2.254700469970703
aF2.2247027587890624
aF2.2365382385253905
aF2.226940155029297
aF2.2303472900390626
aF2.229797821044922
aF2.237102355957031
aF2.2172705078125
aF2.2221063232421874
aF2.207822265625
aF2.205938415527344
aF2.213360595703125
aF2.202317657470703
aF2.1993470764160157
aF2.203935241699219
aF2.20368896484375
aF2.1951866149902344
aF2.1947833251953126
aF2.188821563720703
aF2.1758547973632814
aF2.1840394592285155
aF2.184479064941406
aF2.184317779541016
aF2.1683663940429687
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004279312335078149
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 54s'
p10
sS'final_test_loss'
p11
F2.1683663940429687
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xdf\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.