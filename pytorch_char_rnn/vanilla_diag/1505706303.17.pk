(dp0
S'train_loss'
p1
(lp2
F4.628456115722656
aF4.547308654785156
aF4.467873229980468
aF4.38780517578125
aF4.306030883789062
aF4.222119445800781
aF4.13306396484375
aF4.059598388671875
aF3.9813888549804686
aF3.8860675048828126
aF3.79540283203125
aF3.707554931640625
aF3.626235046386719
aF3.5199618530273438
aF3.4427899169921874
aF3.3613619995117188
aF3.3057766723632813
aF3.211947326660156
aF3.1779098510742188
aF3.107005615234375
aF3.0369049072265626
aF3.0018408203125
aF2.9803277587890626
aF2.905018615722656
aF2.918001403808594
aF2.914483642578125
aF2.8878851318359375
aF2.8589791870117187
aF2.8527874755859375
aF2.821535949707031
aF2.8513064575195313
aF2.8177960205078123
aF2.785115051269531
aF2.7619448852539064
aF2.7638455200195313
aF2.7575674438476563
aF2.714132995605469
aF2.7235150146484375
aF2.7156573486328126
aF2.728141174316406
aF2.706109619140625
aF2.716267395019531
aF2.712041320800781
aF2.6890963745117187
aF2.6787820434570313
aF2.6837060546875
aF2.6743533325195314
aF2.6497152709960936
aF2.659682312011719
aF2.6440460205078127
aF2.6561138916015623
aF2.6385809326171876
aF2.642272644042969
aF2.63522216796875
aF2.64041259765625
aF2.6434222412109376
aF2.6211468505859377
aF2.6187939453125
aF2.6198870849609377
aF2.6172381591796876
aF2.619595947265625
aF2.597350769042969
aF2.600074462890625
aF2.607998046875
aF2.5906219482421875
aF2.6007073974609374
aF2.5658889770507813
aF2.588927917480469
aF2.5902215576171876
aF2.572873840332031
aF2.576920166015625
aF2.562256164550781
aF2.584091796875
aF2.5810110473632815
aF2.537875061035156
aF2.572135314941406
aF2.5728115844726562
aF2.54352294921875
aF2.5518177795410155
aF2.5531350708007814
aF2.5507278442382812
aF2.541448974609375
aF2.5565057373046876
aF2.539284210205078
aF2.530589599609375
aF2.540499420166016
aF2.5283854675292967
aF2.532681732177734
aF2.5397187805175783
aF2.5278451538085935
aF2.5326693725585936
aF2.542472686767578
aF2.515667724609375
aF2.528916931152344
aF2.5330935668945314
aF2.5110411071777343
aF2.5129856872558594
aF2.503752899169922
aF2.535988311767578
aF2.5046675109863283
asS'test_loss'
p3
(lp4
F4.547694396972656
aF4.468341674804687
aF4.389026489257812
aF4.307197265625
aF4.221781311035156
aF4.144932556152344
aF4.046449890136719
aF3.9628482055664063
aF3.880303955078125
aF3.78484130859375
aF3.6919064331054687
aF3.591663818359375
aF3.5038677978515627
aF3.4471990966796877
aF3.346910095214844
aF3.295507507324219
aF3.198985290527344
aF3.14638427734375
aF3.1067279052734373
aF3.047164306640625
aF2.9921231079101562
aF2.9759765625
aF2.9543377685546877
aF2.9107565307617187
aF2.9050537109375
aF2.883822021484375
aF2.839971923828125
aF2.856687316894531
aF2.8155868530273436
aF2.7910614013671875
aF2.7988217163085936
aF2.781233825683594
aF2.774168701171875
aF2.7774017333984373
aF2.7433322143554686
aF2.743504943847656
aF2.722510986328125
aF2.7420828247070315
aF2.7269406127929687
aF2.7266525268554687
aF2.695286865234375
aF2.701924133300781
aF2.7133770751953126
aF2.6631182861328124
aF2.6666329956054686
aF2.6721151733398436
aF2.6446096801757815
aF2.6670635986328124
aF2.663204345703125
aF2.6496310424804688
aF2.642169189453125
aF2.642608337402344
aF2.624900207519531
aF2.642578125
aF2.599015197753906
aF2.6212924194335936
aF2.58903564453125
aF2.6116912841796873
aF2.6109576416015625
aF2.5916714477539062
aF2.5741241455078123
aF2.5924859619140626
aF2.5889837646484377
aF2.56948486328125
aF2.583372802734375
aF2.5702099609375
aF2.569375305175781
aF2.568284912109375
aF2.580332946777344
aF2.5766860961914064
aF2.5532830810546874
aF2.568320007324219
aF2.546317443847656
aF2.5681631469726565
aF2.5483627319335938
aF2.545387725830078
aF2.5462911987304686
aF2.5639102172851564
aF2.5351356506347655
aF2.535244903564453
aF2.549349060058594
aF2.5261302185058594
aF2.555469207763672
aF2.517922058105469
aF2.527047424316406
aF2.5277560424804686
aF2.523363494873047
aF2.5219273376464844
aF2.514787292480469
aF2.5256996154785156
aF2.5193852233886718
aF2.5091792297363282
aF2.5011613464355467
aF2.5031672668457032
aF2.509012908935547
aF2.5021794128417967
aF2.5250360107421876
aF2.5028929138183593
aF2.504741973876953
aF2.5079933166503907
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005807112467613897
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 0s'
p10
sS'final_test_loss'
p11
F2.5079933166503907
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf3\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.