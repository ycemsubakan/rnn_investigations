(dp0
S'train_loss'
p1
(lp2
F4.620486450195313
aF3.879915466308594
aF3.206243591308594
aF2.924888000488281
aF2.8487530517578126
aF2.7645269775390626
aF2.750385437011719
aF2.676466979980469
aF2.6613311767578125
aF2.6250350952148436
aF2.616582336425781
aF2.59517333984375
aF2.5716873168945313
aF2.555114288330078
aF2.564588623046875
aF2.527993927001953
aF2.539759216308594
aF2.538657531738281
aF2.499478302001953
aF2.4912648010253906
aF2.511322479248047
aF2.5016128540039064
aF2.4828170776367187
aF2.480892333984375
aF2.46402587890625
aF2.4667681884765624
aF2.4690452575683595
aF2.469402008056641
aF2.4410504150390624
aF2.4295140075683594
aF2.4384185791015627
aF2.424420166015625
aF2.4226373291015624
aF2.4115890502929687
aF2.4109336853027346
aF2.3946234130859376
aF2.3829197692871094
aF2.387390441894531
aF2.3617594909667967
aF2.3501513671875
aF2.3573980712890625
aF2.3698306274414063
aF2.3518075561523437
aF2.3473365783691404
aF2.341127624511719
aF2.3400494384765627
aF2.3298490905761717
aF2.3270216369628907
aF2.317667236328125
aF2.3230519104003906
aF2.2908586120605468
aF2.309039764404297
aF2.3120651245117188
aF2.2828526306152344
aF2.2783642578125
aF2.2927169799804688
aF2.2671640014648435
aF2.274716339111328
aF2.2634016418457032
aF2.255931549072266
aF2.257982635498047
aF2.2443385314941406
aF2.2284918212890625
aF2.2456317138671875
aF2.235160675048828
aF2.2178564453125
aF2.2119342041015626
aF2.2191571044921874
aF2.2273768615722656
aF2.200491485595703
aF2.211059112548828
aF2.1921661376953123
aF2.178596954345703
aF2.1906051635742188
aF2.1776295471191407
aF2.1510711669921876
aF2.1593359375
aF2.1748045349121092
aF2.15085693359375
aF2.148299102783203
aF2.1469390869140623
aF2.1509909057617187
aF2.14843505859375
aF2.148695526123047
aF2.156898956298828
aF2.1333576965332033
aF2.137933807373047
aF2.1384512329101564
aF2.1514511108398438
aF2.131117858886719
aF2.1192645263671874
aF2.1118458557128905
aF2.128282928466797
aF2.1275146484375
aF2.107963104248047
aF2.1047793579101564
aF2.112099609375
aF2.103557891845703
aF2.103143615722656
aF2.0931437683105467
asS'test_loss'
p3
(lp4
F3.854332275390625
aF3.2044638061523436
aF2.9087295532226562
aF2.837460632324219
aF2.7400161743164064
aF2.73630859375
aF2.6889724731445312
aF2.66728271484375
aF2.6399713134765626
aF2.6145550537109377
aF2.5949807739257813
aF2.5538539123535156
aF2.5499534606933594
aF2.5559202575683595
aF2.5180677795410156
aF2.5231500244140626
aF2.5454591369628905
aF2.5018626403808595
aF2.511158142089844
aF2.4949717712402344
aF2.4857742309570314
aF2.4837350463867187
aF2.464699554443359
aF2.446068115234375
aF2.4587933349609377
aF2.457018280029297
aF2.4579428100585936
aF2.4336228942871094
aF2.4330284118652346
aF2.4239068603515626
aF2.409449005126953
aF2.4316777038574218
aF2.4145108032226563
aF2.4008538818359373
aF2.3773175048828126
aF2.376473236083984
aF2.3573484802246094
aF2.3611105346679686
aF2.367890167236328
aF2.356650085449219
aF2.3590876770019533
aF2.345151062011719
aF2.3363331604003905
aF2.3365570068359376
aF2.326346435546875
aF2.3236680603027344
aF2.32964599609375
aF2.2921488952636717
aF2.2944046020507813
aF2.309156494140625
aF2.2831491088867186
aF2.3040431213378905
aF2.2815570068359374
aF2.2718829345703124
aF2.272286071777344
aF2.258706512451172
aF2.255696258544922
aF2.279457244873047
aF2.2369557189941407
aF2.2486827087402346
aF2.2401387023925783
aF2.235617370605469
aF2.2088639831542967
aF2.2146763610839844
aF2.212632904052734
aF2.208564453125
aF2.2126998901367188
aF2.2087411499023437
aF2.183910217285156
aF2.1909494018554687
aF2.203169708251953
aF2.17896240234375
aF2.1729920959472655
aF2.1759613037109373
aF2.166700439453125
aF2.1605235290527345
aF2.1721192932128908
aF2.1662107849121095
aF2.155423583984375
aF2.149899139404297
aF2.1451446533203127
aF2.1470411682128905
aF2.1261329650878906
aF2.1362869262695314
aF2.140994873046875
aF2.127177276611328
aF2.12093017578125
aF2.140039825439453
aF2.103379211425781
aF2.099081726074219
aF2.1357681274414064
aF2.0934130859375
aF2.0940484619140625
aF2.1007540893554686
aF2.1002629089355467
aF2.091768798828125
aF2.0816168212890624
aF2.091657867431641
aF2.089488983154297
aF2.0617521667480467
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.007873878576172286
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 58s'
p10
sS'final_test_loss'
p11
F2.0617521667480467
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xeb\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.