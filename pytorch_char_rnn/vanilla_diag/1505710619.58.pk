(dp0
S'train_loss'
p1
(lp2
F4.634795227050781
aF4.309656677246093
aF3.9854193115234375
aF3.6294061279296876
aF3.2866473388671875
aF3.076593322753906
aF2.949962463378906
aF2.8691424560546874
aF2.86273681640625
aF2.8062127685546874
aF2.774903564453125
aF2.72916748046875
aF2.722198486328125
aF2.7130487060546873
aF2.681800537109375
aF2.6554595947265627
aF2.6566531372070314
aF2.5996759033203123
aF2.6147552490234376
aF2.601190490722656
aF2.5874774169921877
aF2.591679992675781
aF2.5575592041015627
aF2.5456105041503907
aF2.5659860229492186
aF2.544394683837891
aF2.5521673583984374
aF2.551563415527344
aF2.519401397705078
aF2.5208277893066406
aF2.5055087280273436
aF2.520313415527344
aF2.501197967529297
aF2.4942617797851563
aF2.500718078613281
aF2.481106414794922
aF2.4817694091796874
aF2.452543182373047
aF2.4774476623535158
aF2.4774407958984375
aF2.4609483337402343
aF2.468553924560547
aF2.4597206115722656
aF2.4387672424316404
aF2.4380198669433595
aF2.4228102111816407
aF2.419001007080078
aF2.4131649780273436
aF2.4012715148925783
aF2.379020233154297
aF2.395049591064453
aF2.4052693176269533
aF2.389243316650391
aF2.389881134033203
aF2.397782287597656
aF2.36814208984375
aF2.3563383483886717
aF2.349212341308594
aF2.3496026611328125
aF2.3372613525390626
aF2.3493556213378906
aF2.353335418701172
aF2.3542359924316405
aF2.351461944580078
aF2.31337158203125
aF2.3240016174316405
aF2.3218922424316406
aF2.3267288208007812
aF2.3156117248535155
aF2.324939422607422
aF2.2978755187988282
aF2.3083535766601564
aF2.30890869140625
aF2.3053884887695313
aF2.307805633544922
aF2.3134309387207033
aF2.284409484863281
aF2.2913276672363283
aF2.2817686462402342
aF2.276031494140625
aF2.2677886962890623
aF2.255289764404297
aF2.2840394592285156
aF2.246745147705078
aF2.2595285034179686
aF2.250910186767578
aF2.2465696716308594
aF2.2479872131347656
aF2.249698486328125
aF2.245680084228516
aF2.219038238525391
aF2.2182167053222654
aF2.2201690673828125
aF2.2178155517578126
aF2.228246154785156
aF2.2368289184570314
aF2.2291697692871093
aF2.204073028564453
aF2.211258850097656
aF2.205915679931641
asS'test_loss'
p3
(lp4
F4.306681823730469
aF3.9771878051757814
aF3.638957214355469
aF3.2828805541992185
aF3.07796142578125
aF2.931504211425781
aF2.863233642578125
aF2.8373193359375
aF2.799244384765625
aF2.742393798828125
aF2.7218353271484377
aF2.710637512207031
aF2.6999331665039064
aF2.6933889770507813
aF2.6497271728515623
aF2.6284136962890625
aF2.607314453125
aF2.60221923828125
aF2.5914291381835937
aF2.559339141845703
aF2.5541152954101562
aF2.5735736083984375
aF2.545756530761719
aF2.5737237548828125
aF2.5294532775878906
aF2.5419561767578127
aF2.5311289978027345
aF2.516307067871094
aF2.535405731201172
aF2.511966552734375
aF2.4946748352050783
aF2.5115707397460936
aF2.497367095947266
aF2.4791827392578125
aF2.4846726989746095
aF2.468549346923828
aF2.465849609375
aF2.4562457275390623
aF2.4580043029785155
aF2.450881042480469
aF2.4509153747558594
aF2.4466127014160155
aF2.445392761230469
aF2.4187138366699217
aF2.4307492065429686
aF2.406760711669922
aF2.4036268615722656
aF2.4098072814941407
aF2.400940246582031
aF2.3905548095703124
aF2.3944642639160154
aF2.3852069091796877
aF2.378390197753906
aF2.375702362060547
aF2.3679800415039063
aF2.364346160888672
aF2.3576199340820314
aF2.3591604614257813
aF2.3488528442382814
aF2.342254638671875
aF2.3495802307128906
aF2.3337799072265626
aF2.326377258300781
aF2.3268235778808593
aF2.3119651794433596
aF2.305844573974609
aF2.3150946044921876
aF2.3161244201660156
aF2.3075941467285155
aF2.295458984375
aF2.3139056396484374
aF2.2814637756347658
aF2.301719207763672
aF2.2679364013671877
aF2.2857844543457033
aF2.2720314025878907
aF2.2770124816894532
aF2.272011566162109
aF2.273026885986328
aF2.267883758544922
aF2.2767088317871096
aF2.273282012939453
aF2.25627197265625
aF2.2503001403808596
aF2.243202972412109
aF2.2431553649902343
aF2.2496978759765627
aF2.250246887207031
aF2.242449951171875
aF2.244110870361328
aF2.225388641357422
aF2.229475860595703
aF2.221426239013672
aF2.212386169433594
aF2.217823791503906
aF2.230812683105469
aF2.184866638183594
aF2.202659149169922
aF2.20208984375
aF2.1933551025390625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004444332921027725
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 34s'
p10
sS'final_test_loss'
p11
F2.1933551025390625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xa2\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.