(dp0
S'train_loss'
p1
(lp2
F4.622425231933594
aF4.535960388183594
aF4.449293518066407
aF4.365040588378906
aF4.269544067382813
aF4.185303649902344
aF4.087784118652344
aF3.9981512451171874
aF3.8913623046875
aF3.8049777221679686
aF3.7089639282226563
aF3.6111654663085937
aF3.5224514770507813
aF3.443059997558594
aF3.3470599365234377
aF3.289072265625
aF3.2190472412109377
aF3.17362548828125
aF3.13316650390625
aF3.0912686157226563
aF3.0561566162109375
aF3.0099411010742188
aF2.9998193359375
aF2.9554397583007814
aF2.9375619506835937
aF2.904383239746094
aF2.8731069946289063
aF2.8385870361328127
aF2.8447909545898438
aF2.809443359375
aF2.801083679199219
aF2.789109802246094
aF2.7851602172851564
aF2.7702987670898436
aF2.780542297363281
aF2.734530029296875
aF2.7522625732421875
aF2.7524365234375
aF2.7292718505859375
aF2.6924563598632814
aF2.698217468261719
aF2.672435302734375
aF2.6737060546875
aF2.683896179199219
aF2.6591177368164063
aF2.6568234252929686
aF2.6393023681640626
aF2.657281799316406
aF2.621314697265625
aF2.625482177734375
aF2.63635009765625
aF2.6255767822265623
aF2.636891174316406
aF2.6012887573242187
aF2.6078012084960935
aF2.6083218383789064
aF2.6157952880859376
aF2.5907223510742186
aF2.5741265869140624
aF2.5823150634765626
aF2.602562255859375
aF2.5909112548828124
aF2.5747726440429686
aF2.5836102294921877
aF2.5538784790039064
aF2.5542848205566404
aF2.5609207153320312
aF2.5742572021484373
aF2.568533630371094
aF2.5434625244140623
aF2.544716796875
aF2.5586712646484373
aF2.5452967834472657
aF2.549518890380859
aF2.5469906616210936
aF2.5448117065429687
aF2.529357604980469
aF2.522941131591797
aF2.5325163269042967
aF2.537394714355469
aF2.529583740234375
aF2.5254862976074217
aF2.5415281677246093
aF2.534839630126953
aF2.5293223571777346
aF2.5128347778320315
aF2.5143353271484377
aF2.5131044006347656
aF2.506449737548828
aF2.517782440185547
aF2.4934037780761718
aF2.5108111572265623
aF2.507995300292969
aF2.50406982421875
aF2.5059579467773436
aF2.505094451904297
aF2.49654296875
aF2.492755584716797
aF2.500721740722656
aF2.490917205810547
asS'test_loss'
p3
(lp4
F4.536938171386719
aF4.451001281738281
aF4.3603512573242185
aF4.274380493164062
aF4.182927551269532
aF4.086351013183593
aF3.99536376953125
aF3.901372375488281
aF3.7947592163085937
aF3.686287841796875
aF3.578971252441406
aF3.53521728515625
aF3.442338562011719
aF3.360706481933594
aF3.2801043701171877
aF3.2353070068359373
aF3.1661627197265627
aF3.1136627197265625
aF3.074255065917969
aF3.0372119140625
aF2.986475524902344
aF2.952278747558594
aF2.9586337280273436
aF2.9131240844726562
aF2.896478271484375
aF2.880576171875
aF2.8696771240234376
aF2.834059753417969
aF2.8264901733398435
aF2.7943826293945313
aF2.791383056640625
aF2.7682843017578125
aF2.7521047973632813
aF2.7477713012695313
aF2.741455383300781
aF2.7176528930664063
aF2.7248263549804688
aF2.7103515625
aF2.6816326904296877
aF2.6842611694335936
aF2.6883428955078124
aF2.6899432373046874
aF2.660479736328125
aF2.6542474365234376
aF2.6383050537109374
aF2.6304800415039065
aF2.6459085083007814
aF2.65107666015625
aF2.618896179199219
aF2.6235952758789063
aF2.615133972167969
aF2.6128314208984373
aF2.5937322998046874
aF2.5901834106445314
aF2.598289794921875
aF2.6007965087890623
aF2.5995819091796877
aF2.5783096313476563
aF2.565662841796875
aF2.5857781982421875
aF2.562283630371094
aF2.580565185546875
aF2.575535888671875
aF2.5594775390625
aF2.563049011230469
aF2.578657531738281
aF2.5628646850585937
aF2.5613638305664064
aF2.5629425048828125
aF2.544029541015625
aF2.5522592163085935
aF2.544950256347656
aF2.536598358154297
aF2.544963531494141
aF2.515455474853516
aF2.5493624877929686
aF2.523551788330078
aF2.5210891723632813
aF2.522908020019531
aF2.514198913574219
aF2.5140345764160155
aF2.518910369873047
aF2.533805694580078
aF2.523576202392578
aF2.5183392333984376
aF2.505967864990234
aF2.5082942199707032
aF2.5117697143554687
aF2.49339599609375
aF2.5101028442382813
aF2.516432800292969
aF2.4966880798339846
aF2.5011868286132812
aF2.507403564453125
aF2.4931727600097657
aF2.487898864746094
aF2.492796630859375
aF2.502848358154297
aF2.4836859130859374
aF2.470919494628906
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0007306500163193395
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 49s'
p10
sS'final_test_loss'
p11
F2.470919494628906
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xce\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.