(dp0
S'train_loss'
p1
(lp2
F4.564481811523438
aF4.4756005859375
aF4.387781066894531
aF4.286931762695312
aF4.185175170898438
aF4.080254821777344
aF3.9665374755859375
aF3.8563775634765625
aF3.7237924194335936
aF3.631355895996094
aF3.513599548339844
aF3.4227975463867186
aF3.346088562011719
aF3.2646530151367186
aF3.21015380859375
aF3.16270263671875
aF3.1187405395507812
aF3.093730163574219
aF3.04145263671875
aF3.028833923339844
aF2.989602355957031
aF2.968617248535156
aF2.947773742675781
aF2.945838623046875
aF2.8992489624023436
aF2.9033322143554687
aF2.8590957641601564
aF2.8689010620117186
aF2.8238775634765627
aF2.8245855712890626
aF2.829461669921875
aF2.810466613769531
aF2.782212829589844
aF2.815757141113281
aF2.738016357421875
aF2.7697103881835936
aF2.7573870849609374
aF2.7406326293945313
aF2.739729919433594
aF2.7155999755859375
aF2.721013488769531
aF2.689955139160156
aF2.6803109741210935
aF2.6893670654296873
aF2.676497497558594
aF2.6510125732421876
aF2.6565679931640624
aF2.6696484375
aF2.6407046508789063
aF2.6259521484375
aF2.619058837890625
aF2.6214453125
aF2.6037088012695313
aF2.61028564453125
aF2.616495361328125
aF2.5912039184570315
aF2.5718682861328124
aF2.596244201660156
aF2.5660687255859376
aF2.576216735839844
aF2.56282958984375
aF2.5621759033203126
aF2.5548402404785158
aF2.5640219116210936
aF2.553542175292969
aF2.542566833496094
aF2.5329591369628908
aF2.5277195739746094
aF2.5027366638183595
aF2.5175091552734377
aF2.500041961669922
aF2.504452972412109
aF2.499281921386719
aF2.5103689575195314
aF2.494539947509766
aF2.493247528076172
aF2.4716310119628906
aF2.474466094970703
aF2.475637664794922
aF2.4659642028808593
aF2.466569671630859
aF2.4625868225097656
aF2.4679541015625
aF2.475977783203125
aF2.4522357177734375
aF2.4541867065429686
aF2.4684898376464846
aF2.4440621948242187
aF2.447692413330078
aF2.441469421386719
aF2.460080261230469
aF2.4396726989746096
aF2.411343994140625
aF2.443526611328125
aF2.415242004394531
aF2.4283592224121096
aF2.4282891845703123
aF2.404541320800781
aF2.4198089599609376
aF2.4104209899902345
asS'test_loss'
p3
(lp4
F4.475221557617187
aF4.378458862304687
aF4.2853701782226565
aF4.183717041015625
aF4.085215759277344
aF3.972230224609375
aF3.8624838256835936
aF3.7231982421875
aF3.6139645385742187
aF3.5065133666992185
aF3.404690856933594
aF3.334009094238281
aF3.260661926269531
aF3.1841226196289063
aF3.1572564697265624
aF3.091103820800781
aF3.0795794677734376
aF3.0106686401367186
aF2.998390808105469
aF3.0019586181640623
aF2.975096435546875
aF2.959713134765625
aF2.8968804931640624
aF2.9085903930664063
aF2.9029617309570312
aF2.8813372802734376
aF2.8652029418945313
aF2.8357882690429688
aF2.8186431884765626
aF2.813349304199219
aF2.7636264038085936
aF2.7722442626953123
aF2.7793350219726562
aF2.7555386352539064
aF2.7433792114257813
aF2.7416732788085936
aF2.7365011596679687
aF2.7170401000976563
aF2.708857116699219
aF2.7026095581054688
aF2.6684552001953126
aF2.67485107421875
aF2.681924133300781
aF2.6591705322265624
aF2.667978515625
aF2.6382687377929686
aF2.661227111816406
aF2.6068161010742186
aF2.63411865234375
aF2.598468017578125
aF2.60183837890625
aF2.6120974731445314
aF2.5997689819335936
aF2.614217529296875
aF2.5789700317382813
aF2.600056457519531
aF2.5824899291992187
aF2.580828857421875
aF2.5731710815429687
aF2.555634002685547
aF2.5402488708496094
aF2.56186279296875
aF2.541156463623047
aF2.5366397094726563
aF2.55613037109375
aF2.5306004333496093
aF2.5444987487792967
aF2.497896881103516
aF2.509640350341797
aF2.497960968017578
aF2.4982916259765626
aF2.498948669433594
aF2.4954893493652346
aF2.497815704345703
aF2.4916436767578123
aF2.4904360961914063
aF2.4756344604492186
aF2.4680763244628907
aF2.4612252807617185
aF2.4724668884277343
aF2.4641958618164064
aF2.4700308227539063
aF2.455702209472656
aF2.451298828125
aF2.426866912841797
aF2.4427076721191407
aF2.422948303222656
aF2.4234243774414064
aF2.430260009765625
aF2.424776611328125
aF2.4298051452636718
aF2.415923614501953
aF2.419280242919922
aF2.4105709838867186
aF2.4187542724609377
aF2.4056295776367187
aF2.4024244689941407
aF2.408156433105469
aF2.414283447265625
aF2.4117280578613283
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.003551028589786483
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 3s'
p10
sS'final_test_loss'
p11
F2.4117280578613283
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'9\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.