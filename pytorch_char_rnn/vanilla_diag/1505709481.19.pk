(dp0
S'train_loss'
p1
(lp2
F4.628219909667969
aF4.347654113769531
aF4.089219055175781
aF3.857085876464844
aF3.6367724609375
aF3.4623553466796877
aF3.3109393310546875
aF3.1706887817382814
aF3.0667739868164063
aF3.003478698730469
aF2.9128659057617186
aF2.867748107910156
aF2.7764791870117187
aF2.785399169921875
aF2.7516387939453124
aF2.7016470336914065
aF2.6691900634765626
aF2.6710220336914063
aF2.6480450439453125
aF2.6175128173828126
aF2.6410586547851564
aF2.6224740600585936
aF2.5882830810546875
aF2.6125198364257813
aF2.5966796875
aF2.57273193359375
aF2.567559814453125
aF2.5736309814453127
aF2.563492431640625
aF2.5563356018066408
aF2.5459490966796876
aF2.546125030517578
aF2.53588623046875
aF2.5216629028320314
aF2.5302684020996096
aF2.5301727294921874
aF2.520322113037109
aF2.5171405029296876
aF2.495345458984375
aF2.5033045959472657
aF2.504049987792969
aF2.502876434326172
aF2.499020233154297
aF2.484765930175781
aF2.5113612365722657
aF2.462137908935547
aF2.4845782470703126
aF2.472319793701172
aF2.478086395263672
aF2.4699395751953124
aF2.477624816894531
aF2.4897190856933595
aF2.474063720703125
aF2.4719139099121095
aF2.4784666442871095
aF2.456848602294922
aF2.464308013916016
aF2.4534959411621093
aF2.4749029541015624
aF2.448374786376953
aF2.458734436035156
aF2.4674301147460938
aF2.4583509826660155
aF2.4545550537109375
aF2.4479473876953124
aF2.456700439453125
aF2.454505615234375
aF2.4482118225097658
aF2.428480529785156
aF2.4352630615234374
aF2.4222703552246094
aF2.4185757446289062
aF2.4254997253417967
aF2.437531280517578
aF2.4332493591308593
aF2.421280975341797
aF2.4263992309570312
aF2.431185760498047
aF2.4215408325195313
aF2.435793914794922
aF2.4151046752929686
aF2.4118988037109377
aF2.404285736083984
aF2.413121337890625
aF2.407713623046875
aF2.390968933105469
aF2.399563903808594
aF2.3887379455566404
aF2.3829710388183596
aF2.387107391357422
aF2.3940000915527344
aF2.378447723388672
aF2.3819125366210936
aF2.3836370849609376
aF2.390086822509766
aF2.3907171630859376
aF2.3558843994140624
aF2.3801776123046876
aF2.3887188720703123
aF2.3794149780273437
asS'test_loss'
p3
(lp4
F4.3445523071289065
aF4.086607971191406
aF3.845618896484375
aF3.638330993652344
aF3.4830441284179687
aF3.293132629394531
aF3.1799365234375
aF3.075965576171875
aF2.978486328125
aF2.90582275390625
aF2.8402410888671876
aF2.803175354003906
aF2.7611203002929687
aF2.7301446533203126
aF2.7005462646484375
aF2.6959988403320314
aF2.6754885864257814
aF2.6493594360351564
aF2.631352844238281
aF2.6321722412109376
aF2.5990524291992188
aF2.5840585327148435
aF2.57880615234375
aF2.5779156494140625
aF2.5819970703125
aF2.570912170410156
aF2.55236083984375
aF2.541674041748047
aF2.5460009765625
aF2.528983459472656
aF2.520053253173828
aF2.5168629455566407
aF2.5133863830566407
aF2.5160890197753907
aF2.518016052246094
aF2.5119371032714843
aF2.514449157714844
aF2.4834420776367185
aF2.4982725524902345
aF2.496815948486328
aF2.5019764709472656
aF2.479910430908203
aF2.4813780212402343
aF2.479334716796875
aF2.488824462890625
aF2.484810791015625
aF2.4822456359863283
aF2.473535919189453
aF2.462991180419922
aF2.4504998779296874
aF2.466425018310547
aF2.4616104125976563
aF2.4594241333007814
aF2.4720645141601563
aF2.448262939453125
aF2.466686248779297
aF2.44472900390625
aF2.4635816955566407
aF2.441526336669922
aF2.432963409423828
aF2.4476283264160155
aF2.447420654296875
aF2.4588351440429688
aF2.426937255859375
aF2.4282516479492187
aF2.4348081970214843
aF2.4380953979492186
aF2.444436340332031
aF2.434276885986328
aF2.43755126953125
aF2.4175064086914064
aF2.418538818359375
aF2.421123809814453
aF2.442757263183594
aF2.4154931640625
aF2.4240653991699217
aF2.4128901672363283
aF2.396951141357422
aF2.4063987731933594
aF2.4149185180664063
aF2.3911837768554687
aF2.3986856079101564
aF2.395006561279297
aF2.392241668701172
aF2.398037567138672
aF2.382584991455078
aF2.3995452880859376
aF2.386241912841797
aF2.3701937866210936
aF2.3931072998046874
aF2.3702406311035156
aF2.3885577392578123
aF2.3806639099121094
aF2.3620912170410158
aF2.369794921875
aF2.372156982421875
aF2.3589987182617187
aF2.3620506286621095
aF2.3815487670898436
aF2.365644073486328
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0027860281016015126
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 9s'
p10
sS'final_test_loss'
p11
F2.365644073486328
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xbc\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.