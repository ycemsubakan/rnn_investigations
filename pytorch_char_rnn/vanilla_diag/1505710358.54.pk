(dp0
S'train_loss'
p1
(lp2
F4.6278683471679685
aF4.614016723632813
aF4.601239624023438
aF4.59608642578125
aF4.586077270507812
aF4.581496276855469
aF4.5714810180664065
aF4.557318420410156
aF4.549841613769531
aF4.5385400390625
aF4.52706787109375
aF4.523136291503906
aF4.511157531738281
aF4.504367065429688
aF4.490968627929687
aF4.483829956054688
aF4.4744570922851565
aF4.463115539550781
aF4.454164428710937
aF4.445101928710938
aF4.441339111328125
aF4.428898620605469
aF4.421466979980469
aF4.406289978027344
aF4.397308349609375
aF4.383833923339844
aF4.3761962890625
aF4.371604614257812
aF4.364548950195313
aF4.354569396972656
aF4.351560363769531
aF4.337587280273437
aF4.330941772460937
aF4.304335632324219
aF4.313003845214844
aF4.297896118164062
aF4.287354431152344
aF4.270670166015625
aF4.2694549560546875
aF4.269013977050781
aF4.2451934814453125
aF4.250548706054688
aF4.2400820922851565
aF4.2186062622070315
aF4.212684020996094
aF4.21126708984375
aF4.1959600830078125
aF4.175516052246094
aF4.175360107421875
aF4.164829406738281
aF4.165034484863281
aF4.146695251464844
aF4.134561462402344
aF4.125700988769531
aF4.112977294921875
aF4.121806945800781
aF4.101929626464844
aF4.096085510253906
aF4.074200439453125
aF4.073031616210938
aF4.061206359863281
aF4.038723449707032
aF4.0309521484375
aF4.024214782714844
aF4.032325134277344
aF4.0000393676757815
aF4.012936096191407
aF3.9909808349609377
aF3.982305603027344
aF3.9694464111328127
aF3.9593685913085936
aF3.942246398925781
aF3.9279251098632812
aF3.9190164184570313
aF3.92313720703125
aF3.925274658203125
aF3.8993771362304686
aF3.893487548828125
aF3.8781228637695313
aF3.8684097290039063
aF3.8540048217773437
aF3.854556884765625
aF3.83848876953125
aF3.8324551391601562
aF3.8128277587890627
aF3.805262451171875
aF3.786582336425781
aF3.7950204467773436
aF3.7928271484375
aF3.7709552001953126
aF3.765054016113281
aF3.7351141357421875
aF3.7404824829101564
aF3.7215390014648437
aF3.7230050659179685
aF3.6924365234375
aF3.7043746948242187
aF3.6920025634765623
aF3.670765075683594
aF3.681401672363281
asS'test_loss'
p3
(lp4
F4.611412048339844
aF4.603945922851563
aF4.593750915527344
aF4.584653625488281
aF4.57407958984375
aF4.564151916503906
aF4.561622009277344
aF4.552254638671875
aF4.537733459472657
aF4.527556457519531
aF4.517729797363281
aF4.508974304199219
aF4.502513427734375
aF4.49474365234375
aF4.4832675170898435
aF4.4766064453125
aF4.462741394042968
aF4.454006042480469
aF4.446944580078125
aF4.435951232910156
aF4.427158203125
aF4.420613403320313
aF4.403685302734375
aF4.397577514648438
aF4.392133178710938
aF4.380078125
aF4.374087829589843
aF4.358591918945312
aF4.352065124511719
aF4.336846923828125
aF4.333040466308594
aF4.3248779296875
aF4.3151483154296875
aF4.30688232421875
aF4.294755554199218
aF4.289481201171875
aF4.282970581054688
aF4.269697265625
aF4.253920593261719
aF4.246238403320312
aF4.237437438964844
aF4.235865173339843
aF4.220005798339844
aF4.209022827148438
aF4.203672790527344
aF4.1870050048828125
aF4.1821826171875
aF4.173854675292969
aF4.166662902832031
aF4.16056640625
aF4.148214721679688
aF4.128954467773437
aF4.1310336303710935
aF4.115049743652344
aF4.112289733886719
aF4.093135681152344
aF4.085253295898437
aF4.076244201660156
aF4.074818725585938
aF4.058656921386719
aF4.056741333007812
aF4.0313043212890625
aF4.024463195800781
aF4.012734985351562
aF4.019977416992187
aF3.9834115600585935
aF3.9880056762695313
aF3.974115905761719
aF3.9732205200195314
aF3.9569921875
aF3.9432623291015627
aF3.92699951171875
aF3.9206640625
aF3.9295944213867187
aF3.9008212280273438
aF3.8906826782226562
aF3.881267395019531
aF3.882585144042969
aF3.8618524169921873
aF3.8503964233398436
aF3.8490667724609375
aF3.8352420043945314
aF3.8280368041992188
aF3.834250183105469
aF3.7830731201171877
aF3.8078759765625
aF3.7868222045898436
aF3.7708770751953127
aF3.7717520141601564
aF3.735651550292969
aF3.7381460571289065
aF3.73400146484375
aF3.728865966796875
aF3.7181243896484375
aF3.7142269897460936
aF3.6993887329101565
aF3.6939202880859376
aF3.6865914916992186
aF3.6588168334960938
aF3.670900573730469
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00013494156710624211
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 55s'
p10
sS'final_test_loss'
p11
F3.670900573730469
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\x90\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.