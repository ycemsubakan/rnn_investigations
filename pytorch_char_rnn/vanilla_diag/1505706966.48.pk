(dp0
S'train_loss'
p1
(lp2
F4.644513244628906
aF4.628096313476562
aF4.605885009765625
aF4.581202392578125
aF4.5604855346679685
aF4.537894287109375
aF4.519733276367187
aF4.500233154296875
aF4.478551940917969
aF4.455656433105469
aF4.4369027709960935
aF4.413546142578125
aF4.398731079101562
aF4.378434448242188
aF4.354034423828125
aF4.331382751464844
aF4.318546752929688
aF4.2904730224609375
aF4.271441345214844
aF4.250973205566407
aF4.229252014160156
aF4.2068359375
aF4.186607055664062
aF4.165986022949219
aF4.152340698242187
aF4.123934020996094
aF4.092221984863281
aF4.0775830078125
aF4.042482604980469
aF4.037018432617187
aF4.004950256347656
aF3.993152770996094
aF3.9753057861328127
aF3.9477215576171876
aF3.9356869506835936
aF3.9034695434570312
aF3.8861761474609375
aF3.858985595703125
aF3.8395281982421876
aF3.8070523071289064
aF3.803014221191406
aF3.7657296752929685
aF3.7639566040039063
aF3.7494122314453127
aF3.70623779296875
aF3.685894775390625
aF3.6682913208007815
aF3.6397833251953124
aF3.6249813842773437
aF3.580976867675781
aF3.591885986328125
aF3.5553054809570312
aF3.5256048583984376
aF3.511252746582031
aF3.487020263671875
aF3.48464599609375
aF3.451837463378906
aF3.438970031738281
aF3.4170822143554687
aF3.3731719970703127
aF3.379649963378906
aF3.3556915283203126
aF3.359090576171875
aF3.3224368286132813
aF3.317554626464844
aF3.2762347412109376
aF3.2488189697265626
aF3.260841064453125
aF3.2077520751953124
aF3.22096435546875
aF3.1936160278320314
aF3.1928564453125
aF3.178957824707031
aF3.16007080078125
aF3.150108642578125
aF3.1298452758789064
aF3.1210833740234376
aF3.097271728515625
aF3.0692977905273438
aF3.1029351806640624
aF3.0906130981445314
aF3.0549588012695312
aF3.0381204223632814
aF3.056133117675781
aF3.040327453613281
aF3.0254669189453125
aF3.0043756103515626
aF2.9990167236328125
aF2.9904934692382814
aF2.9841159057617186
aF2.9779510498046875
aF2.94498291015625
aF2.9607177734375
aF2.948446960449219
aF2.9474212646484377
aF2.92563232421875
aF2.9093673706054686
aF2.901611633300781
aF2.9009518432617187
aF2.88773193359375
asS'test_loss'
p3
(lp4
F4.625338134765625
aF4.60213623046875
aF4.581908569335938
aF4.559767150878907
aF4.53975830078125
aF4.522030029296875
aF4.496489868164063
aF4.478516540527344
aF4.453228759765625
aF4.433579406738281
aF4.415159606933594
aF4.392616882324218
aF4.373782958984375
aF4.35544921875
aF4.326396179199219
aF4.3058935546875
aF4.288268432617188
aF4.2731390380859375
aF4.243861999511719
aF4.220818786621094
aF4.205262145996094
aF4.178247985839843
aF4.164961547851562
aF4.146766967773438
aF4.11920166015625
aF4.101414794921875
aF4.082036437988282
aF4.052938537597656
aF4.040621643066406
aF4.017647399902343
aF3.997293701171875
aF3.960221252441406
aF3.9429754638671874
aF3.9191940307617186
aF3.9016159057617186
aF3.8786376953125
aF3.8591082763671873
aF3.8329791259765624
aF3.8133578491210938
aF3.796923828125
aF3.7760467529296875
aF3.7405960083007814
aF3.7274050903320313
aF3.692705078125
aF3.6857733154296874
aF3.666239929199219
aF3.639118347167969
aF3.6041073608398437
aF3.6133099365234376
aF3.58302734375
aF3.568475646972656
aF3.5349325561523437
aF3.4923486328125
aF3.4916647338867186
aF3.4742007446289063
aF3.4495162963867188
aF3.4257598876953126
aF3.40059814453125
aF3.391004638671875
aF3.3499346923828126
aF3.3518310546875
aF3.3338043212890627
aF3.306221618652344
aF3.302366943359375
aF3.303599548339844
aF3.2720108032226562
aF3.240072021484375
aF3.238838806152344
aF3.213963928222656
aF3.191630554199219
aF3.182303161621094
aF3.16565185546875
aF3.1399606323242186
aF3.12296142578125
aF3.11380615234375
aF3.1180511474609376
aF3.09632568359375
aF3.0947930908203123
aF3.0585733032226563
aF3.04486083984375
aF3.048350524902344
aF3.036482849121094
aF3.0407232666015624
aF3.0059100341796876
aF3.029215087890625
aF3.0101837158203124
aF2.9822573852539063
aF2.9680245971679686
aF2.9809536743164062
aF2.9623187255859373
aF2.963961486816406
aF2.949427490234375
aF2.919710693359375
aF2.924482727050781
aF2.9321270751953126
aF2.8990521240234375
aF2.90296142578125
aF2.890879821777344
aF2.911354675292969
aF2.884468688964844
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0002462778628502058
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 58s'
p10
sS'final_test_loss'
p11
F2.884468688964844
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xa6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.