(dp0
S'train_loss'
p1
(lp2
F4.636684875488282
aF4.4862823486328125
aF4.344895935058593
aF4.200702209472656
aF4.071338195800781
aF3.932966613769531
aF3.7863027954101565
aF3.6787689208984373
aF3.550337829589844
aF3.4278945922851562
aF3.327905578613281
aF3.256571960449219
aF3.1369149780273435
aF3.081537780761719
aF3.0356625366210936
aF2.947063293457031
aF2.92985595703125
aF2.915274963378906
aF2.8543377685546876
aF2.8273715209960937
aF2.8193646240234376
aF2.770416564941406
aF2.7630877685546875
aF2.7309716796875
aF2.7501641845703126
aF2.7441143798828125
aF2.71520263671875
aF2.68631591796875
aF2.6679443359375
aF2.673018493652344
aF2.6424493408203125
aF2.631513977050781
aF2.620441589355469
aF2.629894104003906
aF2.6255899047851563
aF2.617373046875
aF2.6183416748046877
aF2.5904141235351563
aF2.593994445800781
aF2.5787692260742188
aF2.5556684875488282
aF2.5528627014160157
aF2.5465487670898437
aF2.555866394042969
aF2.538471984863281
aF2.5345423889160155
aF2.5523455810546873
aF2.550503997802734
aF2.5558682250976563
aF2.53595703125
aF2.5317388916015626
aF2.522662353515625
aF2.5068589782714845
aF2.5207904052734373
aF2.515239410400391
aF2.525065612792969
aF2.505307922363281
aF2.515096893310547
aF2.5015690612792967
aF2.508996887207031
aF2.507268371582031
aF2.4912579345703123
aF2.4935928344726563
aF2.502133483886719
aF2.495061950683594
aF2.5060494995117186
aF2.4793241882324217
aF2.496450653076172
aF2.4925631713867187
aF2.4857423400878904
aF2.4886213684082032
aF2.4806362915039064
aF2.484805450439453
aF2.4822645568847657
aF2.46139892578125
aF2.460165252685547
aF2.449437255859375
aF2.4650559997558594
aF2.4509483337402345
aF2.472729034423828
aF2.4535533142089845
aF2.4393370056152346
aF2.461344451904297
aF2.4518524169921876
aF2.4430497741699218
aF2.4407525634765626
aF2.436602020263672
aF2.4572993469238282
aF2.4289897155761717
aF2.42785400390625
aF2.442493743896484
aF2.4380596923828124
aF2.4362449645996094
aF2.423478240966797
aF2.4348405456542968
aF2.433756866455078
aF2.44341064453125
aF2.4387142944335936
aF2.4092193603515626
aF2.4041683959960936
asS'test_loss'
p3
(lp4
F4.481008605957031
aF4.3441162109375
aF4.200689697265625
aF4.062025451660157
aF3.9188348388671876
aF3.7780972290039063
aF3.665141296386719
aF3.5551788330078127
aF3.4114199829101564
aF3.323050842285156
aF3.2358392333984374
aF3.1564761352539064
aF3.0802288818359376
aF3.02395263671875
aF2.957948913574219
aF2.9178057861328126
aF2.8804315185546874
aF2.833599853515625
aF2.8084490966796873
aF2.8027023315429687
aF2.7852056884765624
aF2.7572036743164063
aF2.751134948730469
aF2.725694274902344
aF2.702086181640625
aF2.6807427978515626
aF2.6880377197265624
aF2.66495849609375
aF2.6554940795898436
aF2.6725473022460937
aF2.6567059326171876
aF2.6275115966796876
aF2.614432067871094
aF2.603697509765625
aF2.6089205932617188
aF2.582422790527344
aF2.5930386352539063
aF2.591943664550781
aF2.56399658203125
aF2.5537750244140627
aF2.5696533203125
aF2.5523756408691405
aF2.548719482421875
aF2.5315425109863283
aF2.551324920654297
aF2.5365940856933595
aF2.51198486328125
aF2.521481628417969
aF2.5236968994140625
aF2.517786102294922
aF2.4964334106445314
aF2.5224522399902343
aF2.5128843688964846
aF2.5052534484863282
aF2.5046675109863283
aF2.506150817871094
aF2.498022155761719
aF2.498270263671875
aF2.494990234375
aF2.4935708618164063
aF2.5078367614746093
aF2.4953262329101564
aF2.4947605895996094
aF2.491952362060547
aF2.506856231689453
aF2.4975794982910156
aF2.471136474609375
aF2.481239776611328
aF2.4793714904785156
aF2.4752345275878906
aF2.475563659667969
aF2.4720553588867187
aF2.484288635253906
aF2.461160125732422
aF2.4825477600097656
aF2.4541371154785154
aF2.458654022216797
aF2.456534881591797
aF2.4606634521484376
aF2.452444763183594
aF2.4640298461914063
aF2.457506103515625
aF2.455690460205078
aF2.435872344970703
aF2.4343443298339844
aF2.4417573547363283
aF2.4348960876464845
aF2.4343606567382814
aF2.437685241699219
aF2.447922821044922
aF2.426036529541016
aF2.436989593505859
aF2.4175917053222657
aF2.4020729064941406
aF2.444923858642578
aF2.447842254638672
aF2.4132418823242188
aF2.4287928771972656
aF2.406452484130859
aF2.4188035583496093
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.002756593760856885
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 51s'
p10
sS'final_test_loss'
p11
F2.4188035583496093
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S's\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.