(dp0
S'train_loss'
p1
(lp2
F4.619015808105469
aF4.6010873413085935
aF4.585022583007812
aF4.564548950195313
aF4.547674255371094
aF4.529775390625
aF4.51318359375
aF4.494992980957031
aF4.479090576171875
aF4.457904968261719
aF4.441401977539062
aF4.41810546875
aF4.401479797363281
aF4.386968383789062
aF4.366265869140625
aF4.353964233398438
aF4.332926025390625
aF4.3121142578125
aF4.289224243164062
aF4.265469970703125
aF4.249559936523437
aF4.2254095458984375
aF4.2049267578125
aF4.1839447021484375
aF4.1615298461914065
aF4.135761108398437
aF4.12330322265625
aF4.095223388671875
aF4.076553955078125
aF4.044794921875
aF4.021256713867188
aF3.9893072509765624
aF3.9656732177734373
aF3.9497381591796876
aF3.9057733154296876
aF3.901033935546875
aF3.865661315917969
aF3.8383642578125
aF3.811747741699219
aF3.7932110595703126
aF3.74977783203125
aF3.719632873535156
aF3.699077453613281
aF3.6716604614257813
aF3.6505697631835936
aF3.606376037597656
aF3.58562255859375
aF3.5698394775390625
aF3.5371197509765624
aF3.508033447265625
aF3.4802682495117185
aF3.464203186035156
aF3.4261834716796873
aF3.3924716186523436
aF3.3781478881835936
aF3.3694497680664064
aF3.3443917846679687
aF3.3095098876953126
aF3.3016082763671877
aF3.270507507324219
aF3.243770751953125
aF3.219111022949219
aF3.2342547607421874
aF3.1832901000976563
aF3.1763970947265623
aF3.2012335205078126
aF3.1583047485351563
aF3.143077087402344
aF3.1201129150390625
aF3.0964816284179686
aF3.0755014038085937
aF3.0928436279296876
aF3.063096008300781
aF3.045565185546875
aF3.045737609863281
aF3.016013488769531
aF3.0390530395507813
aF3.0208343505859374
aF3.0052059936523436
aF2.98396728515625
aF2.9839205932617188
aF2.9858444213867186
aF2.965401306152344
aF2.9825259399414064
aF2.9641995239257812
aF2.9661788940429688
aF2.9431585693359374
aF2.945917053222656
aF2.9160226440429686
aF2.942410888671875
aF2.9425320434570312
aF2.90552978515625
aF2.8988027954101563
aF2.8938531494140625
aF2.9141366577148435
aF2.8990206909179688
aF2.904296875
aF2.8924966430664063
aF2.898629455566406
aF2.873291320800781
asS'test_loss'
p3
(lp4
F4.6013116455078125
aF4.582380676269532
aF4.567068176269531
aF4.548666381835938
aF4.529291687011718
aF4.509309692382812
aF4.494342956542969
aF4.476543884277344
aF4.459492492675781
aF4.440993041992187
aF4.421190185546875
aF4.402058715820313
aF4.3846240234375
aF4.363389282226563
aF4.348565979003906
aF4.32501708984375
aF4.306573791503906
aF4.286968078613281
aF4.263219299316407
aF4.241580810546875
aF4.221956176757812
aF4.204884033203125
aF4.177621459960937
aF4.153393859863281
aF4.131978454589844
aF4.118254699707031
aF4.0935580444335935
aF4.072744445800781
aF4.047578735351562
aF4.02111328125
aF3.9994354248046875
aF3.9697308349609375
aF3.9305047607421875
aF3.922474670410156
aF3.8915029907226564
aF3.8711810302734375
aF3.8364224243164062
aF3.795672607421875
aF3.790063781738281
aF3.755625
aF3.730421447753906
aF3.6906008911132813
aF3.669775390625
aF3.6287042236328126
aF3.6218072509765626
aF3.587435302734375
aF3.54863037109375
aF3.53628662109375
aF3.5051284790039063
aF3.4867529296875
aF3.4794912719726563
aF3.4366937255859376
aF3.392435302734375
aF3.3684588623046876
aF3.352020568847656
aF3.3138717651367187
aF3.3147598266601563
aF3.2758712768554688
aF3.2511660766601564
aF3.243441162109375
aF3.2174533081054686
aF3.2083746337890626
aF3.1789340209960937
aF3.1658587646484375
aF3.157593078613281
aF3.120072021484375
aF3.1317498779296873
aF3.1019970703125
aF3.062572326660156
aF3.093059387207031
aF3.0836911010742187
aF3.034549560546875
aF3.038878173828125
aF3.0636605834960937
aF3.054068603515625
aF3.0209085083007814
aF3.009593811035156
aF3.0114617919921876
aF2.9991494750976564
aF2.9766156005859377
aF2.977174987792969
aF2.9587362670898436
aF2.9804702758789063
aF2.9391925048828127
aF2.922762145996094
aF2.912991638183594
aF2.9152587890625
aF2.92757080078125
aF2.9123489379882814
aF2.9080841064453127
aF2.883089599609375
aF2.8885159301757812
aF2.908985290527344
aF2.897974548339844
aF2.886901550292969
aF2.87591064453125
aF2.8903570556640625
aF2.8735928344726562
aF2.8549664306640623
aF2.8616534423828126
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00014195596893191027
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 56s'
p10
sS'final_test_loss'
p11
F2.8616534423828126
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh_diag'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe7\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.