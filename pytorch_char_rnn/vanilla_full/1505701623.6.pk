(dp0
S'train_loss'
p1
(lp2
F4.610479736328125
aF4.595608520507812
aF4.577518005371093
aF4.561185302734375
aF4.542657165527344
aF4.527052917480469
aF4.509889526367187
aF4.490877990722656
aF4.475215148925781
aF4.456115112304688
aF4.437986755371094
aF4.416693420410156
aF4.400279235839844
aF4.379581909179688
aF4.352431640625
aF4.341072082519531
aF4.309983520507813
aF4.289743041992187
aF4.265749206542969
aF4.234140014648437
aF4.216617431640625
aF4.186125793457031
aF4.147351379394531
aF4.119462890625
aF4.091831359863281
aF4.052490234375
aF4.025890808105469
aF3.9882177734375
aF3.963516845703125
aF3.935631103515625
aF3.902390441894531
aF3.842196350097656
aF3.8350735473632813
aF3.797300109863281
aF3.7583328247070313
aF3.722174072265625
aF3.6775238037109377
aF3.6761383056640624
aF3.649932556152344
aF3.6274578857421873
aF3.5975271606445314
aF3.593324890136719
aF3.5515380859375
aF3.5496450805664064
aF3.51934326171875
aF3.516611328125
aF3.5024298095703124
aF3.5119400024414062
aF3.4666748046875
aF3.46335693359375
aF3.4658938598632814
aF3.4493118286132813
aF3.4232186889648437
aF3.4265618896484376
aF3.4027536010742185
aF3.4098712158203126
aF3.394422607421875
aF3.3769268798828125
aF3.353848571777344
aF3.3591964721679686
aF3.3417889404296877
aF3.3601239013671873
aF3.3585302734375
aF3.2887033081054686
aF3.3096688842773436
aF3.3115496826171875
aF3.3147015380859375
aF3.27523193359375
aF3.2990451049804688
aF3.259920349121094
aF3.237357482910156
aF3.2434317016601564
aF3.233096923828125
aF3.228504638671875
aF3.2509808349609375
aF3.220509033203125
aF3.2327764892578124
aF3.220032043457031
aF3.210660705566406
aF3.224489440917969
aF3.181864013671875
aF3.2001116943359373
aF3.1806826782226563
aF3.1648297119140625
aF3.1500469970703127
aF3.1660394287109375
aF3.1663058471679686
aF3.1603683471679687
aF3.1685446166992186
aF3.137071228027344
aF3.1431155395507813
aF3.1396026611328125
aF3.1156671142578123
aF3.118924560546875
aF3.0962017822265624
aF3.1058221435546876
aF3.089680480957031
aF3.087973327636719
aF3.072514953613281
aF3.0863162231445314
asS'test_loss'
p3
(lp4
F4.593587646484375
aF4.578426513671875
aF4.560550842285156
aF4.542530822753906
aF4.525309143066406
aF4.509070739746094
aF4.493108825683594
aF4.477173461914062
aF4.457011108398437
aF4.442718505859375
aF4.419491271972657
aF4.396161499023438
aF4.378788452148438
aF4.355766906738281
aF4.333783569335938
aF4.311499328613281
aF4.289467468261718
aF4.272464599609375
aF4.233724975585938
aF4.207490844726562
aF4.177933349609375
aF4.1550146484375
aF4.1280606079101565
aF4.091738891601563
aF4.061936950683593
aF4.017905578613282
aF3.9890097045898436
aF3.953266906738281
aF3.9169512939453126
aF3.8905734252929687
aF3.857771911621094
aF3.8284561157226564
aF3.780631103515625
aF3.7648519897460937
aF3.7262112426757814
aF3.691790771484375
aF3.6726583862304687
aF3.64005615234375
aF3.6159988403320313
aF3.6005059814453126
aF3.5840032958984374
aF3.5668380737304686
aF3.53770263671875
aF3.521404113769531
aF3.502803955078125
aF3.4828668212890626
aF3.458331298828125
aF3.4771160888671875
aF3.459248352050781
aF3.461603698730469
aF3.451973876953125
aF3.423704833984375
aF3.39622314453125
aF3.4079403686523437
aF3.383300476074219
aF3.4005276489257814
aF3.3837594604492187
aF3.3516033935546874
aF3.337885437011719
aF3.3408047485351564
aF3.3513006591796874
aF3.32918701171875
aF3.316435546875
aF3.299222412109375
aF3.2975726318359375
aF3.2893426513671873
aF3.283157958984375
aF3.2684246826171877
aF3.275107421875
aF3.25500732421875
aF3.24540283203125
aF3.2311856079101564
aF3.23583740234375
aF3.235751037597656
aF3.2119110107421873
aF3.212884826660156
aF3.223757629394531
aF3.189606018066406
aF3.181310119628906
aF3.1802227783203123
aF3.183943176269531
aF3.1736770629882813
aF3.17967529296875
aF3.1465411376953125
aF3.159445495605469
aF3.1412899780273436
aF3.143407287597656
aF3.143868103027344
aF3.1361248779296873
aF3.123594970703125
aF3.1279412841796876
aF3.1156060791015623
aF3.120657958984375
aF3.1098095703125
aF3.126784362792969
aF3.0712789916992187
aF3.0933831787109374
aF3.106431884765625
aF3.0792706298828123
aF3.0777099609375
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0001106005071572955
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 46s'
p10
sS'final_test_loss'
p11
F3.0777099609375
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb2\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.