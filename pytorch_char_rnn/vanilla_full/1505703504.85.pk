(dp0
S'train_loss'
p1
(lp2
F4.592894287109375
aF4.109201049804687
aF3.5338165283203127
aF3.262063293457031
aF3.091985778808594
aF3.0068728637695314
aF2.9286563110351564
aF2.852924499511719
aF2.7681304931640627
aF2.7469671630859374
aF2.668992919921875
aF2.6497091674804687
aF2.608262939453125
aF2.5989361572265626
aF2.5544612121582033
aF2.5199940490722654
aF2.4835359191894533
aF2.491543731689453
aF2.4733399963378906
aF2.449667510986328
aF2.4167689514160156
aF2.4083740234375
aF2.3696192932128906
aF2.3670216369628907
aF2.3605091857910154
aF2.341672821044922
aF2.3437506103515626
aF2.324632568359375
aF2.3216102600097654
aF2.2992640686035157
aF2.293434295654297
aF2.2865521240234377
aF2.261857452392578
aF2.269675598144531
aF2.2504643249511718
aF2.2432792663574217
aF2.2509114074707033
aF2.2311692810058594
aF2.2361402893066407
aF2.2166127014160155
aF2.2076548767089843
aF2.209154510498047
aF2.2013035583496094
aF2.1867127990722657
aF2.1930767822265627
aF2.177767791748047
aF2.1769921875
aF2.147696228027344
aF2.1621746826171875
aF2.16017822265625
aF2.1363365173339846
aF2.156899261474609
aF2.1254202270507814
aF2.1454225158691407
aF2.097162322998047
aF2.119642791748047
aF2.1073150634765625
aF2.0945806884765625
aF2.1011257934570313
aF2.1156724548339843
aF2.0889454650878907
aF2.0998870849609377
aF2.0663014221191407
aF2.0688320922851564
aF2.0813511657714843
aF2.087540588378906
aF2.0615692138671875
aF2.0771697998046874
aF2.051153564453125
aF2.034606170654297
aF2.0601957702636717
aF2.0569981384277343
aF2.045801544189453
aF2.043679046630859
aF2.042475280761719
aF2.045615997314453
aF2.040654754638672
aF2.0303250122070313
aF2.0124356079101564
aF2.0438572692871095
aF2.017649688720703
aF2.0192909240722656
aF2.018601989746094
aF2.0267762756347656
aF2.005726623535156
aF2.0118234252929685
aF2.0105223083496093
aF1.9925248718261719
aF1.9904632568359375
aF1.984742431640625
aF2.003073425292969
aF1.982835235595703
aF2.00052490234375
aF1.9920542907714844
aF1.979315948486328
aF1.9798600769042969
aF1.9692132568359375
aF1.9772865295410156
aF1.9572314453125
aF1.9620932006835938
asS'test_loss'
p3
(lp4
F4.096597290039062
aF3.5296282958984375
aF3.2700921630859376
aF3.0793051147460937
aF3.0242218017578124
aF2.933797607421875
aF2.868292236328125
aF2.7922012329101564
aF2.7113577270507814
aF2.6632373046875
aF2.661968078613281
aF2.622804870605469
aF2.579362487792969
aF2.540193786621094
aF2.535292510986328
aF2.4918743896484377
aF2.4740562438964844
aF2.4541021728515626
aF2.4643600463867186
aF2.435843811035156
aF2.4030877685546876
aF2.3771878051757813
aF2.3650169372558594
aF2.369417877197266
aF2.3376995849609377
aF2.318459014892578
aF2.3087083435058595
aF2.3168634033203124
aF2.2883787536621094
aF2.278098297119141
aF2.2830816650390626
aF2.26744140625
aF2.2517459106445314
aF2.2593400573730467
aF2.2497023010253905
aF2.2397315979003904
aF2.2206607055664063
aF2.223391265869141
aF2.2282235717773435
aF2.1919931030273436
aF2.189031066894531
aF2.176027374267578
aF2.1845640563964843
aF2.169242401123047
aF2.180472106933594
aF2.1584294128417967
aF2.1506138610839844
aF2.1407730102539064
aF2.1420289611816408
aF2.1489942932128905
aF2.1386851501464843
aF2.1255926513671874
aF2.1251071166992186
aF2.0976654052734376
aF2.1109661865234375
aF2.109413299560547
aF2.1161288452148437
aF2.0870742797851562
aF2.1078111267089845
aF2.0959649658203126
aF2.095079345703125
aF2.065199279785156
aF2.0858172607421874
aF2.066397399902344
aF2.0719212341308593
aF2.050443420410156
aF2.0552186584472656
aF2.0413380432128907
aF2.0543226623535156
aF2.0316950988769533
aF2.0368768310546876
aF2.0313829040527343
aF2.0458195495605467
aF2.0294261169433594
aF2.022086181640625
aF2.0215000915527344
aF2.017250671386719
aF2.0199848937988283
aF2.0366668701171875
aF1.9998130798339844
aF2.0030288696289062
aF2.0210604858398438
aF2.006436004638672
aF2.0035443115234375
aF2.0099943542480467
aF2.0173948669433592
aF1.9962701416015625
aF1.9933900451660156
aF2.0026478576660156
aF2.0081803894042967
aF1.967816162109375
aF1.9953189086914063
aF1.9713174438476562
aF1.99804443359375
aF1.971184539794922
aF1.9647216796875
aF1.9713519287109376
aF1.9542982482910156
aF1.9772378540039062
aF1.9661285400390625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008974315688526632
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 49s'
p10
sS'final_test_loss'
p11
F1.9661285400390625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'Z\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.