(dp0
S'train_loss'
p1
(lp2
F4.6688385009765625
aF4.5895538330078125
aF4.51822021484375
aF4.439344787597657
aF4.365982666015625
aF4.292668151855469
aF4.204256896972656
aF4.1040127563476565
aF4.016119689941406
aF3.90760498046875
aF3.806136474609375
aF3.6947613525390626
aF3.571785583496094
aF3.502841796875
aF3.4058596801757814
aF3.334777526855469
aF3.292640380859375
aF3.265234069824219
aF3.2341018676757813
aF3.2250930786132814
aF3.164209899902344
aF3.1774853515625
aF3.149532470703125
aF3.121611328125
aF3.1020599365234376
aF3.0967864990234375
aF3.053994445800781
aF3.0195794677734376
aF3.007710266113281
aF2.9960202026367186
aF2.943719787597656
aF2.936476135253906
aF2.9120562744140623
aF2.9053802490234375
aF2.903179016113281
aF2.9081365966796877
aF2.8896917724609374
aF2.843946838378906
aF2.8529476928710937
aF2.828450927734375
aF2.832467346191406
aF2.797485046386719
aF2.784373474121094
aF2.793498229980469
aF2.774142150878906
aF2.7505233764648436
aF2.770491943359375
aF2.721873474121094
aF2.7183612060546873
aF2.688784484863281
aF2.7209207153320314
aF2.7224636840820313
aF2.716643981933594
aF2.6766311645507814
aF2.682759094238281
aF2.6677178955078125
aF2.6519769287109374
aF2.6417861938476563
aF2.66320556640625
aF2.6505743408203126
aF2.624217834472656
aF2.62391357421875
aF2.612961730957031
aF2.5912548828125
aF2.6141067504882813
aF2.6172103881835938
aF2.6090377807617187
aF2.5913040161132814
aF2.577120666503906
aF2.5874542236328124
aF2.5606915283203127
aF2.5505613708496093
aF2.5334228515625
aF2.534934387207031
aF2.5220098876953125
aF2.528040008544922
aF2.5520503234863283
aF2.528314971923828
aF2.5170669555664062
aF2.512655792236328
aF2.512212371826172
aF2.5092008972167967
aF2.4982928466796874
aF2.5005914306640626
aF2.497963104248047
aF2.4841607666015624
aF2.468228302001953
aF2.476481628417969
aF2.4606031799316406
aF2.472215270996094
aF2.4760282897949217
aF2.4505496215820313
aF2.447537078857422
aF2.437379913330078
aF2.45973876953125
aF2.42887451171875
aF2.4432334899902344
aF2.458487548828125
aF2.4209799194335937
aF2.4408497619628906
asS'test_loss'
p3
(lp4
F4.593951416015625
aF4.518684692382813
aF4.4444189453125
aF4.364588012695313
aF4.2887420654296875
aF4.20568115234375
aF4.104429931640625
aF4.01897216796875
aF3.8991098022460937
aF3.8022732543945312
aF3.679046936035156
aF3.573856201171875
aF3.4608935546875
aF3.4120623779296877
aF3.3577816772460936
aF3.2982208251953127
aF3.2446490478515626
aF3.215916748046875
aF3.178971862792969
aF3.181671142578125
aF3.149014587402344
aF3.115115966796875
aF3.10270263671875
aF3.0865570068359376
aF3.090042724609375
aF3.031635437011719
aF2.9939620971679686
aF2.9793600463867187
aF2.9720703125
aF2.9383648681640624
aF2.9246441650390627
aF2.912931213378906
aF2.897954406738281
aF2.880361328125
aF2.9034695434570312
aF2.851823425292969
aF2.8788018798828126
aF2.8587786865234377
aF2.814458923339844
aF2.8331875610351562
aF2.802407531738281
aF2.8065447998046875
aF2.7911758422851562
aF2.7477789306640625
aF2.7953656005859373
aF2.777648620605469
aF2.7609222412109373
aF2.7375912475585937
aF2.7350872802734374
aF2.7023471069335936
aF2.7045758056640623
aF2.6905996704101565
aF2.6721066284179686
aF2.6956381225585937
aF2.6719839477539065
aF2.6476507568359375
aF2.663169250488281
aF2.632260437011719
aF2.6232183837890624
aF2.628925476074219
aF2.6200851440429687
aF2.6167364501953125
aF2.633329162597656
aF2.5962429809570313
aF2.596147155761719
aF2.5780844116210937
aF2.591086120605469
aF2.575958557128906
aF2.5650830078125
aF2.5443194580078123
aF2.55613037109375
aF2.546755676269531
aF2.5476023864746096
aF2.524478759765625
aF2.518292999267578
aF2.517356414794922
aF2.523197021484375
aF2.5177958679199217
aF2.486490936279297
aF2.5085044860839845
aF2.4970330810546875
aF2.4766969299316406
aF2.4877734375
aF2.503164520263672
aF2.4781614685058595
aF2.4637684631347656
aF2.471534881591797
aF2.4811924743652343
aF2.4741162109375
aF2.443561859130859
aF2.4538642883300783
aF2.4579859924316407
aF2.4422808837890626
aF2.4497836303710936
aF2.4193714904785155
aF2.420837860107422
aF2.4276092529296873
aF2.425137634277344
aF2.4308885192871093
aF2.417074432373047
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0011565001431784266
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 51s'
p10
sS'final_test_loss'
p11
F2.417074432373047
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'b\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.