(dp0
S'train_loss'
p1
(lp2
F4.683219299316407
aF4.563551330566407
aF4.451452331542969
aF4.33550537109375
aF4.220867309570313
aF4.0727017211914065
aF3.966951904296875
aF3.8191018676757813
aF3.6872299194335936
aF3.556326599121094
aF3.4111090087890625
aF3.3315090942382812
aF3.236577453613281
aF3.159547119140625
aF3.10276611328125
aF3.0886749267578124
aF3.0814892578125
aF2.995661315917969
aF2.9841558837890627
aF2.9240447998046877
aF2.902391357421875
aF2.843963928222656
aF2.851235656738281
aF2.8193768310546874
aF2.789559631347656
aF2.7803173828125
aF2.7865145874023436
aF2.737611999511719
aF2.747033996582031
aF2.7287722778320314
aF2.7032760620117187
aF2.7027056884765623
aF2.6597256469726562
aF2.6605743408203124
aF2.6517877197265625
aF2.6304244995117188
aF2.617539978027344
aF2.613161315917969
aF2.596385192871094
aF2.591534423828125
aF2.5972329711914064
aF2.546416473388672
aF2.583240966796875
aF2.5593035888671873
aF2.5544172668457032
aF2.5206634521484377
aF2.530545501708984
aF2.52291259765625
aF2.514007110595703
aF2.4869915771484377
aF2.503050994873047
aF2.4997764587402345
aF2.4844688415527343
aF2.487118682861328
aF2.4661331176757812
aF2.452281036376953
aF2.4574238586425783
aF2.4630743408203126
aF2.4598257446289065
aF2.428306427001953
aF2.424183349609375
aF2.424952392578125
aF2.434164276123047
aF2.4074032592773436
aF2.397715301513672
aF2.381156768798828
aF2.4118495178222656
aF2.385148773193359
aF2.3716644287109374
aF2.381231384277344
aF2.366471862792969
aF2.370546569824219
aF2.3491156005859377
aF2.3493572998046877
aF2.3496470642089844
aF2.3330381774902342
aF2.3377229309082033
aF2.340521240234375
aF2.3280790710449217
aF2.3363775634765624
aF2.3052328491210936
aF2.302451171875
aF2.3344876098632814
aF2.319388427734375
aF2.3096194458007813
aF2.3033399963378907
aF2.2842315673828124
aF2.293290252685547
aF2.2851695251464843
aF2.280301055908203
aF2.2723234558105467
aF2.270488739013672
aF2.280922546386719
aF2.275951232910156
aF2.2530108642578126
aF2.2782356262207033
aF2.262070770263672
aF2.260093536376953
aF2.256710662841797
aF2.255923614501953
asS'test_loss'
p3
(lp4
F4.5660311889648435
aF4.450699157714844
aF4.327158508300781
aF4.208476867675781
aF4.085602722167969
aF3.9637548828125
aF3.81854248046875
aF3.655489807128906
aF3.52564453125
aF3.3992926025390626
aF3.305133056640625
aF3.241231689453125
aF3.1683035278320313
aF3.11534912109375
aF3.1073312377929687
aF3.0381259155273437
aF3.002206115722656
aF2.9704388427734374
aF2.9382989501953123
aF2.8976849365234374
aF2.87003662109375
aF2.8655520629882814
aF2.8243023681640627
aF2.7880709838867186
aF2.773751220703125
aF2.7729571533203123
aF2.738786315917969
aF2.7218057250976564
aF2.7169003295898437
aF2.7109454345703123
aF2.6962771606445313
aF2.6865664672851564
aF2.640752868652344
aF2.6398858642578125
aF2.6156082153320312
aF2.62246826171875
aF2.625246276855469
aF2.6184063720703126
aF2.59943359375
aF2.5802490234375
aF2.5564598083496093
aF2.557853240966797
aF2.571197509765625
aF2.555644836425781
aF2.5298858642578126
aF2.50223876953125
aF2.5266815185546876
aF2.504422607421875
aF2.499778594970703
aF2.5046925354003906
aF2.476268615722656
aF2.4638046264648437
aF2.488872833251953
aF2.4632044982910157
aF2.438562927246094
aF2.445113525390625
aF2.450747833251953
aF2.4121153259277346
aF2.4202125549316404
aF2.427064208984375
aF2.420551452636719
aF2.405578155517578
aF2.403234100341797
aF2.359707489013672
aF2.4045790100097655
aF2.383225402832031
aF2.3703338623046877
aF2.37473388671875
aF2.3684761047363283
aF2.373289794921875
aF2.3605519104003907
aF2.3415399169921876
aF2.329014892578125
aF2.3466331481933596
aF2.3415061950683596
aF2.335094757080078
aF2.327156982421875
aF2.330713195800781
aF2.327525634765625
aF2.3278434753417967
aF2.311750793457031
aF2.3161676025390623
aF2.3247251892089844
aF2.3017015075683593
aF2.3071917724609374
aF2.2750839233398437
aF2.292266845703125
aF2.2771583557128907
aF2.274526824951172
aF2.2851411437988283
aF2.277057647705078
aF2.272703094482422
aF2.268906707763672
aF2.270137939453125
aF2.251836700439453
aF2.2388128662109374
aF2.255221099853516
aF2.2623419189453124
aF2.2475212097167967
aF2.221833038330078
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0006746097510391668
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 13s'
p10
sS'final_test_loss'
p11
F2.221833038330078
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe3\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.