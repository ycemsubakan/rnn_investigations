(dp0
S'train_loss'
p1
(lp2
F4.677565612792969
aF3.7941326904296875
aF3.055669250488281
aF2.856094970703125
aF2.8130197143554687
aF2.7115298461914064
aF2.5926702880859374
aF2.549791717529297
aF2.5018719482421874
aF2.4477830505371094
aF2.425226745605469
aF2.4114129638671873
aF2.358156433105469
aF2.3638156127929686
aF2.3029798889160156
aF2.3064956665039062
aF2.294882507324219
aF2.2697674560546877
aF2.2546287536621095
aF2.2419000244140626
aF2.2265866088867186
aF2.2197529602050783
aF2.1742680358886717
aF2.160158386230469
aF2.155792541503906
aF2.128619842529297
aF2.13747314453125
aF2.133417510986328
aF2.1158792114257814
aF2.1090846252441406
aF2.098511199951172
aF2.0746688842773438
aF2.0660052490234375
aF2.0756333923339843
aF2.057474365234375
aF2.047451629638672
aF2.027454681396484
aF2.0191056823730467
aF2.007426300048828
aF2.0197073364257814
aF2.0218359375
aF1.991228790283203
aF2.0064131164550782
aF2.004556121826172
aF1.9772486877441406
aF2.0083052062988282
aF1.9917489624023437
aF1.9762478637695313
aF1.959680633544922
aF1.9613670349121093
aF1.962196807861328
aF1.938338165283203
aF1.9189938354492186
aF1.9403077697753905
aF1.9247613525390626
aF1.9440093994140626
aF1.9335231018066406
aF1.9186395263671876
aF1.9167037963867188
aF1.899600067138672
aF1.91706787109375
aF1.9015878295898438
aF1.897751007080078
aF1.8927444458007812
aF1.899794464111328
aF1.876646728515625
aF1.8937965393066407
aF1.8819374084472655
aF1.8890252685546876
aF1.8445794677734375
aF1.858140869140625
aF1.851606903076172
aF1.8444395446777344
aF1.879164581298828
aF1.8515663146972656
aF1.8342735290527343
aF1.829658203125
aF1.8175662231445313
aF1.81330078125
aF1.8422845458984376
aF1.8273561096191406
aF1.8342315673828125
aF1.8107557678222657
aF1.835870819091797
aF1.8237370300292968
aF1.8112495422363282
aF1.8095637512207032
aF1.8210089111328125
aF1.780286865234375
aF1.7955868530273438
aF1.806320037841797
aF1.8109428405761718
aF1.7790830993652345
aF1.7703466796875
aF1.8060760498046875
aF1.7817738342285157
aF1.8231643676757812
aF1.7785514831542968
aF1.8014163208007812
aF1.7684022521972655
asS'test_loss'
p3
(lp4
F3.8052899169921877
aF3.027684631347656
aF2.8423587036132814
aF2.8529345703125
aF2.7128875732421873
aF2.6159268188476563
aF2.5454898071289063
aF2.4979043579101563
aF2.4464276123046873
aF2.395355224609375
aF2.3908348083496094
aF2.3670851135253907
aF2.332716064453125
aF2.2942787170410157
aF2.306177978515625
aF2.2817584228515626
aF2.251841278076172
aF2.2185101318359375
aF2.2277186584472655
aF2.187876739501953
aF2.187905120849609
aF2.1817835998535156
aF2.166273498535156
aF2.1627851867675782
aF2.1444132995605467
aF2.1309963989257814
aF2.1201345825195315
aF2.1087602233886718
aF2.099855499267578
aF2.0816485595703127
aF2.0655674743652344
aF2.0711381530761717
aF2.0789535522460936
aF2.046042022705078
aF2.0518510437011717
aF2.0211431884765627
aF2.029495849609375
aF2.030533905029297
aF2.0205392456054687
aF2.0162892150878906
aF1.9955839538574218
aF1.9945867919921876
aF1.995486297607422
aF1.9823252868652343
aF1.967908935546875
aF1.940791015625
aF1.965294647216797
aF1.9568348693847657
aF1.9408073425292969
aF1.9564012145996095
aF1.9546237182617188
aF1.9525167846679687
aF1.9449253845214844
aF1.9357565307617188
aF1.9371951293945313
aF1.913729705810547
aF1.9102610778808593
aF1.9243299865722656
aF1.9168540954589843
aF1.90968017578125
aF1.9093721008300781
aF1.9078305053710938
aF1.896641082763672
aF1.8794871520996095
aF1.87771728515625
aF1.8703794860839844
aF1.8515716552734376
aF1.8691790771484376
aF1.891618194580078
aF1.8648333740234375
aF1.8780500793457031
aF1.8655912780761719
aF1.892349395751953
aF1.848377685546875
aF1.8545033264160156
aF1.8749322509765625
aF1.8460292053222656
aF1.8544081115722657
aF1.842624969482422
aF1.8104405212402344
aF1.8342781066894531
aF1.8397940063476563
aF1.823865203857422
aF1.8049534606933593
aF1.8052650451660157
aF1.8236752319335938
aF1.816561737060547
aF1.7901100158691405
aF1.811202392578125
aF1.7980523681640626
aF1.7971693420410155
aF1.8161819458007813
aF1.8061073303222657
aF1.8005087280273437
aF1.8128289794921875
aF1.7561856079101563
aF1.789272003173828
aF1.7763252258300781
aF1.7761215209960937
aF1.7764082336425782
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00790014927551274
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 10s'
p10
sS'final_test_loss'
p11
F1.7764082336425782
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xd1\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.