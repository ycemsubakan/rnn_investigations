(dp0
S'train_loss'
p1
(lp2
F4.653951110839844
aF4.561031494140625
aF4.467153625488281
aF4.3608642578125
aF4.227471618652344
aF4.081611938476563
aF3.9307318115234375
aF3.797524108886719
aF3.68468994140625
aF3.610186767578125
aF3.551416015625
aF3.4874951171875
aF3.4861785888671877
aF3.404596862792969
aF3.4150970458984373
aF3.36779541015625
aF3.3908544921875
aF3.3221246337890626
aF3.304443359375
aF3.3208187866210936
aF3.298799743652344
aF3.265213623046875
aF3.2480548095703123
aF3.2532254028320313
aF3.2009844970703125
aF3.214530334472656
aF3.1757684326171876
aF3.1860446166992187
aF3.1413009643554686
aF3.123030090332031
aF3.1140020751953124
aF3.06097900390625
aF3.061398620605469
aF3.04070068359375
aF3.0240469360351563
aF3.00990234375
aF2.999511413574219
aF2.984234619140625
aF2.936326904296875
aF2.929520263671875
aF2.916929931640625
aF2.912847595214844
aF2.9111312866210937
aF2.8626373291015623
aF2.8740554809570313
aF2.84940185546875
aF2.8284320068359374
aF2.802051696777344
aF2.795774230957031
aF2.780558776855469
aF2.7717886352539063
aF2.743740234375
aF2.752701721191406
aF2.731043701171875
aF2.744203796386719
aF2.698283386230469
aF2.6789324951171873
aF2.6732138061523436
aF2.6845849609375
aF2.6627682495117186
aF2.6456475830078126
aF2.642065124511719
aF2.6176531982421873
aF2.62055419921875
aF2.6081002807617186
aF2.5849423217773437
aF2.588043518066406
aF2.577525634765625
aF2.570552978515625
aF2.5657696533203125
aF2.5404266357421874
aF2.5260569763183596
aF2.540662994384766
aF2.5263104248046875
aF2.514864959716797
aF2.509105224609375
aF2.4964199829101563
aF2.4946173095703124
aF2.4870616149902345
aF2.4930740356445313
aF2.4723988342285157
aF2.4609071350097658
aF2.4621424865722656
aF2.4411274719238283
aF2.441264343261719
aF2.4665965270996093
aF2.4354316711425783
aF2.4185494995117187
aF2.4190415954589843
aF2.4197879028320313
aF2.438398590087891
aF2.409219970703125
aF2.3884654235839844
aF2.4043687438964843
aF2.3738778686523436
aF2.3843670654296876
aF2.3841217041015623
aF2.378520965576172
aF2.378179473876953
aF2.368535919189453
asS'test_loss'
p3
(lp4
F4.5652734375
aF4.46888671875
aF4.356996154785156
aF4.22314208984375
aF4.07125
aF3.9072711181640627
aF3.790872802734375
aF3.6826922607421877
aF3.6106137084960936
aF3.5481304931640625
aF3.509399719238281
aF3.4693182373046874
aF3.42836669921875
aF3.4113446044921876
aF3.4192120361328127
aF3.349407043457031
aF3.3446331787109376
aF3.3185107421875
aF3.299488220214844
aF3.2983639526367186
aF3.273390197753906
aF3.2544085693359377
aF3.2200213623046876
aF3.215798034667969
aF3.1998028564453125
aF3.1625482177734376
aF3.1614508056640624
aF3.1575811767578124
aF3.132268981933594
aF3.10252197265625
aF3.0669000244140623
aF3.0613497924804687
aF3.04330078125
aF3.010046081542969
aF2.9860943603515624
aF3.0027142333984376
aF2.9738201904296875
aF2.951452331542969
aF2.916373596191406
aF2.9067254638671876
aF2.9105877685546875
aF2.884549560546875
aF2.8933758544921875
aF2.84151123046875
aF2.8257272338867185
aF2.84096435546875
aF2.786763000488281
aF2.796199951171875
aF2.7893051147460937
aF2.778827819824219
aF2.74095947265625
aF2.72108642578125
aF2.700317077636719
aF2.7081207275390624
aF2.7020974731445313
aF2.6939163208007812
aF2.6634530639648437
aF2.6627987670898436
aF2.66553955078125
aF2.631112060546875
aF2.633818359375
aF2.6366741943359373
aF2.6024038696289065
aF2.61245849609375
aF2.577403259277344
aF2.579583740234375
aF2.5577174377441407
aF2.5431822204589842
aF2.563818054199219
aF2.5545941162109376
aF2.5334814453125
aF2.5282818603515627
aF2.522968444824219
aF2.4982028198242188
aF2.4942245483398438
aF2.4822381591796874
aF2.491688079833984
aF2.475008850097656
aF2.473447723388672
aF2.4576576232910154
aF2.458043212890625
aF2.4341854858398437
aF2.448514251708984
aF2.4393115234375
aF2.4451608276367187
aF2.4378936767578123
aF2.4083689880371093
aF2.418903656005859
aF2.4001197814941406
aF2.410666961669922
aF2.3903733825683595
aF2.4090948486328125
aF2.4071478271484374
aF2.3933265686035154
aF2.390832824707031
aF2.3779913330078126
aF2.3733749389648438
aF2.3687594604492186
aF2.362783203125
aF2.348332977294922
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0018691726559799948
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 8s'
p10
sS'final_test_loss'
p11
F2.348332977294922
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'F\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.