(dp0
S'train_loss'
p1
(lp2
F4.647010803222656
aF4.638374633789063
aF4.632484436035156
aF4.62147216796875
aF4.614216613769531
aF4.605005493164063
aF4.595802917480468
aF4.587006530761719
aF4.574522399902344
aF4.571068115234375
aF4.560487060546875
aF4.553829650878907
aF4.548068237304688
aF4.535381469726563
aF4.531194152832032
aF4.5188467407226565
aF4.512476501464843
aF4.5069921875
aF4.494500427246094
aF4.484845275878906
aF4.480188903808593
aF4.470665588378906
aF4.456742858886718
aF4.451334228515625
aF4.440317687988281
aF4.4333493041992185
aF4.4247262573242185
aF4.413206176757813
aF4.406959838867188
aF4.393955078125
aF4.384083862304688
aF4.3762210083007815
aF4.367474975585938
aF4.350462646484375
aF4.3472802734375
aF4.333533935546875
aF4.32324951171875
aF4.316612243652344
aF4.303725891113281
aF4.283853454589844
aF4.277347412109375
aF4.267820129394531
aF4.255901489257813
aF4.243664855957031
aF4.232676696777344
aF4.213738098144531
aF4.204933471679688
aF4.191892395019531
aF4.175596923828125
aF4.154930419921875
aF4.150484619140625
aF4.131832580566407
aF4.112537231445312
aF4.094893798828125
aF4.081204833984375
aF4.065403747558594
aF4.043066101074219
aF4.026720581054687
aF4.009825439453125
aF3.9814572143554687
aF3.9730657958984374
aF3.955901794433594
aF3.9259628295898437
aF3.9125909423828125
aF3.8880349731445314
aF3.8524765014648437
aF3.847819519042969
aF3.8282098388671875
aF3.788604431152344
aF3.7983139038085936
aF3.7532745361328126
aF3.736773681640625
aF3.7230398559570315
aF3.6938101196289064
aF3.6619757080078124
aF3.6439453125
aF3.6200741577148436
aF3.611433410644531
aF3.610250244140625
aF3.5877337646484375
aF3.54275390625
aF3.5529583740234374
aF3.5486859130859374
aF3.5128228759765623
aF3.4961920166015625
aF3.4885333251953123
aF3.475045471191406
aF3.4427151489257812
aF3.461320495605469
aF3.4312680053710936
aF3.425179748535156
aF3.428239440917969
aF3.4027337646484375
aF3.4006695556640625
aF3.4059823608398436
aF3.370453186035156
aF3.374833679199219
aF3.3563232421875
aF3.3495367431640624
aF3.36213623046875
asS'test_loss'
p3
(lp4
F4.635936279296875
aF4.629923400878906
aF4.619853210449219
aF4.612438659667969
aF4.602978210449219
aF4.595731201171875
aF4.583642578125
aF4.578194274902343
aF4.569527282714843
aF4.563211975097656
aF4.553307800292969
aF4.5479824829101565
aF4.538555297851563
aF4.529061279296875
aF4.525586547851563
aF4.508123779296875
aF4.500973205566407
aF4.492665405273438
aF4.482205505371094
aF4.477312927246094
aF4.468798828125
aF4.456916809082031
aF4.446391296386719
aF4.440059204101562
aF4.430120544433594
aF4.424887084960938
aF4.416536560058594
aF4.405060424804687
aF4.396829528808594
aF4.3820193481445315
aF4.370417785644531
aF4.363346557617188
aF4.355096435546875
aF4.341820068359375
aF4.333584594726562
aF4.319028625488281
aF4.314302368164062
aF4.300505065917969
aF4.285570983886719
aF4.274515075683594
aF4.264480285644531
aF4.249973754882813
aF4.24042236328125
aF4.229701538085937
aF4.2133544921875
aF4.202227783203125
aF4.186590881347656
aF4.172937927246093
aF4.152099609375
aF4.138757934570313
aF4.126695251464843
aF4.112103271484375
aF4.094650573730469
aF4.082230529785156
aF4.053491516113281
aF4.03580810546875
aF4.02655517578125
aF4.005244750976562
aF3.9768447875976562
aF3.9785336303710936
aF3.944534912109375
aF3.92650390625
aF3.9035211181640626
aF3.893479309082031
aF3.858656311035156
aF3.8384335327148436
aF3.8256399536132815
aF3.7993203735351564
aF3.7724172973632815
aF3.7529168701171876
aF3.736679992675781
aF3.716031494140625
aF3.6907183837890627
aF3.686268005371094
aF3.6414981079101563
aF3.625355529785156
aF3.60872802734375
aF3.599781188964844
aF3.5819537353515627
aF3.564858093261719
aF3.5749417114257813
aF3.5265185546875
aF3.5065567016601564
aF3.5238406372070314
aF3.4705691528320313
aF3.469813232421875
aF3.456706848144531
aF3.4348529052734373
aF3.4438824462890625
aF3.4324679565429688
aF3.4083071899414064
aF3.3966934204101564
aF3.3813461303710937
aF3.3684347534179686
aF3.3811651611328126
aF3.373764953613281
aF3.342651062011719
aF3.349170837402344
aF3.333909912109375
aF3.3258837890625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00011120803044810925
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 53s'
p10
sS'final_test_loss'
p11
F3.3258837890625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'{\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.