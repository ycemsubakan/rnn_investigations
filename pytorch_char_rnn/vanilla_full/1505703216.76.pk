(dp0
S'train_loss'
p1
(lp2
F4.655270385742187
aF4.563505859375
aF4.4694586181640625
aF4.366362609863281
aF4.240079650878906
aF4.099946899414062
aF3.937139892578125
aF3.7749765014648435
aF3.6578750610351562
aF3.54204345703125
aF3.486815185546875
aF3.4253573608398438
aF3.3714178466796874
aF3.356949462890625
aF3.347393798828125
aF3.2988555908203123
aF3.2703897094726564
aF3.2457720947265627
aF3.236470031738281
aF3.186904296875
aF3.1532968139648436
aF3.169519958496094
aF3.1159283447265627
aF3.0945697021484375
aF3.0545504760742186
aF3.0278549194335938
aF3.004677429199219
aF3.025692138671875
aF2.958385314941406
aF2.927986755371094
aF2.937457275390625
aF2.9216653442382814
aF2.9269775390625
aF2.866799621582031
aF2.859120178222656
aF2.84088623046875
aF2.84212158203125
aF2.8127206420898436
aF2.8022225952148436
aF2.7943768310546875
aF2.8011383056640624
aF2.7438815307617186
aF2.7338290405273438
aF2.7190823364257812
aF2.6925393676757814
aF2.713486633300781
aF2.700865478515625
aF2.6891323852539064
aF2.658843688964844
aF2.6540887451171873
aF2.6533099365234376
aF2.62456298828125
aF2.6015789794921873
aF2.6095431518554686
aF2.6013494873046876
aF2.5956039428710938
aF2.57613037109375
aF2.5488345336914064
aF2.5528378295898437
aF2.541398468017578
aF2.551089324951172
aF2.543643035888672
aF2.52451416015625
aF2.521387634277344
aF2.5284544372558595
aF2.5008287048339843
aF2.4825808715820314
aF2.4613829040527344
aF2.4890602111816404
aF2.4615544128417968
aF2.4778207397460936
aF2.4358843994140624
aF2.4533674621582033
aF2.4588352966308595
aF2.4412722778320313
aF2.4328416442871093
aF2.4306512451171876
aF2.426189422607422
aF2.3941294860839846
aF2.413116760253906
aF2.4031126403808596
aF2.3838685607910155
aF2.4003004455566406
aF2.3795263671875
aF2.3653105163574217
aF2.371492462158203
aF2.3671812438964843
aF2.3744741821289064
aF2.34793212890625
aF2.3510443115234376
aF2.3413932800292967
aF2.345579071044922
aF2.3405392456054686
aF2.326474304199219
aF2.3620391845703126
aF2.3279310607910157
aF2.3313563537597655
aF2.3152398681640625
aF2.3131126403808593
aF2.2935249328613283
asS'test_loss'
p3
(lp4
F4.565518798828125
aF4.4684451293945315
aF4.365140686035156
aF4.241203002929687
aF4.085213623046875
aF3.9315185546875
aF3.7756613159179686
aF3.640078125
aF3.5637051391601564
aF3.4863214111328125
aF3.443422546386719
aF3.3946966552734374
aF3.352960205078125
aF3.3230667114257812
aF3.297193603515625
aF3.267420654296875
aF3.2331918334960936
aF3.2254443359375
aF3.1573086547851563
aF3.1489752197265624
aF3.106195068359375
aF3.072464599609375
aF3.0873370361328125
aF3.0766415405273437
aF3.048368225097656
aF3.006269836425781
aF2.9924990844726564
aF2.951064453125
aF2.9693072509765623
aF2.9399603271484374
aF2.931207580566406
aF2.9104339599609377
aF2.87626953125
aF2.8731640625
aF2.8371627807617186
aF2.840086669921875
aF2.81781982421875
aF2.791551208496094
aF2.7817514038085935
aF2.7863592529296874
aF2.7598086547851564
aF2.76076904296875
aF2.7195733642578124
aF2.693523254394531
aF2.71510986328125
aF2.669951171875
aF2.6800582885742186
aF2.6397671508789062
aF2.6506500244140625
aF2.6304336547851563
aF2.613802795410156
aF2.612213134765625
aF2.60861083984375
aF2.6021783447265623
aF2.5536865234375
aF2.587594299316406
aF2.556826477050781
aF2.54069580078125
aF2.543359375
aF2.538979034423828
aF2.5295068359375
aF2.523550567626953
aF2.4925262451171877
aF2.508486785888672
aF2.492480926513672
aF2.485881652832031
aF2.4882939147949217
aF2.4727018737792967
aF2.4490370178222656
aF2.445836181640625
aF2.430619964599609
aF2.429725341796875
aF2.4409967041015626
aF2.4270310974121094
aF2.4191012573242188
aF2.4230108642578125
aF2.4092533874511717
aF2.411802978515625
aF2.403777770996094
aF2.381966857910156
aF2.389789886474609
aF2.381888122558594
aF2.3672775268554687
aF2.372542724609375
aF2.3410189819335936
aF2.3743075561523437
aF2.3540690612792967
aF2.354781494140625
aF2.317740173339844
aF2.3397042846679685
aF2.3220347595214843
aF2.3146101379394532
aF2.324964599609375
aF2.327820587158203
aF2.316061553955078
aF2.3116213989257814
aF2.294588165283203
aF2.302269287109375
aF2.3139874267578127
aF2.303099822998047
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0008599833740283407
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 29s'
p10
sS'final_test_loss'
p11
F2.303099822998047
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x8c\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.