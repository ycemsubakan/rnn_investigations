(dp0
S'train_loss'
p1
(lp2
F4.634640808105469
aF4.549212036132812
aF4.466030578613282
aF4.379637451171875
aF4.283038635253906
aF4.183344421386718
aF4.068575134277344
aF3.9423892211914064
aF3.793082275390625
aF3.670086669921875
aF3.540670166015625
aF3.47558349609375
aF3.4031549072265626
aF3.388629150390625
aF3.3147293090820313
aF3.279649353027344
aF3.2830319213867187
aF3.24605224609375
aF3.1971026611328126
aF3.1555426025390627
aF3.110417785644531
aF3.089852294921875
aF3.0654031372070314
aF3.016745910644531
aF2.9919302368164065
aF3.0065142822265627
aF2.983587341308594
aF2.952308349609375
aF2.9383624267578123
aF2.9058380126953125
aF2.8975210571289063
aF2.89204833984375
aF2.834788513183594
aF2.8330148315429686
aF2.8119631958007814
aF2.79887451171875
aF2.8011288452148437
aF2.7768539428710937
aF2.7679632568359374
aF2.74860595703125
aF2.747286071777344
aF2.7241217041015626
aF2.724957275390625
aF2.7118096923828126
aF2.6627850341796875
aF2.698492431640625
aF2.6491549682617186
aF2.6508123779296877
aF2.6489910888671875
aF2.604738464355469
aF2.6374801635742187
aF2.5926223754882813
aF2.6007290649414063
aF2.5648626708984374
aF2.5817770385742187
aF2.56981689453125
aF2.5583058166503907
aF2.553990783691406
aF2.5606134033203123
aF2.533490753173828
aF2.5373104858398436
aF2.5328741455078125
aF2.528828125
aF2.499766387939453
aF2.4858016967773438
aF2.4805133056640627
aF2.4787059020996094
aF2.4732511901855467
aF2.464602813720703
aF2.4639097595214845
aF2.4698141479492186
aF2.447060546875
aF2.4265733337402344
aF2.4338160705566407
aF2.428805236816406
aF2.427827911376953
aF2.415336151123047
aF2.4200244140625
aF2.4037867736816407
aF2.41305908203125
aF2.4223960876464843
aF2.3910699462890626
aF2.377258758544922
aF2.3877032470703123
aF2.3810043334960938
aF2.3796682739257813
aF2.3647210693359373
aF2.3737826538085938
aF2.3409883117675783
aF2.361757354736328
aF2.3284175109863283
aF2.352860107421875
aF2.3405859375
aF2.3370039367675783
aF2.319100646972656
aF2.317873687744141
aF2.326105499267578
aF2.3232353210449217
aF2.31302734375
aF2.3178265380859373
asS'test_loss'
p3
(lp4
F4.548394775390625
aF4.461463317871094
aF4.377274475097656
aF4.2839352416992185
aF4.168356018066406
aF4.069502868652344
aF3.928482666015625
aF3.7837677001953125
aF3.6660226440429686
aF3.5389208984375
aF3.478676452636719
aF3.4147598266601564
aF3.3831640625
aF3.302357482910156
aF3.2519985961914064
aF3.2338818359375
aF3.1911495971679686
aF3.159581604003906
aF3.126517639160156
aF3.117157287597656
aF3.093568115234375
aF3.0790533447265624
aF3.0215725708007812
aF3.022108459472656
aF3.004879150390625
aF2.9661599731445314
aF2.95005859375
aF2.9349566650390626
aF2.917283630371094
aF2.882273864746094
aF2.893152770996094
aF2.8439700317382814
aF2.8531454467773436
aF2.8098513793945314
aF2.7875640869140623
aF2.798184509277344
aF2.788846740722656
aF2.746277160644531
aF2.7488253784179686
aF2.7393218994140627
aF2.7273675537109376
aF2.6900509643554686
aF2.6715618896484377
aF2.7043356323242187
aF2.6710379028320315
aF2.6396408081054688
aF2.6399026489257813
aF2.6209075927734373
aF2.6471261596679687
aF2.6015289306640623
aF2.5915518188476563
aF2.5874591064453125
aF2.5868316650390626
aF2.5689437866210936
aF2.5572801208496094
aF2.5681729125976562
aF2.5431983947753904
aF2.5436332702636717
aF2.5118280029296876
aF2.5084329223632813
aF2.5070423889160156
aF2.5177156066894533
aF2.5165528869628906
aF2.4814080810546875
aF2.498857116699219
aF2.4812403869628907
aF2.46885009765625
aF2.469618682861328
aF2.4675784301757813
aF2.463417663574219
aF2.4357008361816406
aF2.421609344482422
aF2.431454772949219
aF2.4270614624023437
aF2.4227069091796873
aF2.406167449951172
aF2.4107844543457033
aF2.4017333984375
aF2.3956219482421877
aF2.3925247192382812
aF2.3839183044433594
aF2.3748899841308595
aF2.3850328063964845
aF2.3842921447753906
aF2.3576820373535154
aF2.3736883544921876
aF2.341713714599609
aF2.3517005920410154
aF2.3391844177246095
aF2.323469543457031
aF2.343662109375
aF2.3349560546875
aF2.3232872009277346
aF2.32951904296875
aF2.328749542236328
aF2.3172113037109376
aF2.3427642822265624
aF2.3236204528808595
aF2.311145782470703
aF2.285725860595703
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0003847433125684422
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 14s'
p10
sS'final_test_loss'
p11
F2.285725860595703
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf9\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.