(dp0
S'train_loss'
p1
(lp2
F4.662593078613281
aF4.397512817382813
aF4.152330017089843
aF3.8650137329101564
aF3.5419882202148436
aF3.291600341796875
aF3.1709384155273437
aF3.0943417358398437
aF3.0231924438476563
aF2.9502215576171875
aF2.9224554443359376
aF2.874674072265625
aF2.836195068359375
aF2.8095904541015626
aF2.767666931152344
aF2.7302264404296874
aF2.719981994628906
aF2.6950946044921875
aF2.674876708984375
aF2.6284725952148436
aF2.6089581298828124
aF2.61015380859375
aF2.577138671875
aF2.5457579040527345
aF2.51349853515625
aF2.5241520690917967
aF2.4781402587890624
aF2.485098876953125
aF2.508866882324219
aF2.4391915893554685
aF2.4465841674804687
aF2.4320498657226564
aF2.4079893493652342
aF2.398771057128906
aF2.416051940917969
aF2.388553009033203
aF2.3676467895507813
aF2.3878363037109374
aF2.370482940673828
aF2.3499909973144533
aF2.3299658203125
aF2.333255615234375
aF2.330022430419922
aF2.3393399047851564
aF2.317752685546875
aF2.312928466796875
aF2.313063659667969
aF2.305264892578125
aF2.2803634643554687
aF2.2723049926757812
aF2.279637908935547
aF2.2669526672363283
aF2.2488449096679686
aF2.2518801879882813
aF2.2407508850097657
aF2.233411865234375
aF2.237274627685547
aF2.2379469299316406
aF2.2319923400878907
aF2.210594024658203
aF2.202550964355469
aF2.2107034301757813
aF2.1867372131347658
aF2.198700714111328
aF2.20329345703125
aF2.196702728271484
aF2.1836976623535156
aF2.200223846435547
aF2.1665225219726563
aF2.16453857421875
aF2.175273742675781
aF2.1753915405273436
aF2.1553021240234376
aF2.173032073974609
aF2.166900634765625
aF2.1604154968261717
aF2.1549493408203126
aF2.1417449951171874
aF2.1215951538085935
aF2.14202880859375
aF2.123718719482422
aF2.1251588439941407
aF2.1282261657714843
aF2.1188508605957033
aF2.120066375732422
aF2.1098524475097657
aF2.103548278808594
aF2.112816162109375
aF2.117079620361328
aF2.0915011596679687
aF2.1064349365234376
aF2.110399627685547
aF2.106838836669922
aF2.105532531738281
aF2.112243957519531
aF2.081498565673828
aF2.0834117126464844
aF2.0864453125
aF2.0845516967773436
aF2.0786334228515626
asS'test_loss'
p3
(lp4
F4.4000552368164065
aF4.142884521484375
aF3.8628509521484373
aF3.5346945190429686
aF3.293750915527344
aF3.1665853881835937
aF3.1116278076171877
aF3.025814208984375
aF2.984416809082031
aF2.9140399169921873
aF2.8699221801757813
aF2.803709716796875
aF2.8148907470703124
aF2.740780944824219
aF2.7145904541015624
aF2.715245361328125
aF2.691068115234375
aF2.6499432373046874
aF2.622489013671875
aF2.5940179443359375
aF2.57036865234375
aF2.5529376220703126
aF2.5315565490722656
aF2.5338429260253905
aF2.507330017089844
aF2.480995941162109
aF2.4500466918945314
aF2.4539950561523436
aF2.451082916259766
aF2.4247544860839843
aF2.4231480407714843
aF2.4051123046875
aF2.412488555908203
aF2.4004554748535156
aF2.3824777221679687
aF2.3778709411621093
aF2.346240692138672
aF2.3619125366210936
aF2.3476942443847655
aF2.3436346435546875
aF2.348047790527344
aF2.3164939880371094
aF2.3228518676757814
aF2.308066711425781
aF2.3069920349121094
aF2.277926330566406
aF2.30692626953125
aF2.271750946044922
aF2.2680435180664062
aF2.264163360595703
aF2.2532412719726564
aF2.259091033935547
aF2.241194152832031
aF2.2310446166992186
aF2.2353245544433595
aF2.238607635498047
aF2.210570068359375
aF2.2238005065917967
aF2.2169647216796875
aF2.213082122802734
aF2.2084141540527344
aF2.1976145935058593
aF2.1991421508789064
aF2.1896551513671874
aF2.1854985046386717
aF2.1978619384765623
aF2.170005340576172
aF2.1704165649414064
aF2.155773162841797
aF2.1744345092773436
aF2.1622149658203127
aF2.1572610473632814
aF2.1633457946777344
aF2.1477012634277344
aF2.146534423828125
aF2.15421630859375
aF2.141693878173828
aF2.1243431091308596
aF2.1229428100585936
aF2.1263966369628906
aF2.122650146484375
aF2.1148382568359376
aF2.1247988891601564
aF2.092886657714844
aF2.118760833740234
aF2.108836822509766
aF2.106272430419922
aF2.0989324951171877
aF2.105657653808594
aF2.09557861328125
aF2.0852049255371092
aF2.0901513671875
aF2.0827322387695313
aF2.0972918701171874
aF2.0699642944335936
aF2.083318328857422
aF2.077808380126953
aF2.0634231567382812
aF2.058017578125
aF2.0794052124023437
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0037331603664768074
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 51s'
p10
sS'final_test_loss'
p11
F2.0794052124023437
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'n\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.