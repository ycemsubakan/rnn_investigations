(dp0
S'train_loss'
p1
(lp2
F4.644117736816407
aF4.419559936523438
aF4.136866760253906
aF3.7873721313476563
aF3.5363677978515624
aF3.386055603027344
aF3.3352703857421875
aF3.2651577758789063
aF3.218205871582031
aF3.1219827270507814
aF3.0741864013671876
aF3.00999267578125
aF2.9678387451171875
aF2.9212469482421874
aF2.8821820068359374
aF2.8408209228515626
aF2.8230682373046876
aF2.768380126953125
aF2.7208737182617186
aF2.7124758911132814
aF2.67987060546875
aF2.6461248779296875
aF2.62595458984375
aF2.6136029052734373
aF2.587580261230469
aF2.5659722900390625
aF2.554445953369141
aF2.513538513183594
aF2.5003366088867187
aF2.495810852050781
aF2.468905487060547
aF2.45403564453125
aF2.458212432861328
aF2.431767578125
aF2.424427185058594
aF2.3948912048339843
aF2.396779022216797
aF2.384357604980469
aF2.356159515380859
aF2.3531924438476564
aF2.355430908203125
aF2.3412539672851564
aF2.3264396667480467
aF2.3103427124023437
aF2.321536102294922
aF2.300837097167969
aF2.2755252075195314
aF2.294237518310547
aF2.2896244812011717
aF2.2599252319335936
aF2.255401916503906
aF2.2411517333984374
aF2.2378509521484373
aF2.246919860839844
aF2.225587158203125
aF2.223732757568359
aF2.218664245605469
aF2.226740264892578
aF2.2126866149902344
aF2.1864215087890626
aF2.1916964721679686
aF2.1923020935058593
aF2.188821258544922
aF2.1616937255859376
aF2.1884393310546875
aF2.1810215759277343
aF2.1715771484375
aF2.1675431823730467
aF2.145014190673828
aF2.1377879333496095
aF2.14277099609375
aF2.153592987060547
aF2.1125103759765627
aF2.134452209472656
aF2.126146697998047
aF2.135419464111328
aF2.1029093933105467
aF2.1042829895019532
aF2.0949526977539064
aF2.0720199584960937
aF2.102805480957031
aF2.095743865966797
aF2.069946594238281
aF2.0791937255859376
aF2.075145416259766
aF2.0629257202148437
aF2.069221954345703
aF2.0532102966308594
aF2.063448181152344
aF2.0580828857421873
aF2.045315399169922
aF2.054285430908203
aF2.0453871154785155
aF2.034390411376953
aF2.0385455322265624
aF2.03474365234375
aF2.018564910888672
aF2.013563690185547
aF2.0191485595703127
aF2.010292205810547
asS'test_loss'
p3
(lp4
F4.4143765258789065
aF4.13635009765625
aF3.7767568969726564
aF3.5499371337890624
aF3.386113586425781
aF3.3103369140625
aF3.2670669555664062
aF3.1870684814453125
aF3.15417724609375
aF3.0595669555664062
aF3.029790344238281
aF2.9401370239257814
aF2.8975680541992186
aF2.887584533691406
aF2.8554693603515626
aF2.79020263671875
aF2.7587435913085936
aF2.75269775390625
aF2.7152587890625
aF2.6943368530273437
aF2.6463555908203125
aF2.6230523681640623
aF2.578724060058594
aF2.5844964599609375
aF2.5529606628417967
aF2.539736633300781
aF2.5079415893554686
aF2.5210781860351563
aF2.481575775146484
aF2.455988006591797
aF2.431463623046875
aF2.4405503845214844
aF2.4283546447753905
aF2.411976013183594
aF2.3942347717285157
aF2.378529357910156
aF2.382333526611328
aF2.3678207397460938
aF2.3518185424804687
aF2.3492698669433594
aF2.3434490966796875
aF2.3229383850097656
aF2.3103533935546876
aF2.296217498779297
aF2.3030686950683594
aF2.2833482360839845
aF2.2806961059570314
aF2.271641845703125
aF2.2618988037109373
aF2.2466549682617187
aF2.2453912353515624
aF2.245627593994141
aF2.2441162109375
aF2.228467559814453
aF2.215298309326172
aF2.2064805603027344
aF2.2005552673339843
aF2.207059173583984
aF2.1872793579101564
aF2.1998135375976564
aF2.188110046386719
aF2.1853419494628907
aF2.1675628662109374
aF2.1598214721679687
aF2.1673321533203125
aF2.1399960327148437
aF2.144881286621094
aF2.1461932373046877
aF2.138224334716797
aF2.140962829589844
aF2.1161227416992188
aF2.1360017395019533
aF2.0992050170898438
aF2.0944154357910154
aF2.1038412475585937
aF2.105992431640625
aF2.0993276977539064
aF2.095823974609375
aF2.0808860778808596
aF2.096121826171875
aF2.0808901977539063
aF2.0712608337402343
aF2.061024475097656
aF2.0726304626464844
aF2.0603703308105468
aF2.0374154663085937
aF2.0753048706054686
aF2.0385081481933596
aF2.03767578125
aF2.0407286071777344
aF2.048795623779297
aF2.0428759765625
aF2.023759002685547
aF2.0230085754394533
aF2.035652770996094
aF2.0023890686035157
aF2.0264517211914064
aF2.0230621337890624
aF2.005040588378906
aF1.996068572998047
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0027430801005787053
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 26s'
p10
sS'final_test_loss'
p11
F1.996068572998047
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x81\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.