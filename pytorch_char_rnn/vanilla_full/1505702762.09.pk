(dp0
S'train_loss'
p1
(lp2
F4.667744140625
aF4.58696533203125
aF4.508801574707031
aF4.434609375
aF4.343816528320312
aF4.254418640136719
aF4.1747439575195315
aF4.077793884277344
aF3.974185791015625
aF3.8714862060546875
aF3.7640283203125
aF3.6666555786132813
aF3.5612783813476563
aF3.4522833251953124
aF3.386466064453125
aF3.3259564208984376
aF3.284639587402344
aF3.197246398925781
aF3.1953094482421873
aF3.1957943725585936
aF3.1539370727539064
aF3.1110110473632813
aF3.1102691650390626
aF3.0802682495117186
aF3.0897702026367186
aF3.0449258422851564
aF3.0145025634765625
aF2.995589599609375
aF2.970425720214844
aF2.951168212890625
aF2.9385836791992186
aF2.9036599731445314
aF2.8980474853515625
aF2.870614318847656
aF2.8718026733398436
aF2.852821960449219
aF2.8490896606445313
aF2.841109313964844
aF2.8131817626953124
aF2.78503662109375
aF2.797196960449219
aF2.788033447265625
aF2.753387145996094
aF2.74432373046875
aF2.734219665527344
aF2.7268338012695312
aF2.7163601684570313
aF2.7122418212890627
aF2.6999542236328127
aF2.6921621704101564
aF2.687557373046875
aF2.6795864868164063
aF2.6321844482421874
aF2.6687109375
aF2.641871337890625
aF2.6225823974609375
aF2.6288076782226564
aF2.614125061035156
aF2.6291259765625
aF2.6119448852539064
aF2.5776708984375
aF2.5682803344726564
aF2.5634494018554688
aF2.5784906005859374
aF2.5722140502929687
aF2.5317121887207032
aF2.5487179565429687
aF2.539867401123047
aF2.511222686767578
aF2.5187158203125
aF2.5376231384277346
aF2.5200875854492186
aF2.519805145263672
aF2.505656433105469
aF2.4906982421875
aF2.490272979736328
aF2.489121398925781
aF2.478963623046875
aF2.476954040527344
aF2.474851379394531
aF2.4541827392578126
aF2.4497015380859377
aF2.4630368041992186
aF2.462109375
aF2.446005096435547
aF2.457533111572266
aF2.430975341796875
aF2.442213287353516
aF2.437181854248047
aF2.432974853515625
aF2.4448606872558596
aF2.42098876953125
aF2.413776397705078
aF2.4069563293457032
aF2.4132511901855467
aF2.4220643615722657
aF2.3992271423339844
aF2.4017408752441405
aF2.3934675598144532
aF2.3939805603027344
asS'test_loss'
p3
(lp4
F4.583134155273438
aF4.496241760253906
aF4.425108337402344
aF4.33734619140625
aF4.256980590820312
aF4.162640991210938
aF4.076512145996094
aF3.9830743408203126
aF3.881200866699219
aF3.763612060546875
aF3.658585205078125
aF3.5559368896484376
aF3.454598693847656
aF3.374781188964844
aF3.2856765747070313
aF3.2511477661132813
aF3.1939773559570312
aF3.196446838378906
aF3.1605999755859373
aF3.1471710205078125
aF3.1327920532226563
aF3.07544189453125
aF3.0980908203125
aF3.0764056396484376
aF3.0471514892578124
aF3.020055847167969
aF2.9964666748046875
aF2.9802386474609377
aF2.9377035522460937
aF2.912353515625
aF2.9023089599609375
aF2.8849688720703126
aF2.8619403076171874
aF2.866784973144531
aF2.8461489868164063
aF2.842408447265625
aF2.830019836425781
aF2.807286376953125
aF2.7946212768554686
aF2.7636773681640623
aF2.777205810546875
aF2.7774700927734375
aF2.7296221923828123
aF2.7357763671875
aF2.7196450805664063
aF2.7112591552734373
aF2.697537841796875
aF2.6799212646484376
aF2.68573486328125
aF2.6722341918945314
aF2.6680987548828123
aF2.654341125488281
aF2.62875732421875
aF2.62725341796875
aF2.6159127807617186
aF2.6205514526367186
aF2.5988812255859375
aF2.596338806152344
aF2.59644287109375
aF2.5963931274414063
aF2.576349182128906
aF2.562666015625
aF2.5496420288085937
aF2.542818908691406
aF2.5609530639648437
aF2.5456782531738282
aF2.560940856933594
aF2.531547088623047
aF2.5331236267089845
aF2.519255828857422
aF2.5109417724609373
aF2.490486907958984
aF2.483471221923828
aF2.5143917846679686
aF2.4801785278320314
aF2.4847508239746094
aF2.5085997009277343
aF2.481892852783203
aF2.460987396240234
aF2.4490293884277343
aF2.4546205139160158
aF2.454057312011719
aF2.465025634765625
aF2.449281768798828
aF2.419233856201172
aF2.443601531982422
aF2.4236566162109376
aF2.4282763671875
aF2.428608856201172
aF2.4044459533691405
aF2.4220196533203127
aF2.4157318115234374
aF2.4272247314453126
aF2.4036679077148437
aF2.401131286621094
aF2.4010377502441407
aF2.4006800842285156
aF2.3790025329589843
aF2.390854949951172
aF2.391368865966797
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0006170526510456075
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 1s'
p10
sS'final_test_loss'
p11
F2.391368865966797
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xab\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.