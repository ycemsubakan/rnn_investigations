(dp0
S'train_loss'
p1
(lp2
F4.606622924804688
aF4.154113159179688
aF3.5802044677734375
aF3.3027713012695314
aF3.2211944580078127
aF3.0676943969726564
aF2.9681561279296873
aF2.8540658569335937
aF2.8182382202148437
aF2.7728826904296877
aF2.718504638671875
aF2.6679376220703124
aF2.6150537109375
aF2.568984069824219
aF2.5509323120117187
aF2.540314178466797
aF2.48525634765625
aF2.446184539794922
aF2.423387908935547
aF2.4201954650878905
aF2.404123077392578
aF2.366279296875
aF2.362021026611328
aF2.3205943298339844
aF2.302786407470703
aF2.3198252868652345
aF2.2896734619140626
aF2.295737457275391
aF2.2628207397460938
aF2.2451963806152344
aF2.244931182861328
aF2.236422424316406
aF2.2254573059082032
aF2.2291180419921877
aF2.196774139404297
aF2.180043487548828
aF2.1902009582519533
aF2.190598602294922
aF2.1531410217285156
aF2.166273498535156
aF2.161778869628906
aF2.1784040832519533
aF2.1272317504882814
aF2.135564270019531
aF2.1437855529785157
aF2.1024211120605467
aF2.098397216796875
aF2.1056663513183596
aF2.105387725830078
aF2.113375701904297
aF2.074496154785156
aF2.0832289123535155
aF2.065723876953125
aF2.0651019287109373
aF2.0472471618652346
aF2.037741394042969
aF2.0226722717285157
aF2.053794708251953
aF2.025188751220703
aF2.01914794921875
aF2.009469451904297
aF1.9956292724609375
aF2.0181201171875
aF2.0023150634765625
aF1.9959004211425782
aF1.9846612548828124
aF1.983265380859375
aF1.9712318420410155
aF1.9863600158691406
aF1.9555303955078125
aF1.9820741271972657
aF1.982945556640625
aF1.9780949401855468
aF1.9583604431152344
aF1.9270529174804687
aF1.9381625366210937
aF1.9442897033691406
aF1.9491566467285155
aF1.9276048278808593
aF1.8970231628417968
aF1.923785400390625
aF1.9263632202148437
aF1.91108642578125
aF1.9188862609863282
aF1.8894355773925782
aF1.9037554931640626
aF1.90119140625
aF1.883335418701172
aF1.8867747497558593
aF1.9057637023925782
aF1.8672257995605468
aF1.903212890625
aF1.8780990600585938
aF1.8800482177734374
aF1.8832606506347656
aF1.872461700439453
aF1.845557403564453
aF1.8667225646972656
aF1.8788668823242187
aF1.8496382141113281
asS'test_loss'
p3
(lp4
F4.151187133789063
aF3.575783996582031
aF3.33597412109375
aF3.237299499511719
aF3.0636846923828127
aF2.8909490966796874
aF2.8608746337890625
aF2.8010574340820313
aF2.747325744628906
aF2.688288269042969
aF2.6631015014648436
aF2.6083428955078123
aF2.580921936035156
aF2.5420463562011717
aF2.541302032470703
aF2.4837464904785156
aF2.452519989013672
aF2.4151966857910154
aF2.40613037109375
aF2.385164947509766
aF2.3699200439453123
aF2.372097625732422
aF2.319336242675781
aF2.3093911743164064
aF2.301246337890625
aF2.301114959716797
aF2.26401123046875
aF2.2692750549316405
aF2.248030242919922
aF2.2360882568359375
aF2.2294483947753907
aF2.206453552246094
aF2.226694793701172
aF2.205870819091797
aF2.19303955078125
aF2.1917263793945314
aF2.153301239013672
aF2.1533055114746094
aF2.163677520751953
aF2.1598158264160157
aF2.1465673828125
aF2.133700714111328
aF2.1295458984375
aF2.1220281982421874
aF2.1088153076171876
aF2.102114562988281
aF2.0621937561035155
aF2.0874533081054687
aF2.075501708984375
aF2.068913879394531
aF2.0853236389160155
aF2.0661521911621095
aF2.074270477294922
aF2.040102691650391
aF2.026769104003906
aF2.023336944580078
aF2.02608154296875
aF2.015985107421875
aF2.021590270996094
aF2.0236027526855467
aF2.016172332763672
aF2.008466949462891
aF2.00751953125
aF1.9694973754882812
aF1.9867388916015625
aF1.9771156311035156
aF1.981209716796875
aF1.9577890014648438
aF1.971814422607422
aF1.9593927001953124
aF1.9596917724609375
aF1.9713983154296875
aF1.9549153137207032
aF1.9440866088867188
aF1.9528581237792968
aF1.9299226379394532
aF1.9255165100097655
aF1.9207008361816407
aF1.9330734252929687
aF1.8985049438476562
aF1.889949188232422
aF1.9087930297851563
aF1.9091513061523437
aF1.896835174560547
aF1.9122087097167968
aF1.8885964965820312
aF1.9283247375488282
aF1.8971670532226563
aF1.9001853942871094
aF1.8729957580566405
aF1.8989889526367187
aF1.8643418884277343
aF1.8805384826660156
aF1.872440643310547
aF1.8422279357910156
aF1.8634437561035155
aF1.8591581726074218
aF1.8515196228027344
aF1.8399566650390624
aF1.8504100036621094
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0019319328456369885
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 33s'
p10
sS'final_test_loss'
p11
F1.8504100036621094
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x1d\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.