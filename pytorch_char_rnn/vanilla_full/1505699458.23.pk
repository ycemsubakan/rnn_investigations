(dp0
S'train_loss'
p1
(lp2
F4.607846984863281
aF3.8723675537109377
aF3.2055047607421874
aF3.0091073608398435
aF2.874353942871094
aF2.712616271972656
aF2.6800003051757812
aF2.6325048828125
aF2.6028012084960936
aF2.554444580078125
aF2.530061950683594
aF2.481405944824219
aF2.468256072998047
aF2.446705322265625
aF2.400390319824219
aF2.3800283813476564
aF2.3657781982421877
aF2.366765899658203
aF2.3175450134277344
aF2.3289146423339844
aF2.2985321044921876
aF2.2846003723144532
aF2.2880609130859373
aF2.2410629272460936
aF2.2316554260253905
aF2.2196673583984374
aF2.2282478332519533
aF2.1940040588378906
aF2.195709228515625
aF2.189331512451172
aF2.1535160827636717
aF2.1778594970703127
aF2.148672790527344
aF2.1400662231445313
aF2.133433532714844
aF2.1342958068847655
aF2.104694519042969
aF2.1080235290527343
aF2.1096092224121095
aF2.106644592285156
aF2.109843444824219
aF2.114402160644531
aF2.0609707641601562
aF2.077513427734375
aF2.082306213378906
aF2.0547589111328124
aF2.0378135681152343
aF2.0563673400878906
aF2.0401646423339845
aF2.0352915954589843
aF2.049979248046875
aF2.0293728637695314
aF2.039566955566406
aF2.0260604858398437
aF2.0108799743652344
aF1.9965081787109376
aF1.9807595825195312
aF1.9773382568359374
aF1.9751950073242188
aF1.9769357299804688
aF1.9604750061035157
aF1.9596382141113282
aF1.9791505432128906
aF1.9573269653320313
aF1.9554107666015625
aF1.9611521911621095
aF1.9674444580078125
aF1.9568154907226563
aF1.9295259094238282
aF1.9572454833984374
aF1.9185342407226562
aF1.9428500366210937
aF1.9093072509765625
aF1.895338592529297
aF1.93178466796875
aF1.9078317260742188
aF1.9229454040527343
aF1.9061631774902343
aF1.9085292053222656
aF1.9013845825195312
aF1.9120753479003907
aF1.90155517578125
aF1.9102529907226562
aF1.883016357421875
aF1.8912765502929687
aF1.8769624328613281
aF1.8773818969726563
aF1.878949432373047
aF1.8594456481933594
aF1.8752394104003907
aF1.8704600524902344
aF1.8598973083496093
aF1.8560057067871094
aF1.8489755249023438
aF1.857622833251953
aF1.8408273315429688
aF1.8251730346679687
aF1.8660614013671875
aF1.8199131774902344
aF1.8325416564941406
asS'test_loss'
p3
(lp4
F3.867231750488281
aF3.2014309692382814
aF3.00202392578125
aF2.854262390136719
aF2.7517620849609377
aF2.697579345703125
aF2.615439147949219
aF2.555328369140625
aF2.5574560546875
aF2.5172773742675782
aF2.476216125488281
aF2.447152099609375
aF2.427409362792969
aF2.4121514892578126
aF2.3798374938964844
aF2.36503173828125
aF2.3268693542480468
aF2.3189375305175783
aF2.3068580627441406
aF2.2934584045410156
aF2.262767791748047
aF2.2436085510253907
aF2.249239501953125
aF2.24291259765625
aF2.2588761901855468
aF2.2166378784179686
aF2.1829689025878904
aF2.1817977905273436
aF2.1844224548339843
aF2.1723971557617188
aF2.1534051513671875
aF2.1310182189941407
aF2.142131042480469
aF2.1201016235351564
aF2.1187399291992186
aF2.127135925292969
aF2.1296942138671877
aF2.120789337158203
aF2.0870115661621096
aF2.0808372497558594
aF2.0785458374023436
aF2.0739312744140626
aF2.076172332763672
aF2.0706126403808596
aF2.042910614013672
aF2.04677490234375
aF2.049928894042969
aF2.0533839416503907
aF2.036778869628906
aF2.0282028198242186
aF2.0204129028320312
aF2.031575164794922
aF1.9974127197265625
aF2.0062355041503905
aF1.9982400512695313
aF1.9848698425292968
aF1.9958839416503906
aF1.9789495849609375
aF1.9679212951660157
aF1.9633999633789063
aF1.9549635314941407
aF1.9533958435058594
aF1.9594194030761718
aF1.9511074829101562
aF1.9486448669433594
aF1.9722271728515626
aF1.9425498962402343
aF1.9296578979492187
aF1.9499433898925782
aF1.9163435363769532
aF1.9284272766113282
aF1.9198405456542968
aF1.9135255432128906
aF1.9219927978515625
aF1.9236152648925782
aF1.911257781982422
aF1.9057295227050781
aF1.9214860534667968
aF1.8900485229492188
aF1.8897760009765625
aF1.8751498413085939
aF1.8849171447753905
aF1.8754689025878906
aF1.9105557250976561
aF1.8503851318359374
aF1.891322021484375
aF1.8646543884277345
aF1.897017059326172
aF1.861122589111328
aF1.839564666748047
aF1.854437255859375
aF1.8572018432617188
aF1.8811848449707032
aF1.8832017517089843
aF1.847326202392578
aF1.8270193481445312
aF1.8596278381347657
aF1.8604071044921875
aF1.8385304260253905
aF1.8301951599121093
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004283073389749089
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 20s'
p10
sS'final_test_loss'
p11
F1.8301951599121093
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf7\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.