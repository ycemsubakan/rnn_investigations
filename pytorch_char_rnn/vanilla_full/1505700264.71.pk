(dp0
S'train_loss'
p1
(lp2
F4.599607849121094
aF3.854505920410156
aF3.205000305175781
aF3.011405029296875
aF2.824024353027344
aF2.7683511352539063
aF2.685530090332031
aF2.6233404541015624
aF2.5748516845703127
aF2.537792510986328
aF2.5056439208984376
aF2.4484812927246096
aF2.461222839355469
aF2.3866554260253907
aF2.399319763183594
aF2.3765995788574217
aF2.3289564514160155
aF2.339837646484375
aF2.309237060546875
aF2.297598571777344
aF2.2608738708496094
aF2.2709086608886717
aF2.264231719970703
aF2.248968048095703
aF2.229109039306641
aF2.239419708251953
aF2.209283905029297
aF2.1869746398925782
aF2.177306365966797
aF2.173645477294922
aF2.1446574401855467
aF2.1582859802246093
aF2.1359078979492185
aF2.1396217346191406
aF2.104140167236328
aF2.123302764892578
aF2.1224404907226564
aF2.0903045654296877
aF2.0881854248046876
aF2.081683349609375
aF2.0698538208007813
aF2.082832489013672
aF2.074849090576172
aF2.073548126220703
aF2.0498417663574218
aF2.03746826171875
aF2.0660108947753906
aF2.0260096740722657
aF2.0389303588867187
aF2.0217379760742187
aF1.9965737915039063
aF2.0203366088867187
aF2.006613922119141
aF2.0061708068847657
aF1.9971759033203125
aF1.9779782104492187
aF1.9935760498046875
aF1.9638571166992187
aF1.9635585021972657
aF1.9793565368652344
aF1.976248779296875
aF1.955633087158203
aF1.9795985412597656
aF1.9807916259765626
aF1.9436129760742187
aF1.9474504089355469
aF1.9696206665039062
aF1.9463323974609374
aF1.9366226196289062
aF1.9365455627441406
aF1.9297087097167969
aF1.93452392578125
aF1.9253244018554687
aF1.9163912963867187
aF1.9239547729492188
aF1.9189833068847657
aF1.9017189025878907
aF1.932670135498047
aF1.9149372863769532
aF1.892308807373047
aF1.9062771606445312
aF1.8901911926269532
aF1.9025039672851562
aF1.899508514404297
aF1.8763999938964844
aF1.866520538330078
aF1.8822076416015625
aF1.8613552856445312
aF1.8487974548339843
aF1.8683319091796875
aF1.8653480529785156
aF1.8817083740234375
aF1.8755491638183595
aF1.864759521484375
aF1.8689898681640624
aF1.8543508911132813
aF1.8634036254882813
aF1.8537660217285157
aF1.8632499694824218
aF1.840792694091797
asS'test_loss'
p3
(lp4
F3.8269882202148438
aF3.1502587890625
aF2.98871337890625
aF2.820007019042969
aF2.7662454223632813
aF2.6739501953125
aF2.5973828125
aF2.5515696716308596
aF2.5138670349121095
aF2.4954878234863282
aF2.462857360839844
aF2.4169937133789063
aF2.395478515625
aF2.364393463134766
aF2.36031494140625
aF2.332627410888672
aF2.3019747924804688
aF2.3000619506835935
aF2.2779884338378906
aF2.2770622253417967
aF2.2558049011230468
aF2.2316656494140625
aF2.2228387451171874
aF2.232230987548828
aF2.197209930419922
aF2.191932373046875
aF2.1767784118652345
aF2.16460693359375
aF2.1590533447265625
aF2.133614501953125
aF2.1418324279785157
aF2.1220079040527344
aF2.144630889892578
aF2.095570831298828
aF2.1100086975097656
aF2.0890791320800783
aF2.097794647216797
aF2.0974215698242187
aF2.0783847045898436
aF2.085429229736328
aF2.0945002746582033
aF2.057314453125
aF2.0722944641113283
aF2.034062957763672
aF2.0393064880371092
aF2.0365476989746094
aF2.0658062744140624
aF2.0359576416015623
aF2.0264828491210936
aF2.0077806091308594
aF1.9800009155273437
aF2.0125910949707033
aF2.00330322265625
aF1.9746775817871094
aF1.9870437622070312
aF1.9886965942382813
aF1.984031982421875
aF1.97578369140625
aF1.9718760681152343
aF1.9666488647460938
aF1.9635421752929687
aF1.9626976013183595
aF1.9577548217773437
aF1.9601905822753907
aF1.9709396362304688
aF1.9413499450683593
aF1.9519108581542968
aF1.9102842712402344
aF1.908365478515625
aF1.9243310546875
aF1.917069091796875
aF1.938074493408203
aF1.90098876953125
aF1.9220892333984374
aF1.9038877868652344
aF1.9160533142089844
aF1.8917926025390626
aF1.8893125915527345
aF1.9015138244628906
aF1.8945307922363281
aF1.9137503051757812
aF1.8816734313964845
aF1.8681449890136719
aF1.9064854431152343
aF1.8653021240234375
aF1.8726991271972657
aF1.870928955078125
aF1.882870635986328
aF1.86947509765625
aF1.8625747680664062
aF1.8567172241210939
aF1.8651788330078125
aF1.8310755920410156
aF1.868302001953125
aF1.854974822998047
aF1.8765261840820313
aF1.8705288696289062
aF1.847572784423828
aF1.8521745300292969
aF1.8358328247070312
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.005778562314258646
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 8s'
p10
sS'final_test_loss'
p11
F1.8358328247070312
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xc6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.