(dp0
S'train_loss'
p1
(lp2
F4.615986022949219
aF4.077943420410156
aF3.4136151123046874
aF3.2678012084960937
aF3.170740661621094
aF3.0084100341796876
aF2.9525985717773438
aF2.8043276977539064
aF2.744364929199219
aF2.711805725097656
aF2.6887478637695312
aF2.6259765625
aF2.5677447509765625
aF2.550912170410156
aF2.5232484436035154
aF2.476129913330078
aF2.4556068420410155
aF2.4381845092773435
aF2.3981752014160156
aF2.3761407470703126
aF2.3719773864746094
aF2.3446315002441405
aF2.3454249572753905
aF2.316468505859375
aF2.32147705078125
aF2.281283111572266
aF2.267761993408203
aF2.262015533447266
aF2.2385763549804687
aF2.232704925537109
aF2.2576580810546876
aF2.2119580078125
aF2.2074703979492187
aF2.192178192138672
aF2.1845994567871094
aF2.1612977600097656
aF2.146742706298828
aF2.1491297912597656
aF2.141222381591797
aF2.1372898864746093
aF2.142204132080078
aF2.109971771240234
aF2.1052142333984376
aF2.127560577392578
aF2.091095733642578
aF2.059539337158203
aF2.073649139404297
aF2.069669189453125
aF2.074634552001953
aF2.0693585205078127
aF2.021287536621094
aF2.054199981689453
aF2.0389082336425783
aF2.0188862609863283
aF2.044449462890625
aF2.011305389404297
aF2.017975616455078
aF1.9957472229003905
aF2.015717010498047
aF1.9791450500488281
aF1.9866360473632811
aF1.9916061401367187
aF1.9908294677734375
aF1.9928512573242188
aF1.9553854370117187
aF1.9527891540527345
aF1.9490071105957032
aF1.9479182434082032
aF1.9502458190917968
aF1.9335920715332031
aF1.9131053161621094
aF1.9522993469238281
aF1.9489114379882813
aF1.9176603698730468
aF1.9377656555175782
aF1.8855180358886718
aF1.90296630859375
aF1.9298388671875
aF1.8938104248046874
aF1.8735208129882812
aF1.8996983337402344
aF1.9009944152832032
aF1.8894505310058594
aF1.883048095703125
aF1.8929452514648437
aF1.8812026977539062
aF1.8765498352050782
aF1.8712098693847656
aF1.870154571533203
aF1.839146728515625
aF1.870531768798828
aF1.8330470275878907
aF1.8161245727539062
aF1.8579232788085938
aF1.8420707702636718
aF1.8510539245605468
aF1.8038258361816406
aF1.8319517517089843
aF1.8288470458984376
aF1.8260006713867187
asS'test_loss'
p3
(lp4
F4.08109375
aF3.4185641479492186
aF3.2647799682617187
aF3.171936340332031
aF3.0133026123046873
aF2.8966412353515625
aF2.811426086425781
aF2.767657470703125
aF2.709073791503906
aF2.674080505371094
aF2.627990417480469
aF2.5805899047851564
aF2.521228942871094
aF2.4971678161621096
aF2.4727494812011717
aF2.443946533203125
aF2.4183306884765625
aF2.386133575439453
aF2.371600341796875
aF2.3761917114257813
aF2.329037933349609
aF2.3170091247558595
aF2.317027740478516
aF2.291598663330078
aF2.261266326904297
aF2.272444000244141
aF2.2573782348632814
aF2.245691680908203
aF2.228975982666016
aF2.207541809082031
aF2.1920001220703127
aF2.196855926513672
aF2.181629943847656
aF2.175568084716797
aF2.1640899658203123
aF2.146900482177734
aF2.1443714904785156
aF2.125314178466797
aF2.126725921630859
aF2.1138250732421877
aF2.101199951171875
aF2.0969232177734374
aF2.0844000244140624
aF2.069166412353516
aF2.0834309387207033
aF2.076427764892578
aF2.059582061767578
aF2.0697589111328125
aF2.0665065002441407
aF2.052042236328125
aF2.044606475830078
aF2.0453067016601563
aF2.0326081848144533
aF2.0256027221679687
aF2.0368589782714843
aF1.9978677368164062
aF1.9984060668945312
aF1.9967308044433594
aF1.9959806823730468
aF1.9800848388671874
aF1.9763836669921875
aF1.963095703125
aF1.972568359375
aF1.9595484924316406
aF1.9466725158691407
aF1.9325228881835939
aF1.9620394897460938
aF1.9486790466308594
aF1.9312234497070313
aF1.9292500305175782
aF1.909186553955078
aF1.9174894714355468
aF1.9257859802246093
aF1.918474884033203
aF1.921253662109375
aF1.91272705078125
aF1.906684112548828
aF1.898918914794922
aF1.89784912109375
aF1.893998260498047
aF1.9069850158691406
aF1.8746212768554686
aF1.8775361633300782
aF1.8742788696289063
aF1.8682908630371093
aF1.8548841857910157
aF1.8507765197753907
aF1.8668678283691407
aF1.8715695190429686
aF1.855869140625
aF1.8419668579101562
aF1.8469342041015624
aF1.841077880859375
aF1.836046905517578
aF1.8350030517578124
aF1.8436187744140624
aF1.8218087768554687
aF1.8221827697753907
aF1.825737762451172
aF1.8102481079101562
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0023334562286509587
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 20s'
p10
sS'final_test_loss'
p11
F1.8102481079101562
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x0e\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.