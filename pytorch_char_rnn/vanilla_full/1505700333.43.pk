(dp0
S'train_loss'
p1
(lp2
F4.666415710449218
aF3.725699157714844
aF3.159056396484375
aF3.0349957275390627
aF2.9030389404296875
aF2.7551171875
aF2.6714593505859376
aF2.620802917480469
aF2.5918731689453125
aF2.530653228759766
aF2.489550018310547
aF2.4603175354003906
aF2.446144714355469
aF2.436844482421875
aF2.379081268310547
aF2.3421527099609376
aF2.3303282165527346
aF2.3329185485839843
aF2.3125898742675783
aF2.2821586608886717
aF2.253894805908203
aF2.2594148254394533
aF2.2446832275390625
aF2.232022552490234
aF2.2167207336425783
aF2.202325744628906
aF2.1801348876953126
aF2.171195220947266
aF2.1829884338378904
aF2.164264373779297
aF2.129901885986328
aF2.152257385253906
aF2.1222300720214844
aF2.128220977783203
aF2.117614440917969
aF2.095950012207031
aF2.1066249084472655
aF2.086376037597656
aF2.085233001708984
aF2.0514956665039064
aF2.060508270263672
aF2.0606199645996095
aF2.06088134765625
aF2.0252420043945314
aF2.0398385620117185
aF2.0254345703125
aF2.0066256713867188
aF2.0208013916015624
aF2.0064797973632813
aF2.009066619873047
aF1.9949884033203125
aF1.9742959594726563
aF1.988152313232422
aF1.977689208984375
aF1.9775634765625
aF1.9845387268066406
aF1.980576171875
aF1.9715069580078124
aF1.9553105163574218
aF1.9633651733398438
aF1.9431648254394531
aF1.946939697265625
aF1.9470623779296874
aF1.93549560546875
aF1.9087879943847657
aF1.9499923706054687
aF1.908927001953125
aF1.9176712036132812
aF1.89998779296875
aF1.9206431579589844
aF1.9037370300292968
aF1.8949652099609375
aF1.898272247314453
aF1.8816383361816407
aF1.8970458984375
aF1.8800552368164063
aF1.875604705810547
aF1.87721435546875
aF1.8545089721679688
aF1.8881431579589845
aF1.8686843872070313
aF1.8476779174804687
aF1.8758717346191407
aF1.8476466369628906
aF1.8552809143066407
aF1.84861572265625
aF1.8510360717773438
aF1.8444111633300782
aF1.835202178955078
aF1.8508131408691406
aF1.8172503662109376
aF1.8076066589355468
aF1.843591766357422
aF1.8105857849121094
aF1.8310691833496093
aF1.8158578491210937
aF1.8169168090820313
aF1.808375701904297
aF1.799383544921875
aF1.815921630859375
asS'test_loss'
p3
(lp4
F3.728648681640625
aF3.172510070800781
aF3.0275418090820314
aF2.876321105957031
aF2.7412750244140627
aF2.628697814941406
aF2.605010986328125
aF2.5504095458984377
aF2.5178321838378905
aF2.478904113769531
aF2.449461669921875
aF2.4222654724121093
aF2.4019395446777345
aF2.3827220153808595
aF2.3644598388671874
aF2.3386575317382814
aF2.314837646484375
aF2.324091339111328
aF2.271112365722656
aF2.2768495178222654
aF2.252196960449219
aF2.2276368713378907
aF2.2464633178710938
aF2.2116873168945315
aF2.190541229248047
aF2.1949192810058595
aF2.162513885498047
aF2.1897787475585937
aF2.161182098388672
aF2.1454739379882812
aF2.1450006103515626
aF2.140758361816406
aF2.107333221435547
aF2.107030944824219
aF2.1068191528320312
aF2.0930841064453123
aF2.072152099609375
aF2.0804209899902344
aF2.0462998962402343
aF2.06857177734375
aF2.0555711364746094
aF2.049832763671875
aF2.028127899169922
aF2.0308152770996095
aF2.0094281005859376
aF2.0353077697753905
aF2.013224639892578
aF2.0122271728515626
aF1.9969813537597656
aF1.9811329650878906
aF1.965520477294922
aF1.9890443420410155
aF1.9774662780761718
aF1.9875726318359375
aF1.9767987060546874
aF1.981727294921875
aF1.9523808288574218
aF1.9496041870117187
aF1.93666748046875
aF1.9403977966308594
aF1.9282675170898438
aF1.950430908203125
aF1.9494929504394531
aF1.928774871826172
aF1.9247314453125
aF1.896727294921875
aF1.9222000122070313
aF1.92655029296875
aF1.9156959533691407
aF1.9090493774414063
aF1.883385467529297
aF1.9086369323730468
aF1.9065644836425781
aF1.8907806396484375
aF1.8950898742675781
aF1.8753462219238282
aF1.8722381591796875
aF1.866208038330078
aF1.890089569091797
aF1.876131591796875
aF1.886294403076172
aF1.8759584045410156
aF1.838091583251953
aF1.8540740966796876
aF1.8673602294921876
aF1.8439337158203124
aF1.8795501708984375
aF1.8431086730957031
aF1.8320845031738282
aF1.8593194580078125
aF1.818175048828125
aF1.829429168701172
aF1.8369938659667968
aF1.82429931640625
aF1.8162971496582032
aF1.841700439453125
aF1.8068362426757814
aF1.8013905334472655
aF1.8430586242675782
aF1.8194465637207031
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008992747856191889
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 8s'
p10
sS'final_test_loss'
p11
F1.8194465637207031
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xca\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.