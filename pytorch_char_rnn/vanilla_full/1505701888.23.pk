(dp0
S'train_loss'
p1
(lp2
F4.637260437011719
aF3.782725830078125
aF3.168541259765625
aF3.17689697265625
aF3.1343142700195314
aF2.9430645751953124
aF2.8195843505859375
aF2.746201477050781
aF2.703709716796875
aF2.6139105224609374
aF2.5692062377929688
aF2.5597607421875
aF2.5202592468261718
aF2.4649578857421877
aF2.450423126220703
aF2.412111511230469
aF2.4007192993164064
aF2.3601058959960937
aF2.35308837890625
aF2.3124119567871095
aF2.307923126220703
aF2.267600402832031
aF2.2705332946777346
aF2.230787353515625
aF2.217046203613281
aF2.230092315673828
aF2.1852171325683596
aF2.1970144653320314
aF2.1864715576171876
aF2.1489431762695315
aF2.135859375
aF2.138724365234375
aF2.103058166503906
aF2.1035398864746093
aF2.081622314453125
aF2.097185516357422
aF2.0951194763183594
aF2.0658116149902344
aF2.0538980102539064
aF2.0686497497558594
aF2.045160217285156
aF2.0290773010253904
aF2.0131727600097657
aF2.0332867431640627
aF2.0099896240234374
aF1.985803985595703
aF1.981463623046875
aF2.006233673095703
aF1.954514617919922
aF1.969161834716797
aF1.9685467529296874
aF1.9402688598632813
aF1.9565751647949219
aF1.9445431518554688
aF1.9441371154785156
aF1.9183004760742188
aF1.9112509155273438
aF1.9172549438476563
aF1.8930267333984374
aF1.8897311401367187
aF1.9029859924316406
aF1.900784912109375
aF1.8884559631347657
aF1.8887203979492186
aF1.8751625061035155
aF1.8615597534179686
aF1.8509486389160157
aF1.8687106323242189
aF1.8288542175292968
aF1.844276123046875
aF1.8412982177734376
aF1.8365994262695313
aF1.813328399658203
aF1.8351756286621095
aF1.8037091064453126
aF1.8216873168945313
aF1.79677734375
aF1.8065936279296875
aF1.8425782775878907
aF1.820122528076172
aF1.8076693725585937
aF1.7859446716308593
aF1.777989044189453
aF1.7679110717773439
aF1.788070068359375
aF1.7821705627441407
aF1.780501251220703
aF1.777826690673828
aF1.7650286865234375
aF1.7789614868164063
aF1.7739991760253906
aF1.775578155517578
aF1.756981201171875
aF1.7400758361816406
aF1.741696319580078
aF1.735801239013672
aF1.75490478515625
aF1.7531890869140625
aF1.7275811767578124
aF1.7301702880859375
asS'test_loss'
p3
(lp4
F3.7839764404296874
aF3.174017333984375
aF3.15873291015625
aF3.13505615234375
aF2.9205780029296875
aF2.8027294921875
aF2.740364685058594
aF2.6955389404296874
aF2.61281005859375
aF2.5828289794921875
aF2.5412889099121094
aF2.4740451049804686
aF2.4515037536621094
aF2.4381211853027343
aF2.382645568847656
aF2.3665815734863282
aF2.3376116943359375
aF2.3286988830566404
aF2.308249969482422
aF2.3004803466796875
aF2.261726837158203
aF2.2437353515625
aF2.2585711669921875
aF2.209153137207031
aF2.220647277832031
aF2.1884169006347656
aF2.189088897705078
aF2.1603668212890623
aF2.1622227478027343
aF2.1431655883789062
aF2.151842498779297
aF2.121480712890625
aF2.105568542480469
aF2.095722351074219
aF2.066614685058594
aF2.0690367126464846
aF2.0666558837890623
aF2.0501033020019532
aF2.0601545715332032
aF2.0186781311035156
aF2.0211585998535155
aF2.0177301025390624
aF2.016050262451172
aF2.0099696350097656
aF1.9831698608398438
aF1.9830012512207031
aF2.0004202270507814
aF1.9749728393554689
aF1.9453036499023437
aF1.9545272827148437
aF1.9279960632324218
aF1.945583038330078
aF1.908647003173828
aF1.9139981079101562
aF1.9342880249023438
aF1.9248371887207032
aF1.9157913208007813
aF1.8978387451171874
aF1.910680389404297
aF1.9108692932128906
aF1.9115412902832032
aF1.870821533203125
aF1.8745008850097655
aF1.8690489196777345
aF1.8741682434082032
aF1.8532920837402345
aF1.8711985778808593
aF1.8398675537109375
aF1.8415544128417969
aF1.85798095703125
aF1.830831298828125
aF1.8516171264648438
aF1.8327757263183593
aF1.8237733459472656
aF1.8314837646484374
aF1.8367767333984375
aF1.8515769958496093
aF1.8097943115234374
aF1.8263575744628906
aF1.8054743957519532
aF1.7907595825195313
aF1.7950469970703125
aF1.8058523559570312
aF1.7709898376464843
aF1.7862290954589843
aF1.7970944213867188
aF1.7513453674316406
aF1.7944969177246093
aF1.7751976013183595
aF1.773779296875
aF1.7775236511230468
aF1.7469393920898437
aF1.7556910705566406
aF1.7399844360351562
aF1.7513638305664063
aF1.7478825378417968
aF1.763504180908203
aF1.7753155517578125
aF1.7553350830078125
aF1.772316436767578
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.008197550406376902
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 2s'
p10
sS'final_test_loss'
p11
F1.772316436767578
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.