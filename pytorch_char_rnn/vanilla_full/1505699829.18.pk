(dp0
S'train_loss'
p1
(lp2
F4.630445556640625
aF4.550888671875
aF4.478787841796875
aF4.39078125
aF4.299779052734375
aF4.199888916015625
aF4.080967712402344
aF3.9724472045898436
aF3.8460369873046876
aF3.7388677978515625
aF3.6693701171875
aF3.60876220703125
aF3.5302090454101562
aF3.4733462524414063
aF3.4408670043945313
aF3.404808349609375
aF3.3946517944335937
aF3.371856994628906
aF3.3221395874023436
aF3.2899624633789064
aF3.267603454589844
aF3.225374755859375
aF3.2014990234375
aF3.168582763671875
aF3.1496319580078125
aF3.109776611328125
aF3.1244021606445314
aF3.1150799560546876
aF3.0928070068359377
aF3.057415771484375
aF3.0162835693359376
aF3.008352966308594
aF3.013827209472656
aF2.985129699707031
aF2.967227783203125
aF2.949507141113281
aF2.930941162109375
aF2.9393572998046875
aF2.9123223876953124
aF2.872608337402344
aF2.8698135375976563
aF2.858719482421875
aF2.85385009765625
aF2.8319497680664063
aF2.8254360961914062
aF2.8142401123046876
aF2.7936691284179687
aF2.780851135253906
aF2.7838824462890623
aF2.77189453125
aF2.753096618652344
aF2.7313803100585936
aF2.7134628295898438
aF2.7119451904296876
aF2.6774057006835936
aF2.7230099487304686
aF2.6817398071289062
aF2.6701449584960937
aF2.6437261962890624
aF2.6519259643554687
aF2.6508084106445313
aF2.6333135986328124
aF2.631978759765625
aF2.6006402587890625
aF2.603140563964844
aF2.592282409667969
aF2.5797357177734375
aF2.5652560424804687
aF2.5785617065429687
aF2.570621337890625
aF2.5752044677734376
aF2.554196624755859
aF2.5308274841308593
aF2.5294322204589843
aF2.538807220458984
aF2.5285687255859375
aF2.516518859863281
aF2.5036717224121094
aF2.5153237915039064
aF2.509897918701172
aF2.473102722167969
aF2.5018663024902343
aF2.4777796936035155
aF2.4536288452148436
aF2.473539123535156
aF2.4538111877441406
aF2.4498944091796875
aF2.4304487609863283
aF2.439889678955078
aF2.42947509765625
aF2.439873046875
aF2.426907653808594
aF2.4024082946777345
aF2.42073974609375
aF2.418520965576172
aF2.4168040466308596
aF2.4143458557128907
aF2.414925537109375
aF2.3937744140625
aF2.4098651123046877
asS'test_loss'
p3
(lp4
F4.552223205566406
aF4.474547119140625
aF4.3819406127929685
aF4.301233215332031
aF4.198935852050782
aF4.082535095214844
aF3.973419494628906
aF3.848078308105469
aF3.749066162109375
aF3.6460720825195314
aF3.5895977783203126
aF3.5312838745117188
aF3.4993328857421875
aF3.43767822265625
aF3.40942626953125
aF3.393270263671875
aF3.3554562377929686
aF3.306064453125
aF3.2803631591796876
aF3.2366412353515623
aF3.218106384277344
aF3.1994528198242187
aF3.19426025390625
aF3.1611508178710936
aF3.1252310180664065
aF3.133908996582031
aF3.094915771484375
aF3.0660821533203126
aF3.062294921875
aF3.0004766845703124
aF2.992151184082031
aF2.9728985595703126
aF2.9681521606445314
aF2.9424179077148436
aF2.954627685546875
aF2.9178707885742186
aF2.919769592285156
aF2.881771240234375
aF2.8926983642578126
aF2.8555038452148436
aF2.8589947509765623
aF2.8503903198242186
aF2.8187030029296873
aF2.8034307861328127
aF2.803194580078125
aF2.7891436767578126
aF2.79332275390625
aF2.765921936035156
aF2.7711944580078125
aF2.740614929199219
aF2.7342959594726564
aF2.7090695190429686
aF2.6936810302734373
aF2.7066635131835937
aF2.6985699462890627
aF2.6712570190429688
aF2.6705081176757814
aF2.658720703125
aF2.6473825073242185
aF2.6310125732421876
aF2.61341552734375
aF2.6212509155273436
aF2.6091854858398436
aF2.5926614379882813
aF2.604966125488281
aF2.567698974609375
aF2.582994384765625
aF2.560838623046875
aF2.565335388183594
aF2.5569407653808596
aF2.550189666748047
aF2.5398187255859375
aF2.5057389831542967
aF2.539156036376953
aF2.513014068603516
aF2.5050653076171874
aF2.5078463745117188
aF2.4966680908203127
aF2.482736053466797
aF2.4764140319824217
aF2.480272216796875
aF2.4664166259765623
aF2.4681242370605467
aF2.4511177062988283
aF2.436565093994141
aF2.445668182373047
aF2.4432525634765625
aF2.4211692810058594
aF2.4391510009765627
aF2.413363037109375
aF2.409913330078125
aF2.4002651977539062
aF2.3979132080078127
aF2.4073170471191405
aF2.3964894104003904
aF2.3973489379882813
aF2.385265197753906
aF2.3863665771484377
aF2.386999053955078
aF2.385284423828125
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005658070929905263
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 36s'
p10
sS'final_test_loss'
p11
F2.385284423828125
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x9e\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.