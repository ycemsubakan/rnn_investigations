(dp0
S'train_loss'
p1
(lp2
F4.613365478515625
aF4.573395690917969
aF4.52838134765625
aF4.490013732910156
aF4.439530944824218
aF4.3844430541992185
aF4.321029968261719
aF4.255718994140625
aF4.166990966796875
aF4.076138610839844
aF3.977815856933594
aF3.895929260253906
aF3.810363464355469
aF3.7292330932617186
aF3.649991455078125
aF3.603233642578125
aF3.565740966796875
aF3.5391354370117187
aF3.4926983642578127
aF3.4481768798828125
aF3.42289306640625
aF3.39023193359375
aF3.3937982177734374
aF3.3770675659179688
aF3.34742919921875
aF3.3335101318359377
aF3.3217822265625
aF3.312314147949219
aF3.2981689453125
aF3.2817633056640627
aF3.222056884765625
aF3.2420587158203125
aF3.2282418823242187
aF3.200943603515625
aF3.189415283203125
aF3.17869873046875
aF3.165907897949219
aF3.163455810546875
aF3.122803039550781
aF3.109783020019531
aF3.1276669311523437
aF3.0847113037109377
aF3.0664971923828124
aF3.0576190185546874
aF3.046409912109375
aF3.019356689453125
aF3.022781982421875
aF3.0212786865234373
aF2.9740554809570314
aF2.9665692138671873
aF2.9524713134765626
aF2.9522329711914064
aF2.938438720703125
aF2.9218505859375
aF2.918723449707031
aF2.9155908203125
aF2.867626953125
aF2.872605895996094
aF2.887696533203125
aF2.849498596191406
aF2.8845834350585937
aF2.84343505859375
aF2.8284542846679686
aF2.82508056640625
aF2.784742736816406
aF2.812767333984375
aF2.787894287109375
aF2.783003234863281
aF2.7768701171875
aF2.7370953369140625
aF2.7493341064453123
aF2.7452789306640626
aF2.7450015258789064
aF2.71486328125
aF2.7061886596679687
aF2.7123907470703124
aF2.6879013061523436
aF2.675438232421875
aF2.6913720703125
aF2.660125732421875
aF2.66129150390625
aF2.6419732666015623
aF2.6548175048828124
aF2.647625732421875
aF2.620713195800781
aF2.6055569458007812
aF2.6243792724609376
aF2.5954669189453123
aF2.5909429931640626
aF2.597371826171875
aF2.5919485473632813
aF2.5823309326171877
aF2.5786874389648435
aF2.5679190063476565
aF2.56819580078125
aF2.536514892578125
aF2.554029693603516
aF2.539337615966797
aF2.5343829345703126
aF2.5326608276367186
asS'test_loss'
p3
(lp4
F4.574592895507813
aF4.530276794433593
aF4.487177124023438
aF4.439794311523437
aF4.383264770507813
aF4.3274710083007815
aF4.249350891113282
aF4.169460754394532
aF4.096873474121094
aF3.9795358276367185
aF3.8859402465820314
aF3.8011798095703124
aF3.7305078125
aF3.664040832519531
aF3.5921890258789064
aF3.573189392089844
aF3.53107421875
aF3.4905459594726564
aF3.460858154296875
aF3.41808349609375
aF3.39356689453125
aF3.3825588989257813
aF3.3456460571289064
aF3.3439785766601564
aF3.333307800292969
aF3.3111993408203126
aF3.289202880859375
aF3.2786599731445314
aF3.27442626953125
aF3.2212460327148436
aF3.2352996826171876
aF3.2277801513671873
aF3.193609619140625
aF3.163778076171875
aF3.170191650390625
aF3.150124816894531
aF3.1329190063476564
aF3.1225717163085935
aF3.125487976074219
aF3.0987103271484373
aF3.086629638671875
aF3.0703781127929686
aF3.03852783203125
aF3.0431857299804688
aF3.028681335449219
aF3.002724609375
aF2.990714416503906
aF2.993587646484375
aF2.9819140625
aF2.95081787109375
aF2.94375732421875
aF2.9241717529296873
aF2.926634521484375
aF2.9183819580078123
aF2.892139587402344
aF2.874378662109375
aF2.883226013183594
aF2.8751980590820314
aF2.871398620605469
aF2.84169921875
aF2.822870788574219
aF2.8331472778320315
aF2.802945861816406
aF2.791528625488281
aF2.7779791259765627
aF2.7795172119140625
aF2.7628643798828123
aF2.7577886962890625
aF2.762198181152344
aF2.7142138671875
aF2.7422946166992186
aF2.7277706909179686
aF2.7135696411132812
aF2.70765380859375
aF2.72710205078125
aF2.683253173828125
aF2.702926025390625
aF2.679933776855469
aF2.6686688232421876
aF2.6648941040039062
aF2.651297912597656
aF2.640464172363281
aF2.6284234619140623
aF2.610001220703125
aF2.6037432861328127
aF2.6186029052734376
aF2.6089764404296876
aF2.5901507568359374
aF2.5966342163085936
aF2.5811526489257814
aF2.5759002685546877
aF2.5648394775390626
aF2.5449891662597657
aF2.5510322570800783
aF2.553419189453125
aF2.533192138671875
aF2.533286437988281
aF2.5384562683105467
aF2.5210105895996096
aF2.532613525390625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0008059379320941014
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 12s'
p10
sS'final_test_loss'
p11
F2.532613525390625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'[\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.