(dp0
S'train_loss'
p1
(lp2
F4.657279968261719
aF4.619125671386719
aF4.58042724609375
aF4.538182373046875
aF4.503060913085937
aF4.467114562988281
aF4.431008911132812
aF4.389228515625
aF4.351083984375
aF4.306123352050781
aF4.277279968261719
aF4.23974609375
aF4.19956298828125
aF4.145251770019531
aF4.107952270507813
aF4.066146240234375
aF4.008800964355469
aF3.9790078735351564
aF3.9235870361328127
aF3.8622869873046874
aF3.8245147705078124
aF3.761110534667969
aF3.697215576171875
aF3.65588623046875
aF3.606412048339844
aF3.5597183227539064
aF3.493971252441406
aF3.4498248291015625
aF3.384879455566406
aF3.35718994140625
aF3.3117681884765626
aF3.2681448364257815
aF3.243826904296875
aF3.2218096923828123
aF3.183187255859375
aF3.1654052734375
aF3.1404156494140625
aF3.1451290893554686
aF3.1353656005859376
aF3.0966452026367186
aF3.09459228515625
aF3.0736566162109376
aF3.0535089111328126
aF3.0179519653320312
aF3.0225119018554687
aF2.9926138305664063
aF2.9790609741210936
aF2.997850036621094
aF2.974989318847656
aF2.9281344604492188
aF2.941441955566406
aF2.927769775390625
aF2.900626220703125
aF2.8908065795898437
aF2.874364013671875
aF2.891370849609375
aF2.8864199829101564
aF2.870888671875
aF2.859516906738281
aF2.855857238769531
aF2.8478121948242188
aF2.829803161621094
aF2.827626037597656
aF2.8119601440429687
aF2.8026663208007814
aF2.7787399291992188
aF2.795362548828125
aF2.790765686035156
aF2.7771337890625
aF2.7691033935546874
aF2.7702560424804688
aF2.7758224487304686
aF2.7554525756835937
aF2.73457763671875
aF2.7378054809570314
aF2.7291607666015625
aF2.73219970703125
aF2.7157723999023435
aF2.7219683837890627
aF2.7069387817382813
aF2.707680358886719
aF2.699443054199219
aF2.6935833740234374
aF2.6829962158203124
aF2.672713317871094
aF2.69567626953125
aF2.6572247314453126
aF2.664966735839844
aF2.65544677734375
aF2.656824951171875
aF2.6404849243164064
aF2.6439450073242186
aF2.636802978515625
aF2.6237451171875
aF2.611689147949219
aF2.6318603515625
aF2.60697509765625
aF2.6293731689453126
aF2.6453521728515623
aF2.6074374389648436
asS'test_loss'
p3
(lp4
F4.618390502929688
aF4.578949890136719
aF4.542719116210938
aF4.502901611328125
aF4.463153381347656
aF4.425816345214844
aF4.388915405273438
aF4.349485473632813
aF4.309614868164062
aF4.269977722167969
aF4.225154724121094
aF4.182022399902344
aF4.151303405761719
aF4.102320861816406
aF4.0588446044921875
aF4.015870056152344
aF3.9701788330078127
aF3.9148049926757813
aF3.86876220703125
aF3.808771667480469
aF3.7685720825195315
aF3.7027847290039064
aF3.646852722167969
aF3.5816409301757814
aF3.5385086059570314
aF3.475984802246094
aF3.4306793212890625
aF3.4168499755859374
aF3.342607421875
aF3.2989874267578125
aF3.2672918701171874
aF3.243910827636719
aF3.1960879516601564
aF3.1744805908203126
aF3.1675955200195314
aF3.1573751831054686
aF3.117901611328125
aF3.12310546875
aF3.104378967285156
aF3.058146667480469
aF3.0621826171875
aF3.077529296875
aF3.0155459594726564
aF3.010004577636719
aF2.9994442749023436
aF2.9882550048828125
aF2.9492755126953125
aF2.9611325073242187
aF2.9491510009765625
aF2.9510992431640624
aF2.9268487548828124
aF2.9092059326171875
aF2.8915899658203124
aF2.868319396972656
aF2.868065185546875
aF2.8637405395507813
aF2.8494805908203125
aF2.8524578857421874
aF2.8469577026367188
aF2.829876708984375
aF2.8294558715820313
aF2.799543762207031
aF2.819200134277344
aF2.7925238037109374
aF2.812662353515625
aF2.8073846435546876
aF2.798933410644531
aF2.7677099609375
aF2.7828070068359376
aF2.746404724121094
aF2.7500958251953125
aF2.7248431396484376
aF2.7404669189453124
aF2.740381774902344
aF2.7143731689453126
aF2.7164541625976564
aF2.716653137207031
aF2.707174987792969
aF2.684754638671875
aF2.683988342285156
aF2.6812689208984377
aF2.69409912109375
aF2.6750128173828127
aF2.688473815917969
aF2.6812127685546874
aF2.6699264526367186
aF2.6801019287109376
aF2.651774597167969
aF2.6664962768554688
aF2.641026916503906
aF2.6417742919921876
aF2.641152648925781
aF2.629796447753906
aF2.61999267578125
aF2.6316925048828126
aF2.608570556640625
aF2.6028988647460936
aF2.601282958984375
aF2.5995843505859373
aF2.601046142578125
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00019731072309331983
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 17s'
p10
sS'final_test_loss'
p11
F2.601046142578125
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xf8\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.