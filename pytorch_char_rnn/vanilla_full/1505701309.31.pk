(dp0
S'train_loss'
p1
(lp2
F4.612721252441406
aF4.4788034057617185
aF4.342343139648437
aF4.175622253417969
aF3.9888375854492186
aF3.7890200805664063
aF3.601151428222656
aF3.4563388061523437
aF3.407029724121094
aF3.33818603515625
aF3.2744970703125
aF3.2209652709960936
aF3.185862731933594
aF3.1208157348632812
aF3.060361022949219
aF3.054002380371094
aF2.994906311035156
aF2.979764099121094
aF2.93479736328125
aF2.8897052001953125
aF2.8875717163085937
aF2.8478665161132812
aF2.83202880859375
aF2.8176388549804687
aF2.809403381347656
aF2.76318603515625
aF2.7497598266601564
aF2.7420639038085937
aF2.7051181030273437
aF2.6950738525390623
aF2.676004638671875
aF2.6481378173828123
aF2.642628173828125
aF2.633611755371094
aF2.5857669067382814
aF2.580191345214844
aF2.570498352050781
aF2.5538926696777344
aF2.545068664550781
aF2.534828796386719
aF2.5260418701171874
aF2.508533172607422
aF2.49535400390625
aF2.4885606384277343
aF2.454026947021484
aF2.4404338073730467
aF2.465628814697266
aF2.4284437561035155
aF2.449126739501953
aF2.4146183776855468
aF2.4214219665527343
aF2.405789031982422
aF2.405451354980469
aF2.406391906738281
aF2.384005126953125
aF2.3856329345703124
aF2.3675331115722655
aF2.382231903076172
aF2.3672691345214845
aF2.3661288452148437
aF2.331858825683594
aF2.339516143798828
aF2.3466258239746094
aF2.3423875427246093
aF2.340555877685547
aF2.3285946655273437
aF2.3094276428222655
aF2.3043150329589843
aF2.3142579650878905
aF2.302556610107422
aF2.2824342346191404
aF2.2986427307128907
aF2.2793321228027343
aF2.2671604919433594
aF2.274267120361328
aF2.271562957763672
aF2.2547598266601563
aF2.243494110107422
aF2.2571533203125
aF2.230643005371094
aF2.2296771240234374
aF2.2371051025390627
aF2.2434458923339844
aF2.2353179931640623
aF2.2134385681152344
aF2.2236624145507813
aF2.214263610839844
aF2.205087432861328
aF2.217331848144531
aF2.192208251953125
aF2.1950497436523437
aF2.2003114318847654
aF2.19396240234375
aF2.1913404846191407
aF2.186847686767578
aF2.1778961181640626
aF2.193829345703125
aF2.171327667236328
aF2.1764112854003907
aF2.167116241455078
asS'test_loss'
p3
(lp4
F4.477552795410157
aF4.334087524414063
aF4.184737243652344
aF3.9975860595703123
aF3.788091125488281
aF3.592579650878906
aF3.456754150390625
aF3.3820343017578125
aF3.3300714111328125
aF3.2649267578125
aF3.2478985595703125
aF3.1640057373046875
aF3.145185546875
aF3.0843597412109376
aF3.0260418701171874
aF2.9864675903320315
aF2.967015686035156
aF2.9307525634765623
aF2.8936648559570313
aF2.910825500488281
aF2.829447021484375
aF2.856236572265625
aF2.7968701171875
aF2.8070721435546875
aF2.7398828125
aF2.7269461059570315
aF2.70107177734375
aF2.67241943359375
aF2.671260986328125
aF2.6691738891601564
aF2.656251220703125
aF2.6427822875976563
aF2.5946697998046875
aF2.6045233154296876
aF2.57712890625
aF2.57056884765625
aF2.559739532470703
aF2.521961517333984
aF2.5325885009765625
aF2.493241729736328
aF2.491617431640625
aF2.5012051391601564
aF2.460616149902344
aF2.476285858154297
aF2.4725462341308595
aF2.4524856567382813
aF2.4553376770019533
aF2.4427635192871096
aF2.4273968505859376
aF2.4078302001953125
aF2.401928253173828
aF2.3889723205566407
aF2.409135437011719
aF2.37363037109375
aF2.372151184082031
aF2.3652085876464843
aF2.3786424255371093
aF2.363592071533203
aF2.3514732360839843
aF2.3378756713867186
aF2.3249713134765626
aF2.3105232238769533
aF2.3221881103515627
aF2.339658508300781
aF2.303062286376953
aF2.3220716857910157
aF2.3092251586914063
aF2.311201629638672
aF2.29473388671875
aF2.289211883544922
aF2.2802694702148436
aF2.2657958984375
aF2.2527976989746095
aF2.2820068359375
aF2.247530670166016
aF2.2560523986816405
aF2.2432545471191405
aF2.263289031982422
aF2.2458872985839844
aF2.230937957763672
aF2.2247349548339845
aF2.2326676940917967
aF2.234175109863281
aF2.2180621337890627
aF2.228009796142578
aF2.216325531005859
aF2.2142999267578123
aF2.209650115966797
aF2.2101815795898436
aF2.2041395568847655
aF2.2001826477050783
aF2.182236328125
aF2.16546875
aF2.1699632263183593
aF2.169663391113281
aF2.177071380615234
aF2.1660870361328124
aF2.1612916564941407
aF2.184457092285156
aF2.1564788818359375
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.000604871630982927
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 23s'
p10
sS'final_test_loss'
p11
F2.1564788818359375
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xfd\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.