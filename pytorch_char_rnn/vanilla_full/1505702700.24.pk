(dp0
S'train_loss'
p1
(lp2
F4.59444091796875
aF4.340567626953125
aF4.082568664550781
aF3.769758605957031
aF3.4602972412109376
aF3.238364562988281
aF3.1383425903320314
aF3.077213134765625
aF3.0340542602539062
aF2.9715753173828126
aF2.9260870361328126
aF2.843509521484375
aF2.8162310791015623
aF2.76845703125
aF2.7449459838867187
aF2.701638488769531
aF2.7053866577148438
aF2.6844296264648437
aF2.6398297119140626
aF2.6084088134765624
aF2.6329248046875
aF2.586020202636719
aF2.5611578369140626
aF2.5286990356445314
aF2.5104800415039064
aF2.515940856933594
aF2.5026170349121095
aF2.461021270751953
aF2.4525526428222655
aF2.4533979797363283
aF2.435080108642578
aF2.4399043273925782
aF2.4149845886230468
aF2.410977478027344
aF2.400481719970703
aF2.3755035400390625
aF2.3565608215332032
aF2.365520477294922
aF2.35488525390625
aF2.3479673767089846
aF2.3650442504882814
aF2.3414581298828123
aF2.309680938720703
aF2.322174530029297
aF2.3013960266113282
aF2.304371185302734
aF2.303422698974609
aF2.294591979980469
aF2.2641346740722654
aF2.2740579223632813
aF2.2717996215820313
aF2.2481907653808593
aF2.2326878356933593
aF2.2494334411621093
aF2.226461181640625
aF2.243588562011719
aF2.2352838134765625
aF2.225511169433594
aF2.231062164306641
aF2.2243035888671874
aF2.207505187988281
aF2.1942576599121093
aF2.1997970581054687
aF2.1808477783203126
aF2.185821228027344
aF2.19874755859375
aF2.193931427001953
aF2.1933303833007813
aF2.167813262939453
aF2.1596754455566405
aF2.164192810058594
aF2.171385498046875
aF2.137908630371094
aF2.1319764709472655
aF2.1368011474609374
aF2.1323068237304685
aF2.15474609375
aF2.1175442504882813
aF2.1266729736328127
aF2.1432756042480468
aF2.1348854064941407
aF2.1082867431640624
aF2.1114627075195314
aF2.109438018798828
aF2.107686004638672
aF2.1319174194335937
aF2.111444549560547
aF2.0837278747558594
aF2.101289978027344
aF2.1089674377441407
aF2.093516845703125
aF2.0926142883300782
aF2.0824734497070314
aF2.091451873779297
aF2.0677871704101562
aF2.0511279296875
aF2.060438385009766
aF2.0729296875
aF2.057920227050781
aF2.0703697204589844
asS'test_loss'
p3
(lp4
F4.342170104980469
aF4.0795285034179685
aF3.7626971435546874
aF3.4856997680664064
aF3.2535498046875
aF3.1420443725585936
aF3.1067318725585937
aF3.012821044921875
aF3.0022076416015624
aF2.897340087890625
aF2.8611724853515623
aF2.79864990234375
aF2.7749093627929686
aF2.77060302734375
aF2.710982666015625
aF2.68805908203125
aF2.6785211181640625
aF2.637198181152344
aF2.6172360229492186
aF2.6145294189453123
aF2.591506042480469
aF2.56310302734375
aF2.554307556152344
aF2.521620178222656
aF2.522910614013672
aF2.5049024963378907
aF2.490813903808594
aF2.468694152832031
aF2.4477882385253906
aF2.4400863647460938
aF2.42648193359375
aF2.395183868408203
aF2.3907899475097656
aF2.3949453735351565
aF2.3534010314941405
aF2.363138885498047
aF2.361581573486328
aF2.333216094970703
aF2.341999359130859
aF2.3207347106933596
aF2.310729522705078
aF2.306978302001953
aF2.3113702392578124
aF2.3010743713378905
aF2.287099151611328
aF2.2747312927246095
aF2.2715516662597657
aF2.277564697265625
aF2.2559423828125
aF2.2670799255371095
aF2.256142578125
aF2.251966552734375
aF2.23538818359375
aF2.2534306335449217
aF2.233991851806641
aF2.2202288818359377
aF2.2242518615722657
aF2.2193875122070312
aF2.209580535888672
aF2.217493896484375
aF2.2142254638671877
aF2.1878012084960936
aF2.203895721435547
aF2.1916526794433593
aF2.181212615966797
aF2.1848876953125
aF2.168076629638672
aF2.165652008056641
aF2.1652352905273435
aF2.16345458984375
aF2.149580383300781
aF2.1384654235839844
aF2.148681335449219
aF2.1458370971679686
aF2.143619384765625
aF2.13076171875
aF2.1209552001953127
aF2.11928955078125
aF2.1076397705078125
aF2.125967559814453
aF2.125989532470703
aF2.1091157531738283
aF2.1139344787597656
aF2.0941506958007814
aF2.1035511779785154
aF2.0918145751953126
aF2.089785614013672
aF2.079933624267578
aF2.104715576171875
aF2.081481475830078
aF2.055588684082031
aF2.085141448974609
aF2.0653230285644533
aF2.0623692321777343
aF2.0711231994628907
aF2.0644544982910156
aF2.065718231201172
aF2.054258270263672
aF2.049901123046875
aF2.059405517578125
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0019752355218788653
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 5s'
p10
sS'final_test_loss'
p11
F2.059405517578125
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.