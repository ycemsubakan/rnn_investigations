(dp0
S'train_loss'
p1
(lp2
F4.622847595214844
aF4.606583557128906
aF4.584717712402344
aF4.570690307617188
aF4.547152404785156
aF4.530475158691406
aF4.516264343261719
aF4.496667785644531
aF4.478183898925781
aF4.456569213867187
aF4.4408807373046875
aF4.428361206054688
aF4.404230346679688
aF4.383378295898438
aF4.366062316894531
aF4.351533203125
aF4.328971252441407
aF4.306532592773437
aF4.295967712402343
aF4.269314575195312
aF4.251767272949219
aF4.225742797851563
aF4.199904479980469
aF4.178380126953125
aF4.160393981933594
aF4.140853576660156
aF4.113912048339844
aF4.07976806640625
aF4.067814331054688
aF4.027646179199219
aF3.9936456298828125
aF3.96070556640625
aF3.94710205078125
aF3.910599670410156
aF3.880559387207031
aF3.8505569458007813
aF3.82182861328125
aF3.7773175048828125
aF3.7481231689453125
aF3.714723815917969
aF3.6760714721679686
aF3.637784423828125
aF3.5916632080078124
aF3.564666748046875
aF3.543313293457031
aF3.498715515136719
aF3.494984130859375
aF3.46310546875
aF3.4323675537109377
aF3.4084442138671873
aF3.3931240844726562
aF3.341724853515625
aF3.3382308959960936
aF3.342762756347656
aF3.319669189453125
aF3.3111026000976564
aF3.285752258300781
aF3.27165283203125
aF3.2670306396484374
aF3.2529522705078127
aF3.2589657592773436
aF3.256636047363281
aF3.2076934814453124
aF3.20654296875
aF3.2051101684570313
aF3.193891906738281
aF3.1889187622070314
aF3.16758056640625
aF3.170160217285156
aF3.150834655761719
aF3.1228851318359374
aF3.124844970703125
aF3.1344873046875
aF3.137120361328125
aF3.1405548095703124
aF3.122456359863281
aF3.0956045532226564
aF3.0774935913085937
aF3.0785147094726564
aF3.0779620361328126
aF3.0987673950195314
aF3.0577056884765623
aF3.0658773803710937
aF3.043501281738281
aF3.043126220703125
aF3.0522116088867186
aF3.0280856323242187
aF3.0449703979492186
aF3.0197283935546877
aF3.0321160888671876
aF3.004644470214844
aF3.015731201171875
aF3.0143353271484377
aF2.9945831298828125
aF3.005057373046875
aF2.995186767578125
aF2.9811541748046877
aF2.982208557128906
aF2.9655401611328127
aF2.96267822265625
asS'test_loss'
p3
(lp4
F4.601178588867188
aF4.587586669921875
aF4.5659027099609375
aF4.549627990722656
aF4.536690063476563
aF4.515436401367188
aF4.4922311401367185
aF4.475942993164063
aF4.457737426757813
aF4.443568420410156
aF4.422185363769532
aF4.403251647949219
aF4.385277404785156
aF4.368424072265625
aF4.3457861328125
aF4.3270394897460935
aF4.3100927734375
aF4.284801940917969
aF4.272514343261719
aF4.2450048828125
aF4.222794494628906
aF4.201170043945313
aF4.178021240234375
aF4.161412353515625
aF4.134920959472656
aF4.1075045776367185
aF4.083916320800781
aF4.06001953125
aF4.027895812988281
aF4.009215393066406
aF3.979365234375
aF3.9466458129882813
aF3.918475341796875
aF3.87020751953125
aF3.8373178100585936
aF3.8129412841796877
aF3.779005126953125
aF3.7511328125
aF3.7036138916015626
aF3.6654013061523436
aF3.619793701171875
aF3.6117999267578127
aF3.5557376098632814
aF3.5368170166015624
aF3.501136474609375
aF3.4802706909179686
aF3.4394024658203124
aF3.439993591308594
aF3.3897802734375
aF3.3740829467773437
aF3.3582550048828126
aF3.344549560546875
aF3.3294488525390626
aF3.3035205078125
aF3.289004211425781
aF3.28601806640625
aF3.2710031127929686
aF3.2508474731445314
aF3.273478698730469
aF3.2332171630859374
aF3.2266891479492186
aF3.222989807128906
aF3.245543212890625
aF3.1851260375976564
aF3.1966961669921874
aF3.202225341796875
aF3.1748220825195315
aF3.1591656494140623
aF3.147194519042969
aF3.1267144775390623
aF3.1355636596679686
aF3.105315856933594
aF3.135819396972656
aF3.11103515625
aF3.1170111083984375
aF3.0953125
aF3.1051119995117187
aF3.0751419067382812
aF3.067402038574219
aF3.062501220703125
aF3.0411810302734374
aF3.068889465332031
aF3.0396142578125
aF3.053133850097656
aF3.043082580566406
aF3.049192199707031
aF3.005958251953125
aF3.040067443847656
aF3.0045950317382815
aF3.0112359619140623
aF3.001995849609375
aF3.002686767578125
aF3.005157165527344
aF2.9837408447265625
aF2.98106201171875
aF2.971182556152344
aF2.988768310546875
aF2.9670806884765626
aF2.9495114135742186
aF2.9602197265625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0001557663384201968
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 4s'
p10
sS'final_test_loss'
p11
F2.9602197265625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xaa\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.