(dp0
S'train_loss'
p1
(lp2
F4.643832397460938
aF4.594475402832031
aF4.538687744140625
aF4.492080383300781
aF4.441341552734375
aF4.386692504882813
aF4.3301324462890625
aF4.2731396484375
aF4.212550354003906
aF4.149385375976562
aF4.072119140625
aF3.9872503662109375
aF3.906993713378906
aF3.8240789794921874
aF3.7398126220703123
aF3.6725604248046877
aF3.585421447753906
aF3.51713623046875
aF3.47279296875
aF3.416793212890625
aF3.3570492553710936
aF3.329717102050781
aF3.30422119140625
aF3.2575494384765626
aF3.2682000732421876
aF3.2353997802734376
aF3.2067755126953124
aF3.203108215332031
aF3.166548767089844
aF3.13994140625
aF3.1801690673828125
aF3.172186279296875
aF3.1326107788085937
aF3.0953030395507812
aF3.099018859863281
aF3.07968505859375
aF3.060286560058594
aF3.0468963623046874
aF3.0010635375976564
aF3.029053039550781
aF2.9926712036132814
aF2.9972708129882815
aF2.9509011840820314
aF2.9708816528320314
aF2.9674310302734375
aF2.93619140625
aF2.913962097167969
aF2.9008953857421873
aF2.895005798339844
aF2.8929843139648437
aF2.8951312255859376
aF2.884904479980469
aF2.888668212890625
aF2.8610598754882814
aF2.841794128417969
aF2.8354180908203124
aF2.7942047119140625
aF2.799313659667969
aF2.8102099609375
aF2.78863525390625
aF2.786727294921875
aF2.77160888671875
aF2.75930908203125
aF2.753477783203125
aF2.7492794799804687
aF2.759626770019531
aF2.7588711547851563
aF2.7385394287109377
aF2.71560546875
aF2.7207412719726562
aF2.730345458984375
aF2.7031875610351563
aF2.6919845581054687
aF2.681634826660156
aF2.6712957763671876
aF2.677723388671875
aF2.6515704345703126
aF2.6687359619140625
aF2.666559143066406
aF2.6562261962890625
aF2.6314068603515626
aF2.636767578125
aF2.615341796875
aF2.61709228515625
aF2.6288156127929687
aF2.6358465576171874
aF2.59938720703125
aF2.614624328613281
aF2.5953765869140626
aF2.5951104736328126
aF2.6010586547851564
aF2.6007421875
aF2.5758511352539064
aF2.565478515625
aF2.5708181762695315
aF2.5641571044921876
aF2.5599484252929687
aF2.554971008300781
aF2.552131805419922
aF2.539711151123047
asS'test_loss'
p3
(lp4
F4.593489379882812
aF4.542514953613281
aF4.489191284179688
aF4.439248046875
aF4.386011047363281
aF4.326614685058594
aF4.2744073486328125
aF4.21384033203125
aF4.139444885253906
aF4.074290161132812
aF3.9955120849609376
aF3.9120553588867186
aF3.8319467163085936
aF3.742453308105469
aF3.663435363769531
aF3.5742852783203123
aF3.51058837890625
aF3.4623867797851564
aF3.396986083984375
aF3.3788104248046875
aF3.331802978515625
aF3.3012078857421874
aF3.2778659057617188
aF3.242043762207031
aF3.2231903076171875
aF3.2434988403320313
aF3.1623611450195312
aF3.136683349609375
aF3.1593792724609373
aF3.148416748046875
aF3.131492004394531
aF3.1250823974609374
aF3.0955514526367187
aF3.0801483154296876
aF3.073431396484375
aF3.0485467529296875
aF3.02651611328125
aF3.0278945922851563
aF3.006044616699219
aF2.99620849609375
aF2.983109436035156
aF2.955224609375
aF2.932476806640625
aF2.9309896850585937
aF2.941595458984375
aF2.92048095703125
aF2.9115289306640624
aF2.8903164672851562
aF2.8967877197265626
aF2.875913391113281
aF2.8757113647460937
aF2.8557339477539063
aF2.862373962402344
aF2.8079656982421874
aF2.818516845703125
aF2.8270333862304686
aF2.813113098144531
aF2.813226318359375
aF2.806285705566406
aF2.773994140625
aF2.7632818603515625
aF2.7686627197265623
aF2.7421697998046874
aF2.76257080078125
aF2.730760192871094
aF2.727816467285156
aF2.724801330566406
aF2.7240670776367186
aF2.716268310546875
aF2.70387939453125
aF2.70488525390625
aF2.6927371215820313
aF2.6769024658203127
aF2.681061096191406
aF2.68353271484375
aF2.6659228515625
aF2.665853271484375
aF2.66635498046875
aF2.6237313842773435
aF2.6407098388671875
aF2.6213088989257813
aF2.6031460571289062
aF2.6201821899414064
aF2.6194952392578124
aF2.5982952880859376
aF2.6087991333007814
aF2.611983642578125
aF2.592987365722656
aF2.613070373535156
aF2.588387145996094
aF2.587950439453125
aF2.572337646484375
aF2.5939181518554686
aF2.565279846191406
aF2.547852783203125
aF2.5634100341796877
aF2.555758361816406
aF2.5635662841796876
aF2.5414215087890626
aF2.542514190673828
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.000953142156406671
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 49s'
p10
sS'final_test_loss'
p11
F2.542514190673828
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'[\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.