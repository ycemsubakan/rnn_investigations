(dp0
S'train_loss'
p1
(lp2
F4.646012878417968
aF4.463064270019531
aF4.281066284179688
aF4.10322021484375
aF3.92048095703125
aF3.709311218261719
aF3.494609680175781
aF3.3100299072265624
aF3.2146697998046876
aF3.1360662841796874
aF3.101435546875
aF3.0541207885742185
aF2.991806640625
aF2.9348785400390627
aF2.9131927490234375
aF2.84597412109375
aF2.811591796875
aF2.7655181884765625
aF2.7712127685546877
aF2.740761413574219
aF2.7503240966796874
aF2.7069134521484375
aF2.6869265747070314
aF2.6690707397460938
aF2.657865295410156
aF2.6143402099609374
aF2.6191827392578126
aF2.59938232421875
aF2.5733108520507812
aF2.5786700439453125
aF2.5662997436523436
aF2.5507269287109375
aF2.5418215942382814
aF2.51626953125
aF2.5094056701660157
aF2.494107513427734
aF2.4945939636230468
aF2.4830047607421877
aF2.4662326049804686
aF2.4649447631835937
aF2.4322706604003907
aF2.4458464050292967
aF2.429521179199219
aF2.4184030151367186
aF2.4038764953613283
aF2.4149156188964844
aF2.3762278747558594
aF2.3885403442382813
aF2.3881610107421873
aF2.374495849609375
aF2.366845703125
aF2.354964141845703
aF2.326307678222656
aF2.3227867126464843
aF2.347859191894531
aF2.336638946533203
aF2.321238708496094
aF2.3074357604980467
aF2.299215545654297
aF2.2883975219726564
aF2.292095947265625
aF2.2851138305664063
aF2.290366973876953
aF2.2747850036621093
aF2.2711692810058595
aF2.277193908691406
aF2.276242218017578
aF2.272099609375
aF2.2607945251464843
aF2.2450657653808594
aF2.2452313232421877
aF2.2385427856445315
aF2.246889343261719
aF2.2384136962890624
aF2.241845703125
aF2.242073974609375
aF2.2210516357421874
aF2.2155661010742187
aF2.20454345703125
aF2.213673095703125
aF2.216593933105469
aF2.2104913330078126
aF2.1807904052734375
aF2.18670654296875
aF2.200721893310547
aF2.20280029296875
aF2.1840859985351564
aF2.175486297607422
aF2.1855729675292968
aF2.166819763183594
aF2.1813174438476564
aF2.1576390075683594
aF2.1552743530273437
aF2.156005401611328
aF2.1742098999023436
aF2.1476681518554686
aF2.1643495178222656
aF2.1407762145996094
aF2.1567976379394533
aF2.135814514160156
asS'test_loss'
p3
(lp4
F4.468793029785156
aF4.289171142578125
aF4.104387817382812
aF3.901343994140625
aF3.686695556640625
aF3.485419921875
aF3.3040255737304687
aF3.1736907958984375
aF3.1300955200195313
aF3.0996636962890625
aF3.0186392211914064
aF2.998992614746094
aF2.9663522338867185
aF2.8882107543945312
aF2.8715399169921874
aF2.8331173706054686
aF2.7824041748046877
aF2.7723770141601562
aF2.7391021728515623
aF2.7234536743164064
aF2.7014495849609377
aF2.7056423950195314
aF2.6725979614257813
aF2.648463134765625
aF2.61934814453125
aF2.604197998046875
aF2.5991500854492187
aF2.582779541015625
aF2.5809481811523436
aF2.559106750488281
aF2.5406980895996094
aF2.5390191650390626
aF2.493446044921875
aF2.5139552307128907
aF2.4791973876953124
aF2.48388427734375
aF2.4616676330566407
aF2.45928955078125
aF2.4408645629882812
aF2.449545135498047
aF2.4289892578125
aF2.413829498291016
aF2.419949645996094
aF2.389700164794922
aF2.375703887939453
aF2.398736877441406
aF2.364024353027344
aF2.358764953613281
aF2.3648892211914063
aF2.347421417236328
aF2.338262634277344
aF2.330375518798828
aF2.3372566223144533
aF2.3242147827148436
aF2.333707275390625
aF2.320496063232422
aF2.2969781494140626
aF2.3039430236816405
aF2.2894386291503905
aF2.2975355529785157
aF2.293496398925781
aF2.285352325439453
aF2.2564715576171874
aF2.2669517517089846
aF2.2757330322265625
aF2.2558595275878908
aF2.271640625
aF2.2522653198242186
aF2.2465472412109375
aF2.2517735290527345
aF2.2418788146972655
aF2.236478271484375
aF2.229189147949219
aF2.2332612609863283
aF2.234316101074219
aF2.2198231506347654
aF2.1997946166992186
aF2.2115948486328123
aF2.2221514892578127
aF2.1981683349609376
aF2.1962568664550783
aF2.1921185302734374
aF2.1737689208984374
aF2.1796525573730468
aF2.18593505859375
aF2.167062225341797
aF2.182322540283203
aF2.1840888977050783
aF2.1600440979003905
aF2.168087921142578
aF2.174940490722656
aF2.1627369689941407
aF2.1697042846679686
aF2.1722817993164063
aF2.152589874267578
aF2.1521636962890627
aF2.13316650390625
aF2.1422184753417968
aF2.1323013305664062
aF2.154705352783203
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0010458919828069384
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 14s'
p10
sS'final_test_loss'
p11
F2.154705352783203
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xdc\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.