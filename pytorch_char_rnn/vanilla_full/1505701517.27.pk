(dp0
S'train_loss'
p1
(lp2
F4.656791076660157
aF4.32966552734375
aF3.877149658203125
aF3.5138470458984377
aF3.33838623046875
aF3.2653854370117186
aF3.1343539428710936
aF3.05164306640625
aF2.957325134277344
aF2.8919512939453127
aF2.839052429199219
aF2.8057244873046874
aF2.758349609375
aF2.7379754638671874
aF2.6516400146484376
aF2.6472872924804687
aF2.605560607910156
aF2.583718566894531
aF2.5450872802734374
aF2.527168884277344
aF2.496999053955078
aF2.4885238647460937
aF2.4398663330078123
aF2.42352294921875
aF2.4106936645507813
aF2.396389617919922
aF2.3927983093261718
aF2.3697335815429685
aF2.3602664184570314
aF2.336595153808594
aF2.330194091796875
aF2.3225299072265626
aF2.3107179260253905
aF2.3185305786132813
aF2.291563720703125
aF2.2656565856933595
aF2.252344207763672
aF2.259824066162109
aF2.249145202636719
aF2.222423858642578
aF2.2179518127441407
aF2.2301329040527342
aF2.19657470703125
aF2.191201629638672
aF2.2110249328613283
aF2.158924407958984
aF2.1701727294921875
aF2.168241271972656
aF2.1434786987304686
aF2.155425262451172
aF2.1386595153808594
aF2.1201861572265623
aF2.12655029296875
aF2.1235092163085936
aF2.1003472900390623
aF2.1103591918945312
aF2.0971334838867186
aF2.1237057495117186
aF2.1036524963378906
aF2.0876148986816405
aF2.1000213623046875
aF2.0726611328125
aF2.06430908203125
aF2.0603567504882814
aF2.0408395385742186
aF2.0535986328125
aF2.0211126708984377
aF2.05061767578125
aF2.0235479736328124
aF2.0418475341796873
aF2.0128619384765627
aF2.018204650878906
aF2.0072091674804686
aF2.0154420471191408
aF1.9883270263671875
aF1.99607421875
aF1.996343994140625
aF1.9783236694335937
aF1.972957763671875
aF1.9845704650878906
aF1.9701914978027344
aF1.9602638244628907
aF1.9531341552734376
aF1.9665704345703126
aF1.9617916870117187
aF1.9339382934570313
aF1.9621766662597657
aF1.9484275817871093
aF1.9585769653320313
aF1.9451983642578126
aF1.9361851501464844
aF1.9357217407226563
aF1.9313238525390626
aF1.9054737854003907
aF1.93591552734375
aF1.9219325256347657
aF1.91473388671875
aF1.9497537231445312
aF1.9184422302246094
aF1.9148440551757813
asS'test_loss'
p3
(lp4
F4.322590942382813
aF3.891795349121094
aF3.491899108886719
aF3.342384033203125
aF3.2356341552734373
aF3.1193698120117186
aF3.043333740234375
aF2.9741644287109374
aF2.877850341796875
aF2.8393377685546874
aF2.8020761108398435
aF2.7396646118164063
aF2.7280001831054688
aF2.6830868530273437
aF2.6400497436523436
aF2.601307373046875
aF2.5676388549804687
aF2.5330404663085937
aF2.5191915893554686
aF2.4935577392578123
aF2.494185791015625
aF2.4422480773925783
aF2.4376153564453125
aF2.401178283691406
aF2.3861428833007814
aF2.3777127075195312
aF2.3669256591796874
aF2.3653910827636717
aF2.318236999511719
aF2.3283734130859375
aF2.3084104919433592
aF2.3099607849121093
aF2.280831604003906
aF2.277246551513672
aF2.2498951721191407
aF2.232366180419922
aF2.2606045532226564
aF2.2278334045410157
aF2.2210629272460936
aF2.217008056640625
aF2.2073912048339843
aF2.1996000671386717
aF2.1818841552734374
aF2.1774449157714844
aF2.1782437133789063
aF2.1671435546875
aF2.1700588989257814
aF2.142544403076172
aF2.1623965454101564
aF2.1345748901367188
aF2.136801910400391
aF2.128697509765625
aF2.1228582763671877
aF2.0971018981933596
aF2.119561767578125
aF2.117287445068359
aF2.1085556030273436
aF2.0970254516601563
aF2.0907907104492187
aF2.061823272705078
aF2.0680915832519533
aF2.0633729553222655
aF2.061812438964844
aF2.058321075439453
aF2.0395579528808594
aF2.032130126953125
aF2.027593536376953
aF2.033529815673828
aF2.0307684326171875
aF2.027834930419922
aF2.020882568359375
aF2.0071783447265625
aF2.005135955810547
aF2.008538818359375
aF2.0181517028808593
aF2.003623199462891
aF1.9936732482910156
aF1.9710972595214844
aF1.9939366149902344
aF1.9654911804199218
aF1.9906565856933593
aF1.9763813781738282
aF1.9762092590332032
aF1.9744076538085937
aF1.9561671447753906
aF1.9564134216308593
aF1.9403521728515625
aF1.9484400939941406
aF1.9633958435058594
aF1.9420318603515625
aF1.9325076293945314
aF1.918889923095703
aF1.9186479187011718
aF1.9072071838378906
aF1.9270303344726563
aF1.9260186767578125
aF1.91567138671875
aF1.9088824462890626
aF1.92670654296875
aF1.8968209838867187
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.002335897560386273
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 45s'
p10
sS'final_test_loss'
p11
F1.8968209838867187
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xbe\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.