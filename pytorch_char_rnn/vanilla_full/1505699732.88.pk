(dp0
S'train_loss'
p1
(lp2
F4.687244567871094
aF4.68239013671875
aF4.676834716796875
aF4.670949096679688
aF4.668739318847656
aF4.660080261230469
aF4.657596435546875
aF4.650980224609375
aF4.645542907714844
aF4.641987915039063
aF4.638828430175781
aF4.633478393554688
aF4.626157531738281
aF4.620059509277343
aF4.614739685058594
aF4.612649841308594
aF4.604297485351562
aF4.599990234375
aF4.595110473632812
aF4.590053100585937
aF4.585418090820313
aF4.576120910644531
aF4.570685119628906
aF4.564510803222657
aF4.5608651733398435
aF4.554181213378906
aF4.546932983398437
aF4.541576538085938
aF4.532121276855468
aF4.526163330078125
aF4.520081176757812
aF4.511517028808594
aF4.501969604492188
aF4.494985961914063
aF4.4910281372070315
aF4.481022644042969
aF4.472351989746094
aF4.463295593261718
aF4.46078369140625
aF4.444738464355469
aF4.435918273925782
aF4.429534912109375
aF4.413706665039062
aF4.408261108398437
aF4.390382995605469
aF4.384972229003906
aF4.3711920166015625
aF4.362583312988281
aF4.350551147460937
aF4.33828857421875
aF4.320329284667968
aF4.3126806640625
aF4.297177429199219
aF4.287032775878906
aF4.274760131835937
aF4.261134948730469
aF4.23884033203125
aF4.228128662109375
aF4.212450561523437
aF4.193061218261719
aF4.184351806640625
aF4.176705322265625
aF4.160540161132812
aF4.140231018066406
aF4.1247689819335935
aF4.108427124023438
aF4.103920288085938
aF4.076945495605469
aF4.067760314941406
aF4.04049072265625
aF4.034269714355469
aF4.02365966796875
aF4.010403747558594
aF3.9850698852539064
aF3.978518371582031
aF3.9710491943359374
aF3.9396533203125
aF3.941011657714844
aF3.9153521728515623
aF3.9062298583984374
aF3.8902032470703123
aF3.8884317016601564
aF3.875951843261719
aF3.872949523925781
aF3.861799621582031
aF3.8408905029296876
aF3.8155340576171874
aF3.797471008300781
aF3.801633605957031
aF3.7815338134765626
aF3.7744720458984373
aF3.762864685058594
aF3.7479931640625
aF3.7458203125
aF3.739965515136719
aF3.7025799560546875
aF3.7306478881835936
aF3.7139337158203123
aF3.687569885253906
aF3.696502990722656
asS'test_loss'
p3
(lp4
F4.683780822753906
aF4.677866516113281
aF4.672602844238281
aF4.668230590820312
aF4.6625909423828125
aF4.659793090820313
aF4.653453369140625
aF4.649037475585938
aF4.643565063476562
aF4.635840759277344
aF4.635239868164063
aF4.62561767578125
aF4.623258666992188
aF4.613903503417969
aF4.612188720703125
aF4.605079040527344
aF4.60260498046875
aF4.5935205078125
aF4.589573669433594
aF4.583377990722656
aF4.5768505859375
aF4.571460876464844
aF4.567438354492188
aF4.559639892578125
aF4.55469482421875
aF4.5445425415039065
aF4.541271362304688
aF4.533870239257812
aF4.527334289550781
aF4.520263061523438
aF4.511056823730469
aF4.50659912109375
aF4.496357727050781
aF4.490426025390625
aF4.480779418945312
aF4.4743804931640625
aF4.463987426757813
aF4.456373901367187
aF4.442401428222656
aF4.434144592285156
aF4.4260952758789065
aF4.414552917480469
aF4.401441040039063
aF4.3939794921875
aF4.382959594726563
aF4.374595031738282
aF4.363121337890625
aF4.345429077148437
aF4.337015380859375
aF4.322548522949218
aF4.310318908691406
aF4.300117797851563
aF4.278731079101562
aF4.270560913085937
aF4.259232482910156
aF4.239431457519531
aF4.224614562988282
aF4.21868896484375
aF4.193755187988281
aF4.189602661132812
aF4.171479187011719
aF4.1559274291992185
aF4.134550476074219
aF4.119320373535157
aF4.110535888671875
aF4.094325256347656
aF4.078621826171875
aF4.066145629882812
aF4.0511080932617185
aF4.044207458496094
aF4.021862487792969
aF4.000083312988282
aF3.997658386230469
aF3.9817694091796874
aF3.9624249267578127
aF3.948114013671875
aF3.9277099609375
aF3.9283151245117187
aF3.9063751220703127
aF3.884044494628906
aF3.8836871337890626
aF3.879411315917969
aF3.8572821044921874
aF3.849421081542969
aF3.838148193359375
aF3.8241400146484374
aF3.8069989013671877
aF3.792599792480469
aF3.797543640136719
aF3.783935852050781
aF3.763660888671875
aF3.7694778442382812
aF3.7501324462890624
aF3.7430349731445314
aF3.709406433105469
aF3.720104064941406
aF3.7077651977539063
aF3.6840264892578123
aF3.686118469238281
aF3.691168212890625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0001365887573672337
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 3s'
p10
sS'final_test_loss'
p11
F3.691168212890625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'=\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.