(dp0
S'train_loss'
p1
(lp2
F4.6130029296875
aF4.431175537109375
aF4.212352905273438
aF3.921255798339844
aF3.6934417724609374
aF3.5238662719726563
aF3.4186419677734374
aF3.375426025390625
aF3.3020175170898436
aF3.2565347290039064
aF3.229550476074219
aF3.1441180419921877
aF3.1297921752929687
aF3.0244259643554687
aF3.0416738891601565
aF2.973701171875
aF2.9667459106445313
aF2.938401794433594
aF2.9207730102539062
aF2.8873910522460937
aF2.8379086303710936
aF2.808394775390625
aF2.7690921020507813
aF2.777207946777344
aF2.73483154296875
aF2.7215972900390626
aF2.7218338012695313
aF2.69044189453125
aF2.6561419677734377
aF2.6497708129882813
aF2.594852294921875
aF2.591697082519531
aF2.5938861083984377
aF2.5732708740234376
aF2.555043029785156
aF2.550732421875
aF2.523618469238281
aF2.496598358154297
aF2.4870561218261718
aF2.4746324157714845
aF2.444846954345703
aF2.453235778808594
aF2.4407586669921875
aF2.418686981201172
aF2.4266232299804686
aF2.410679931640625
aF2.4071185302734377
aF2.372774658203125
aF2.36251953125
aF2.3875198364257812
aF2.3711286926269532
aF2.3509519958496092
aF2.3646534729003905
aF2.3388487243652345
aF2.3478153991699218
aF2.3254905700683595
aF2.299943084716797
aF2.3079222106933592
aF2.311049041748047
aF2.281749725341797
aF2.3011351013183594
aF2.2914158630371095
aF2.2637142944335937
aF2.255279541015625
aF2.261580505371094
aF2.2658074951171874
aF2.247249755859375
aF2.2436038208007814
aF2.230596160888672
aF2.2091300964355467
aF2.22459228515625
aF2.2194300842285157
aF2.215086669921875
aF2.2239830017089846
aF2.2229747009277343
aF2.195118103027344
aF2.1755970764160155
aF2.177107849121094
aF2.209500579833984
aF2.177584991455078
aF2.178022003173828
aF2.178355407714844
aF2.1686314392089843
aF2.168270721435547
aF2.1796592712402343
aF2.1651255798339846
aF2.15215576171875
aF2.15140380859375
aF2.143474273681641
aF2.145127258300781
aF2.1345664978027346
aF2.138997802734375
aF2.1147091674804686
aF2.115791778564453
aF2.1277032470703126
aF2.117996826171875
aF2.1234786987304686
aF2.1003323364257813
aF2.0928526306152344
aF2.092268371582031
asS'test_loss'
p3
(lp4
F4.429212036132813
aF4.215020751953125
aF3.9382958984375
aF3.680231628417969
aF3.5253030395507814
aF3.429540710449219
aF3.3542205810546877
aF3.3004833984375
aF3.264362487792969
aF3.185397033691406
aF3.160472412109375
aF3.0701675415039062
aF3.041845703125
aF3.037206115722656
aF2.978554382324219
aF2.938620910644531
aF2.9305300903320313
aF2.9017465209960935
aF2.861435546875
aF2.843103942871094
aF2.8137020874023437
aF2.7994735717773436
aF2.757604675292969
aF2.725719299316406
aF2.706919860839844
aF2.699193115234375
aF2.6696951293945315
aF2.6797726440429686
aF2.6370449829101563
aF2.608064880371094
aF2.5852761840820313
aF2.565765380859375
aF2.5577127075195314
aF2.53035400390625
aF2.5254525756835937
aF2.511484375
aF2.4795166015625
aF2.4926712036132814
aF2.4654225158691405
aF2.4660052490234374
aF2.462799377441406
aF2.421225280761719
aF2.4108650207519533
aF2.4108207702636717
aF2.3781723022460937
aF2.394278564453125
aF2.3840061950683595
aF2.3633551025390624
aF2.379880676269531
aF2.360981903076172
aF2.344834289550781
aF2.338128967285156
aF2.3044508361816405
aF2.324615173339844
aF2.307312774658203
aF2.3202821350097658
aF2.3015338134765626
aF2.29665283203125
aF2.2858587646484376
aF2.2920271301269532
aF2.2770646667480468
aF2.2677984619140625
aF2.269800109863281
aF2.2464659118652346
aF2.246396179199219
aF2.250092010498047
aF2.2540316772460938
aF2.2353724670410156
aF2.2297427368164064
aF2.228330535888672
aF2.218769989013672
aF2.204814910888672
aF2.227991943359375
aF2.2080256652832033
aF2.2014039611816405
aF2.1988761901855467
aF2.2139738464355467
aF2.180043182373047
aF2.19099365234375
aF2.182711639404297
aF2.1968634033203127
aF2.159820556640625
aF2.1570500183105468
aF2.150138244628906
aF2.151158447265625
aF2.1575807189941405
aF2.1504170227050783
aF2.1344097900390624
aF2.1448358154296874
aF2.1224789428710937
aF2.128156890869141
aF2.1162545776367185
aF2.116314697265625
aF2.1152130126953126
aF2.088466796875
aF2.0820323181152345
aF2.100172271728516
aF2.095663604736328
aF2.0838656616210938
aF2.0760150146484375
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0016748193508657368
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 31s'
p10
sS'final_test_loss'
p11
F2.0760150146484375
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x8e\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.