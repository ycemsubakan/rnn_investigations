(dp0
S'train_loss'
p1
(lp2
F4.650964050292969
aF4.395526123046875
aF4.14054443359375
aF3.856163330078125
aF3.5611508178710936
aF3.302166748046875
aF3.1628237915039064
aF3.1029315185546875
aF3.04329833984375
aF2.9870660400390623
aF2.9201126098632812
aF2.866258239746094
aF2.778711853027344
aF2.793107604980469
aF2.772169189453125
aF2.7403912353515625
aF2.698846740722656
aF2.650235595703125
aF2.6324993896484377
aF2.6463442993164064
aF2.6076165771484376
aF2.583490905761719
aF2.596040954589844
aF2.537415771484375
aF2.542097930908203
aF2.5454400634765624
aF2.5092207336425782
aF2.4923451232910154
aF2.4945616149902343
aF2.4738397216796875
aF2.427773284912109
aF2.452697296142578
aF2.4249720764160156
aF2.440698699951172
aF2.3995458984375
aF2.3943504333496093
aF2.3889364624023437
aF2.3653970336914063
aF2.389354248046875
aF2.3721237182617188
aF2.359577178955078
aF2.3337908935546876
aF2.3388818359375
aF2.336393127441406
aF2.3137998962402344
aF2.313741912841797
aF2.306232452392578
aF2.284673767089844
aF2.2921539306640626
aF2.278827667236328
aF2.260476379394531
aF2.287233581542969
aF2.2727313232421875
aF2.2722259521484376
aF2.262747497558594
aF2.255753936767578
aF2.258728485107422
aF2.2375001525878906
aF2.2325564575195314
aF2.236112365722656
aF2.21909912109375
aF2.2257035827636718
aF2.2141156005859375
aF2.211947479248047
aF2.201321716308594
aF2.1789790344238282
aF2.200547637939453
aF2.208148956298828
aF2.192366638183594
aF2.1860305786132814
aF2.17342529296875
aF2.1872816467285157
aF2.1811842346191406
aF2.1549008178710936
aF2.1601234436035157
aF2.153314514160156
aF2.157364959716797
aF2.1582872009277345
aF2.1431204223632814
aF2.1475875854492186
aF2.1494091796875
aF2.129343719482422
aF2.118993225097656
aF2.134659729003906
aF2.1097567749023436
aF2.1061129760742188
aF2.1148065185546874
aF2.1163424682617187
aF2.1275923156738283
aF2.09737548828125
aF2.108682403564453
aF2.1016972351074217
aF2.099223327636719
aF2.0988404846191404
aF2.1029983520507813
aF2.08075439453125
aF2.08255859375
aF2.1003427124023437
aF2.0597935485839844
aF2.0827845764160156
asS'test_loss'
p3
(lp4
F4.401430358886719
aF4.139512939453125
aF3.861549072265625
aF3.5402847290039063
aF3.314491882324219
aF3.173680725097656
aF3.1100863647460937
aF3.0837637329101564
aF2.988932800292969
aF2.9147305297851562
aF2.8566357421875
aF2.805886535644531
aF2.7621054077148437
aF2.7647735595703127
aF2.715806579589844
aF2.67921630859375
aF2.671772766113281
aF2.6571231079101563
aF2.6148602294921877
aF2.6091265869140625
aF2.584020690917969
aF2.5700253295898436
aF2.5419467163085936
aF2.5288697814941408
aF2.533236389160156
aF2.5211888122558594
aF2.492054138183594
aF2.472978973388672
aF2.4494889831542968
aF2.4378370666503906
aF2.4480755615234373
aF2.4390167236328124
aF2.413146514892578
aF2.3994041442871095
aF2.4075437927246095
aF2.3907232666015625
aF2.364906005859375
aF2.363200988769531
aF2.358104248046875
aF2.3422691345214846
aF2.3395065307617187
aF2.325264739990234
aF2.322042694091797
aF2.303484344482422
aF2.303796844482422
aF2.3077096557617187
aF2.2891236877441408
aF2.2862229919433594
aF2.293534851074219
aF2.2761549377441406
aF2.2790792846679686
aF2.2736480712890623
aF2.2562571716308595
aF2.2370252990722657
aF2.2478590393066407
aF2.246162567138672
aF2.234208221435547
aF2.222056121826172
aF2.2354522705078126
aF2.219865264892578
aF2.207173156738281
aF2.210307159423828
aF2.203458251953125
aF2.2097698974609377
aF2.196788024902344
aF2.210221710205078
aF2.1862751770019533
aF2.1736192321777343
aF2.1815936279296877
aF2.187099609375
aF2.1939955139160157
aF2.151331787109375
aF2.1476708984375
aF2.1638029479980467
aF2.1572776794433595
aF2.158528137207031
aF2.149635467529297
aF2.1351515197753907
aF2.1502529907226564
aF2.1405709838867186
aF2.155808868408203
aF2.112739715576172
aF2.1219918823242185
aF2.1082807922363282
aF2.128573760986328
aF2.122367248535156
aF2.1043685913085937
aF2.1010198974609375
aF2.086778411865234
aF2.099553985595703
aF2.108456268310547
aF2.098521423339844
aF2.110201416015625
aF2.097972564697266
aF2.079334716796875
aF2.088125457763672
aF2.0705136108398436
aF2.0872178649902344
aF2.0892962646484374
aF2.0613584899902344
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0019970817290815603
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 3s'
p10
sS'final_test_loss'
p11
F2.0613584899902344
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb0\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.