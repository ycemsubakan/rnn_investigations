(dp0
S'train_loss'
p1
(lp2
F4.6515966796875
aF4.508717956542969
aF4.368901062011719
aF4.2148809814453125
aF4.056872253417969
aF3.8683786010742187
aF3.700667724609375
aF3.5118359375
aF3.3790493774414063
aF3.273548278808594
aF3.2101589965820314
aF3.1704898071289063
aF3.0878787231445313
aF3.026705017089844
aF3.0296810913085936
aF2.9628094482421874
aF2.920746154785156
aF2.884818115234375
aF2.832525634765625
aF2.8503189086914062
aF2.8006878662109376
aF2.7882928466796875
aF2.792205505371094
aF2.7510546875
aF2.7307781982421875
aF2.7151260375976562
aF2.733624267578125
aF2.682966613769531
aF2.6730242919921876
aF2.6476345825195313
aF2.646185302734375
aF2.625149841308594
aF2.6305966186523437
aF2.6043771362304686
aF2.585061340332031
aF2.575216064453125
aF2.569687805175781
aF2.5453335571289064
aF2.559401550292969
aF2.5121611022949217
aF2.5096588134765625
aF2.5168377685546877
aF2.519498596191406
aF2.487540435791016
aF2.478492431640625
aF2.479815673828125
aF2.4441012573242187
aF2.4467817687988282
aF2.4295359802246095
aF2.4306065368652345
aF2.4183717346191407
aF2.420124359130859
aF2.4219094848632814
aF2.399730224609375
aF2.411171417236328
aF2.40191162109375
aF2.3834159851074217
aF2.3810243225097656
aF2.371515655517578
aF2.3589483642578126
aF2.363325500488281
aF2.3581065368652343
aF2.3465142822265626
aF2.341394805908203
aF2.345312194824219
aF2.3395091247558595
aF2.3240179443359374
aF2.3181690979003906
aF2.3254042053222657
aF2.318486328125
aF2.329697570800781
aF2.3122880554199217
aF2.2846844482421873
aF2.3113558959960936
aF2.283711395263672
aF2.2845257568359374
aF2.2838088989257814
aF2.289961242675781
aF2.286604309082031
aF2.279980010986328
aF2.270519561767578
aF2.2553387451171876
aF2.2885610961914065
aF2.2700540161132814
aF2.253823699951172
aF2.2475738525390625
aF2.2445835876464844
aF2.2322630310058593
aF2.2445758056640623
aF2.2570565795898436
aF2.228027801513672
aF2.2404571533203126
aF2.2271372985839846
aF2.222855529785156
aF2.2191754150390626
aF2.2207342529296876
aF2.2200276184082033
aF2.2131338500976563
aF2.221366729736328
aF2.217268371582031
asS'test_loss'
p3
(lp4
F4.504878234863281
aF4.3673574829101565
aF4.213710632324219
aF4.045032958984375
aF3.8690350341796873
aF3.689608154296875
aF3.50613037109375
aF3.3541357421875
aF3.2608242797851563
aF3.175719909667969
aF3.162139892578125
aF3.074488525390625
aF3.051923828125
aF3.0193975830078124
aF2.9582723999023437
aF2.9278271484375
aF2.859927062988281
aF2.846673889160156
aF2.840201721191406
aF2.808523864746094
aF2.8146893310546877
aF2.777176513671875
aF2.7769061279296876
aF2.7384652709960937
aF2.7129913330078126
aF2.6898553466796873
aF2.6801495361328125
aF2.6577642822265624
aF2.633122863769531
aF2.610754089355469
aF2.604583435058594
aF2.6026986694335936
aF2.5929916381835936
aF2.6112319946289064
aF2.559817199707031
aF2.5504383850097656
aF2.5389387512207033
aF2.5319841003417967
aF2.518304443359375
aF2.515627746582031
aF2.4839686584472656
aF2.494160003662109
aF2.4692979431152344
aF2.473233184814453
aF2.4895425415039063
aF2.4573992919921874
aF2.4373733520507814
aF2.42336669921875
aF2.4357450866699217
aF2.4347518920898437
aF2.431681365966797
aF2.3945098876953126
aF2.3994485473632814
aF2.3794683837890624
aF2.3880854797363282
aF2.3798031616210937
aF2.349798126220703
aF2.357447052001953
aF2.39362548828125
aF2.3505908203125
aF2.3410296630859375
aF2.3537217712402345
aF2.350930938720703
aF2.3509669494628906
aF2.3454327392578125
aF2.320018310546875
aF2.320603790283203
aF2.3318415832519532
aF2.323291015625
aF2.3014561462402345
aF2.313645782470703
aF2.300963134765625
aF2.295795745849609
aF2.310865783691406
aF2.2718234252929688
aF2.288014678955078
aF2.2801011657714843
aF2.268889617919922
aF2.260203094482422
aF2.2461843872070313
aF2.2458207702636717
aF2.263060302734375
aF2.2717144775390623
aF2.2508953857421874
aF2.2339935302734375
aF2.246845550537109
aF2.2389306640625
aF2.2415538024902344
aF2.2191702270507814
aF2.2468991088867187
aF2.2301957702636717
aF2.229805145263672
aF2.2287496948242187
aF2.2232391357421877
aF2.2091940307617186
aF2.209613037109375
aF2.225840301513672
aF2.2058596801757813
aF2.1834925842285156
aF2.192893371582031
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0013786791704744788
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 58s'
p10
sS'final_test_loss'
p11
F2.192893371582031
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x96\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.