(dp0
S'train_loss'
p1
(lp2
F4.637433166503906
aF4.2169921875
aF3.7040093994140624
aF3.267454833984375
aF3.1133251953125
aF3.035299987792969
aF2.9250479125976563
aF2.8574365234375
aF2.8080657958984374
aF2.7563836669921873
aF2.7086557006835936
aF2.689474792480469
aF2.681390380859375
aF2.6104144287109374
aF2.578898620605469
aF2.5741067504882813
aF2.5283058166503904
aF2.505026397705078
aF2.492578430175781
aF2.4676248168945314
aF2.451610260009766
aF2.4286572265625
aF2.4334474182128907
aF2.394081573486328
aF2.379835662841797
aF2.3842176818847656
aF2.372276611328125
aF2.362160949707031
aF2.34387939453125
aF2.335767364501953
aF2.3245085144042967
aF2.313047332763672
aF2.283896636962891
aF2.278950958251953
aF2.2718077087402344
aF2.282467498779297
aF2.2608221435546874
aF2.244917755126953
aF2.2368157958984374
aF2.227632751464844
aF2.2165316772460937
aF2.2225929260253907
aF2.231195373535156
aF2.1916929626464845
aF2.2094898986816407
aF2.1673092651367187
aF2.184711608886719
aF2.1712710571289064
aF2.1773841857910154
aF2.157431640625
aF2.1606329345703124
aF2.147684326171875
aF2.153403472900391
aF2.1480343627929686
aF2.1352394104003904
aF2.1292559814453127
aF2.118721466064453
aF2.117220916748047
aF2.126177978515625
aF2.1071347045898436
aF2.0943711853027343
aF2.1093399047851564
aF2.086414794921875
aF2.072579803466797
aF2.1077499389648438
aF2.0729739379882814
aF2.0693893432617188
aF2.0724697875976563
aF2.0628089904785156
aF2.064564514160156
aF2.0250929260253905
aF2.063221282958984
aF2.058107452392578
aF2.036399383544922
aF2.056929016113281
aF2.0505802917480467
aF2.0518829345703127
aF2.02705322265625
aF2.031660614013672
aF2.0231083679199218
aF2.019467010498047
aF2.0298558044433594
aF2.0217216491699217
aF1.9934181213378905
aF2.0193748474121094
aF2.0386894226074217
aF1.9725765991210937
aF2.0031529235839844
aF1.999606170654297
aF2.0164833068847656
aF1.9846365356445312
aF1.9997062683105469
aF1.9966156005859375
aF1.9701319885253907
aF1.9758544921875
aF1.9778837585449218
aF1.9737469482421874
aF1.9601907348632812
aF1.9703919982910156
aF1.9628160095214844
asS'test_loss'
p3
(lp4
F4.204857482910156
aF3.693411560058594
aF3.2597622680664062
aF3.1323538208007813
aF3.0053750610351564
aF2.9386572265625
aF2.8438143920898438
aF2.8114520263671876
aF2.7569961547851562
aF2.7455108642578123
aF2.673433837890625
aF2.656258544921875
aF2.614019775390625
aF2.5785736083984374
aF2.559095611572266
aF2.5069096374511717
aF2.5023583984375
aF2.493474884033203
aF2.4665342712402345
aF2.452805480957031
aF2.4313819885253904
aF2.4125701904296877
aF2.3945745849609374
aF2.383348083496094
aF2.361344757080078
aF2.36208251953125
aF2.357518768310547
aF2.3413885498046874
aF2.315482330322266
aF2.3182449340820312
aF2.289705963134766
aF2.294109191894531
aF2.289967803955078
aF2.2547430419921874
aF2.2687672424316405
aF2.253073272705078
aF2.238356475830078
aF2.2468222045898436
aF2.246717834472656
aF2.2060809326171875
aF2.2065301513671876
aF2.206077880859375
aF2.203257293701172
aF2.1903143310546875
aF2.185702056884766
aF2.164388885498047
aF2.1782501220703123
aF2.1961549377441405
aF2.157942352294922
aF2.1487435913085937
aF2.135945129394531
aF2.1368896484375
aF2.148338623046875
aF2.1094390869140627
aF2.1176356506347656
aF2.122281188964844
aF2.1249430847167967
aF2.0982925415039064
aF2.111767578125
aF2.1142628479003904
aF2.0894325256347654
aF2.08662353515625
aF2.0919534301757814
aF2.079276885986328
aF2.0877037048339844
aF2.062761688232422
aF2.055836944580078
aF2.0565530395507814
aF2.0741763305664063
aF2.0513775634765623
aF2.0699478149414063
aF2.0734716796875
aF2.039739685058594
aF2.0343754577636717
aF2.0527333068847655
aF2.01030517578125
aF2.025511474609375
aF2.032906951904297
aF2.0304275512695313
aF2.015436553955078
aF2.030436706542969
aF2.014527130126953
aF2.016340637207031
aF1.987494659423828
aF2.0182379150390624
aF1.9892398071289064
aF1.9994676208496094
aF1.9792697143554687
aF2.0054331970214845
aF1.9803144836425781
aF1.9750523376464844
aF1.9821549987792968
aF1.9708787536621093
aF1.9799745178222656
aF1.9854635620117187
aF1.980474395751953
aF1.9708261108398437
aF1.9497137451171875
aF1.943668975830078
aF1.9456626892089843
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0062455333715231015
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 53s'
p10
sS'final_test_loss'
p11
F1.9456626892089843
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'u\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.