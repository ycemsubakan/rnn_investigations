(dp0
S'train_loss'
p1
(lp2
F4.6258392333984375
aF4.606612548828125
aF4.596815795898437
aF4.581720886230468
aF4.568721618652344
aF4.551674194335938
aF4.538468322753906
aF4.526959533691406
aF4.5133734130859375
aF4.495274353027344
aF4.482049865722656
aF4.465987854003906
aF4.4521484375
aF4.431283569335937
aF4.419095153808594
aF4.400371704101563
aF4.385985412597656
aF4.3608856201171875
aF4.336712951660156
aF4.3185208129882815
aF4.294685363769531
aF4.2725677490234375
aF4.242686767578125
aF4.219496154785157
aF4.200027770996094
aF4.165345764160156
aF4.144010314941406
aF4.113235778808594
aF4.095061645507813
aF4.0668637084960935
aF4.037716979980469
aF3.9994155883789064
aF3.984544677734375
aF3.961739501953125
aF3.9236831665039062
aF3.893114013671875
aF3.873839111328125
aF3.8602520751953127
aF3.8258792114257814
aF3.8060009765625
aF3.786813049316406
aF3.7658734130859375
aF3.7477423095703126
aF3.73544921875
aF3.7199172973632812
aF3.6856280517578126
aF3.6857589721679687
aF3.65498779296875
aF3.6444656372070314
aF3.620557861328125
aF3.623782958984375
aF3.6188360595703126
aF3.606511535644531
aF3.58381591796875
aF3.5855426025390624
aF3.557774658203125
aF3.5397271728515625
aF3.5406283569335937
aF3.52258056640625
aF3.5149749755859374
aF3.5120440673828126
aF3.4919500732421875
aF3.4825289916992186
aF3.4820294189453125
aF3.4455706787109377
aF3.4488571166992186
aF3.4370318603515626
aF3.44972412109375
aF3.4278335571289062
aF3.4303213500976564
aF3.421988830566406
aF3.4284173583984376
aF3.4126153564453126
aF3.401217041015625
aF3.4106039428710937
aF3.4124185180664064
aF3.37076904296875
aF3.37509033203125
aF3.376847839355469
aF3.3707571411132813
aF3.38065673828125
aF3.3777960205078124
aF3.3639898681640625
aF3.366585693359375
aF3.369875793457031
aF3.3495892333984374
aF3.3548565673828126
aF3.3485650634765625
aF3.3500439453125
aF3.3388873291015626
aF3.331593017578125
aF3.35099609375
aF3.313737487792969
aF3.3117059326171874
aF3.3291751098632814
aF3.3347314453125
aF3.3398617553710936
aF3.304368591308594
aF3.307653503417969
aF3.2952520751953127
asS'test_loss'
p3
(lp4
F4.61092529296875
aF4.5988525390625
aF4.583514099121094
aF4.567320556640625
aF4.554618530273437
aF4.541856079101563
aF4.524774169921875
aF4.514208679199219
aF4.499983825683594
aF4.483421020507812
aF4.467940063476562
aF4.4494866943359375
aF4.436965026855469
aF4.4202401733398435
aF4.402199096679688
aF4.378040466308594
aF4.358214416503906
aF4.338184204101562
aF4.321930541992187
aF4.292779235839844
aF4.26880859375
aF4.247442626953125
aF4.220223999023437
aF4.201264038085937
aF4.165585327148437
aF4.140386962890625
aF4.111325073242187
aF4.090321350097656
aF4.050621337890625
aF4.030535278320312
aF4.0059747314453125
aF3.9663204956054687
aF3.9595706176757814
aF3.9203326416015627
aF3.880945739746094
aF3.890018310546875
aF3.8632855224609375
aF3.8470254516601563
aF3.81919677734375
aF3.7818264770507812
aF3.7766650390625
aF3.7458270263671873
aF3.7075927734375
aF3.691966552734375
aF3.693330078125
aF3.6788754272460937
aF3.6621353149414064
aF3.6526028442382814
aF3.631089172363281
aF3.6199185180664064
aF3.5952703857421877
aF3.5828536987304687
aF3.5630377197265624
aF3.5587957763671874
aF3.5527008056640623
aF3.5441259765625
aF3.5231365966796875
aF3.4973330688476563
aF3.523902587890625
aF3.494040222167969
aF3.4875057983398436
aF3.4641748046875
aF3.4703436279296875
aF3.4474942016601564
aF3.4550537109375
aF3.4605984497070312
aF3.444061584472656
aF3.4236688232421875
aF3.425321044921875
aF3.4437005615234373
aF3.4368521118164064
aF3.3830252075195313
aF3.399369201660156
aF3.400094299316406
aF3.392547912597656
aF3.419630126953125
aF3.3931478881835937
aF3.383328552246094
aF3.3757479858398436
aF3.402832336425781
aF3.3558251953125
aF3.3681259155273438
aF3.3455303955078124
aF3.366877136230469
aF3.329920349121094
aF3.3535028076171876
aF3.35417236328125
aF3.3316015625
aF3.3349468994140623
aF3.32946044921875
aF3.3200497436523437
aF3.32606201171875
aF3.3134219360351564
aF3.305152587890625
aF3.313529052734375
aF3.301918640136719
aF3.3159671020507813
aF3.296441955566406
aF3.2929498291015626
aF3.3120068359375
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00036460390515742446
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 2s'
p10
sS'final_test_loss'
p11
F3.3120068359375
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.