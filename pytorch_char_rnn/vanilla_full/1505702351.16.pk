(dp0
S'train_loss'
p1
(lp2
F4.637555541992188
aF4.299512329101563
aF3.9750234985351565
aF3.6241519165039064
aF3.3217483520507813
aF3.139751281738281
aF3.014344482421875
aF2.973358459472656
aF2.8718878173828126
aF2.81333984375
aF2.770045166015625
aF2.732572326660156
aF2.709264221191406
aF2.673495178222656
aF2.64783935546875
aF2.6365280151367188
aF2.575331115722656
aF2.550226593017578
aF2.5397512817382815
aF2.508997344970703
aF2.510950622558594
aF2.4675706481933593
aF2.496085357666016
aF2.437851867675781
aF2.4424168395996095
aF2.4209698486328124
aF2.4216676330566407
aF2.4156620788574217
aF2.38621337890625
aF2.383645782470703
aF2.367517852783203
aF2.3530157470703124
aF2.3357008361816405
aF2.3285789489746094
aF2.320576019287109
aF2.3033262634277345
aF2.2923484802246095
aF2.2828993225097656
aF2.2903538513183594
aF2.2619534301757813
aF2.246629638671875
aF2.277402191162109
aF2.2477574157714844
aF2.2407554626464843
aF2.229962158203125
aF2.2337777709960935
aF2.2242626953125
aF2.2295835876464842
aF2.2092135620117186
aF2.215639801025391
aF2.1895184326171875
aF2.2036961364746093
aF2.1997500610351564
aF2.2015736389160154
aF2.164544982910156
aF2.168512268066406
aF2.170590667724609
aF2.169955139160156
aF2.1505450439453124
aF2.149880065917969
aF2.145557861328125
aF2.122210693359375
aF2.1412510681152344
aF2.1462968444824218
aF2.1157542419433595
aF2.1036636352539064
aF2.116357727050781
aF2.1071128845214844
aF2.107003631591797
aF2.1027708435058594
aF2.076360626220703
aF2.091203155517578
aF2.084184265136719
aF2.086353759765625
aF2.0732188415527344
aF2.0837448120117186
aF2.0679742431640626
aF2.073018341064453
aF2.061145935058594
aF2.0516046142578124
aF2.0628102111816404
aF2.0647343444824218
aF2.0585397338867186
aF2.0512576293945313
aF2.052021789550781
aF2.0418885803222655
aF2.047674102783203
aF2.048595733642578
aF2.0382504272460937
aF2.0193827819824217
aF2.0228411865234377
aF2.031827697753906
aF2.036328125
aF2.0263401794433595
aF2.0370513916015627
aF2.0199736022949217
aF2.0061276245117186
aF2.0028480529785155
aF2.0008290100097654
aF2.0145960998535157
asS'test_loss'
p3
(lp4
F4.2956369018554685
aF3.965276184082031
aF3.604250183105469
aF3.3052142333984373
aF3.109374694824219
aF2.978883056640625
aF2.9544366455078124
aF2.8700131225585936
aF2.824804382324219
aF2.760738525390625
aF2.7167205810546875
aF2.704822692871094
aF2.6583782958984377
aF2.645399475097656
aF2.620075378417969
aF2.600713806152344
aF2.545410919189453
aF2.519442596435547
aF2.5078582763671875
aF2.507459259033203
aF2.481644592285156
aF2.44736083984375
aF2.44298828125
aF2.4448472595214845
aF2.412669982910156
aF2.412217712402344
aF2.3842146301269533
aF2.392958679199219
aF2.388441467285156
aF2.3547235107421876
aF2.3343777465820312
aF2.3335452270507813
aF2.3259324645996093
aF2.3205743408203126
aF2.3061453247070314
aF2.266291198730469
aF2.294137725830078
aF2.2841610717773437
aF2.2703456115722656
aF2.2550599670410154
aF2.242845306396484
aF2.2532275390625
aF2.2419479370117186
aF2.2259980773925783
aF2.2223480224609373
aF2.2177354431152345
aF2.1973060607910155
aF2.2156797790527345
aF2.2174954223632812
aF2.191777648925781
aF2.1865171813964843
aF2.1814834594726564
aF2.1822579956054686
aF2.1882481384277344
aF2.183771514892578
aF2.1428042602539064
aF2.1604754638671877
aF2.1495785522460937
aF2.1497584533691407
aF2.1420472717285155
aF2.1370059204101564
aF2.128490142822266
aF2.137622528076172
aF2.136479949951172
aF2.116025848388672
aF2.1121005249023437
aF2.1007330322265627
aF2.1030694580078126
aF2.0933872985839845
aF2.078172302246094
aF2.1042034912109373
aF2.0743173217773436
aF2.088265838623047
aF2.067591094970703
aF2.0888365173339842
aF2.0852879333496093
aF2.0729135131835936
aF2.093927764892578
aF2.0492097473144533
aF2.0434625244140623
aF2.0551531982421873
aF2.045332946777344
aF2.0739561462402345
aF2.048550720214844
aF2.0594464111328126
aF2.0366937255859376
aF2.037487335205078
aF2.0415020751953126
aF2.032815399169922
aF2.0280804443359375
aF2.015441436767578
aF2.0290182495117186
aF2.030104675292969
aF2.0264517211914064
aF2.0074815368652343
aF2.0170881652832033
aF1.9911674499511718
aF2.004496765136719
aF2.015483856201172
aF1.9928758239746094
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0014893589095317736
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 22s'
p10
sS'final_test_loss'
p11
F1.9928758239746094
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x15\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.