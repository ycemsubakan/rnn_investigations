(dp0
S'train_loss'
p1
(lp2
F4.612400207519531
aF4.394161376953125
aF4.157450256347656
aF3.8702096557617187
aF3.598796081542969
aF3.4263824462890624
aF3.3030044555664064
aF3.254707946777344
aF3.204544677734375
aF3.1726568603515624
aF3.087113952636719
aF3.0283502197265624
aF2.9790243530273437
aF2.9330010986328126
aF2.8812081909179685
aF2.833154296875
aF2.7910699462890625
aF2.7838836669921876
aF2.7668838500976562
aF2.7331393432617186
aF2.693404235839844
aF2.6864614868164063
aF2.6471954345703126
aF2.615631408691406
aF2.603515930175781
aF2.5857504272460936
aF2.555857391357422
aF2.5659671020507813
aF2.5265571594238283
aF2.518731689453125
aF2.4929495239257813
aF2.4694384765625
aF2.4634634399414064
aF2.454132843017578
aF2.4350265502929687
aF2.409800720214844
aF2.4073023986816406
aF2.3931678771972655
aF2.405764312744141
aF2.392078399658203
aF2.392270965576172
aF2.359788513183594
aF2.3453541564941407
aF2.332957458496094
aF2.322641143798828
aF2.293790283203125
aF2.3165289306640626
aF2.2989501953125
aF2.300018768310547
aF2.3078997802734373
aF2.2917494201660156
aF2.2839601135253904
aF2.285423126220703
aF2.257598876953125
aF2.242492218017578
aF2.2570526123046877
aF2.2545680236816406
aF2.247264862060547
aF2.2386619567871096
aF2.220197906494141
aF2.2324298095703123
aF2.2080999755859376
aF2.2266241455078126
aF2.1922735595703124
aF2.2085374450683593
aF2.206884460449219
aF2.1833544921875
aF2.175156555175781
aF2.1667527770996093
aF2.159055023193359
aF2.1523687744140627
aF2.168498229980469
aF2.1445252990722654
aF2.1610069274902344
aF2.136361083984375
aF2.1501834106445314
aF2.135187225341797
aF2.102603759765625
aF2.1248294067382814
aF2.1284498596191406
aF2.131114196777344
aF2.1197914123535155
aF2.1121791076660155
aF2.1296037292480468
aF2.1067010498046876
aF2.1161654663085936
aF2.0952328491210936
aF2.0894390869140627
aF2.0950576782226564
aF2.0892208862304686
aF2.0852728271484375
aF2.081509857177734
aF2.071758270263672
aF2.077486572265625
aF2.0621603393554686
aF2.0697618103027344
aF2.064927825927734
aF2.059908142089844
aF2.052587127685547
aF2.0221401977539064
asS'test_loss'
p3
(lp4
F4.392230834960937
aF4.159432983398437
aF3.8756332397460938
aF3.593536682128906
aF3.422205810546875
aF3.339502868652344
aF3.2702130126953124
aF3.2086825561523438
aF3.1601739501953126
aF3.0645318603515626
aF3.03333740234375
aF2.98147705078125
aF2.935893249511719
aF2.8822509765625
aF2.8541748046875
aF2.801660461425781
aF2.783497009277344
aF2.7718402099609376
aF2.70613525390625
aF2.708476867675781
aF2.6690689086914063
aF2.653408203125
aF2.6172305297851564
aF2.6107611083984374
aF2.5881497192382814
aF2.5463914489746093
aF2.5345587158203124
aF2.53465576171875
aF2.5126138305664063
aF2.5100386047363283
aF2.4472396850585936
aF2.455835723876953
aF2.465894927978516
aF2.43949462890625
aF2.418298797607422
aF2.3959696960449217
aF2.387129364013672
aF2.403887939453125
aF2.3783924865722654
aF2.3573239135742186
aF2.3534820556640623
aF2.354131774902344
aF2.362367858886719
aF2.3212152099609376
aF2.331528778076172
aF2.31481689453125
aF2.300309906005859
aF2.299593200683594
aF2.276145477294922
aF2.2767843627929687
aF2.2906954956054686
aF2.267705383300781
aF2.2560578918457033
aF2.250685729980469
aF2.2255224609375
aF2.2243441772460937
aF2.230701141357422
aF2.2191921997070314
aF2.234520263671875
aF2.224525909423828
aF2.2166375732421875
aF2.2089306640625
aF2.2077410888671873
aF2.195876159667969
aF2.1911776733398436
aF2.1779151916503907
aF2.1692800903320313
aF2.1784294128417967
aF2.1666499328613282
aF2.169131622314453
aF2.162569580078125
aF2.1454931640625
aF2.139559783935547
aF2.14501220703125
aF2.138669891357422
aF2.1327890014648436
aF2.1430392456054688
aF2.1307505798339843
aF2.1267591857910157
aF2.1247369384765626
aF2.109453582763672
aF2.1088816833496096
aF2.0977407836914064
aF2.087146453857422
aF2.088759002685547
aF2.1159437561035155
aF2.112018890380859
aF2.075653839111328
aF2.072391357421875
aF2.0750027465820313
aF2.066322479248047
aF2.061250915527344
aF2.0556790161132814
aF2.050281677246094
aF2.056309814453125
aF2.070365447998047
aF2.0643606567382813
aF2.0694903564453124
aF2.0230363464355468
aF2.0610691833496095
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0009118815290008003
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 16s'
p10
sS'final_test_loss'
p11
F2.0610691833496095
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x06\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.