(dp0
S'train_loss'
p1
(lp2
F4.607928161621094
aF4.428187561035156
aF4.226903991699219
aF3.9636721801757813
aF3.6876287841796875
aF3.5006573486328123
aF3.3974346923828125
aF3.314708251953125
aF3.2786944580078123
aF3.1895587158203127
aF3.193875427246094
aF3.1199346923828126
aF3.0614556884765625
aF3.0150045776367187
aF2.9786422729492186
aF2.96338623046875
aF2.902777099609375
aF2.8760134887695314
aF2.8614993286132813
aF2.8249688720703126
aF2.7983782958984373
aF2.7837399291992186
aF2.7540628051757814
aF2.7220053100585937
aF2.6835232543945313
aF2.67649169921875
aF2.6306060791015624
aF2.617276306152344
aF2.5898272705078127
aF2.5837911987304687
aF2.585750732421875
aF2.5614828491210937
aF2.5493734741210936
aF2.5313290405273436
aF2.5151240539550783
aF2.497882843017578
aF2.4748184204101564
aF2.4715122985839844
aF2.4645870971679686
aF2.454362335205078
aF2.413227844238281
aF2.4299250793457032
aF2.417519073486328
aF2.3916134643554687
aF2.4006422424316405
aF2.3769790649414064
aF2.3587892150878904
aF2.3755732727050782
aF2.363255615234375
aF2.340068664550781
aF2.3494418334960936
aF2.341683654785156
aF2.313812561035156
aF2.29619384765625
aF2.3006834411621093
aF2.288457946777344
aF2.3150601196289062
aF2.298186798095703
aF2.290840301513672
aF2.2755274963378906
aF2.279681396484375
aF2.2654295349121094
aF2.2493246459960936
aF2.2549252319335937
aF2.2404608154296874
aF2.2333663940429687
aF2.240638885498047
aF2.227733154296875
aF2.2178956604003908
aF2.216490936279297
aF2.2049771118164063
aF2.2053733825683595
aF2.228917541503906
aF2.193326110839844
aF2.189312744140625
aF2.1906756591796874
aF2.1675396728515626
aF2.1858518981933592
aF2.159106903076172
aF2.1649163818359374
aF2.1777955627441408
aF2.161679534912109
aF2.1424794006347656
aF2.145735168457031
aF2.1607823181152344
aF2.1461912536621095
aF2.1358908081054686
aF2.1383154296875
aF2.1096644592285156
aF2.124520111083984
aF2.1269956970214845
aF2.108919982910156
aF2.114937286376953
aF2.1018988037109376
aF2.117173767089844
aF2.0948672485351563
aF2.0868684387207033
aF2.1004148864746095
aF2.102663726806641
aF2.0942349243164062
asS'test_loss'
p3
(lp4
F4.429012451171875
aF4.222530517578125
aF3.96580078125
aF3.6713943481445312
aF3.4859222412109374
aF3.3766961669921876
aF3.3183251953125
aF3.2649356079101564
aF3.228699951171875
aF3.182657470703125
aF3.155909423828125
aF3.06925048828125
aF3.0323486328125
aF2.956478271484375
aF2.9667279052734377
aF2.90904296875
aF2.89474609375
aF2.8492050170898438
aF2.8109664916992188
aF2.794747619628906
aF2.7549456787109374
aF2.7430758666992188
aF2.7207852172851563
aF2.6847589111328123
aF2.6706170654296875
aF2.641108093261719
aF2.6179901123046876
aF2.613971862792969
aF2.587524719238281
aF2.5539669799804687
aF2.547895965576172
aF2.5244210815429686
aF2.5119090270996094
aF2.4958226013183595
aF2.4969131469726564
aF2.4885353088378905
aF2.45659423828125
aF2.4541506958007813
aF2.443301086425781
aF2.4336952209472655
aF2.4260345458984376
aF2.4139060974121094
aF2.4034609985351563
aF2.3852468872070314
aF2.3689694213867187
aF2.352520294189453
aF2.3755223083496095
aF2.336478729248047
aF2.346998748779297
aF2.3423556518554687
aF2.322035827636719
aF2.328369903564453
aF2.284138641357422
aF2.318933410644531
aF2.283633270263672
aF2.2892471313476563
aF2.2765565490722657
aF2.28076904296875
aF2.2734696960449217
aF2.251026153564453
aF2.2392868041992187
aF2.269305877685547
aF2.240672149658203
aF2.2451406860351564
aF2.22761474609375
aF2.219951477050781
aF2.232218475341797
aF2.189985656738281
aF2.1957586669921874
aF2.203655090332031
aF2.183802185058594
aF2.2030548095703124
aF2.191447448730469
aF2.194326171875
aF2.191935272216797
aF2.1789996337890627
aF2.180975036621094
aF2.162372894287109
aF2.1600935363769533
aF2.1577198791503904
aF2.150260467529297
aF2.1584169006347658
aF2.1445321655273437
aF2.148101043701172
aF2.134052581787109
aF2.1417127990722657
aF2.1335905456542967
aF2.11699951171875
aF2.1326947021484375
aF2.099837188720703
aF2.10033935546875
aF2.092853240966797
aF2.090667724609375
aF2.1059335327148436
aF2.1146363830566406
aF2.0841030883789062
aF2.096208038330078
aF2.0823651123046876
aF2.0671676635742187
aF2.081623077392578
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0013595115986474895
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 41s'
p10
sS'final_test_loss'
p11
F2.081623077392578
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xaa\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.