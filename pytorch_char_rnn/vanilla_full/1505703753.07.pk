(dp0
S'train_loss'
p1
(lp2
F4.636886291503906
aF4.609775390625
aF4.583603515625
aF4.561669921875
aF4.537538757324219
aF4.508298645019531
aF4.4846734619140625
aF4.457969360351562
aF4.43129150390625
aF4.402275695800781
aF4.378179016113282
aF4.345272521972657
aF4.317383422851562
aF4.278172912597657
aF4.24042236328125
aF4.206715087890625
aF4.164976806640625
aF4.127803344726562
aF4.078515625
aF4.0364111328125
aF3.979367980957031
aF3.938609619140625
aF3.9064401245117186
aF3.8508538818359375
aF3.7793612670898438
aF3.74452880859375
aF3.696535339355469
aF3.6640744018554687
aF3.622958068847656
aF3.5830563354492186
aF3.56963623046875
aF3.5398516845703125
aF3.4923526000976564
aF3.4847235107421874
aF3.469794921875
aF3.4408663940429687
aF3.4361083984375
aF3.4117169189453125
aF3.397136535644531
aF3.382139892578125
aF3.370377197265625
aF3.3605587768554686
aF3.330771789550781
aF3.336212158203125
aF3.3053802490234374
aF3.32267578125
aF3.3097183227539064
aF3.2935409545898438
aF3.2771490478515624
aF3.287583312988281
aF3.235026550292969
aF3.2390936279296874
aF3.2377020263671876
aF3.235670166015625
aF3.2278213500976562
aF3.1954946899414063
aF3.18073486328125
aF3.184228820800781
aF3.180093994140625
aF3.161789855957031
aF3.1611187744140623
aF3.168859558105469
aF3.149957275390625
aF3.1221469116210936
aF3.14654052734375
aF3.112462158203125
aF3.1228176879882814
aF3.1199383544921875
aF3.0974298095703126
aF3.0738995361328123
aF3.075749816894531
aF3.0762448120117187
aF3.0522140502929687
aF3.0533761596679687
aF3.066971435546875
aF3.0362930297851562
aF3.057645568847656
aF3.043290710449219
aF3.0226263427734374
aF3.024017028808594
aF3.0040151977539065
aF3.0012222290039063
aF2.981982421875
aF2.991917724609375
aF2.9627685546875
aF2.962904357910156
aF2.9766357421875
aF2.9554348754882813
aF2.99514404296875
aF2.9430801391601564
aF2.940077209472656
aF2.9366302490234375
aF2.9288735961914063
aF2.9239041137695314
aF2.904739990234375
aF2.90368408203125
aF2.9025750732421876
aF2.895815124511719
aF2.918045959472656
aF2.865231628417969
asS'test_loss'
p3
(lp4
F4.609716796875
aF4.585917663574219
aF4.560242614746094
aF4.536830444335937
aF4.509771118164062
aF4.486268920898437
aF4.457422180175781
aF4.432945861816406
aF4.40562255859375
aF4.378821105957031
aF4.350095520019531
aF4.310421752929687
aF4.279659729003907
aF4.246902160644531
aF4.2111749267578125
aF4.166829223632813
aF4.129726257324219
aF4.088237609863281
aF4.032315979003906
aF3.985682373046875
aF3.934822998046875
aF3.87933837890625
aF3.8309292602539062
aF3.7936416625976563
aF3.7403829956054686
aF3.707503662109375
aF3.6579620361328127
aF3.622669982910156
aF3.5946832275390626
aF3.552631530761719
aF3.5175238037109375
aF3.509549865722656
aF3.480167236328125
aF3.4742745971679687
aF3.4528372192382815
aF3.4642681884765625
aF3.410286865234375
aF3.403187255859375
aF3.381092834472656
aF3.3714263916015623
aF3.372822265625
aF3.370467529296875
aF3.340243225097656
aF3.3098745727539063
aF3.299141845703125
aF3.29046630859375
aF3.290238952636719
aF3.2729632568359377
aF3.2772811889648437
aF3.243941345214844
aF3.2365496826171873
aF3.238770751953125
aF3.2098965454101562
aF3.2171380615234373
aF3.190427551269531
aF3.2024224853515624
aF3.174386291503906
aF3.1660421752929686
aF3.17199951171875
aF3.161766357421875
aF3.135129089355469
aF3.1038504028320313
aF3.1445269775390625
aF3.0979244995117186
aF3.14127685546875
aF3.1064303588867186
aF3.0843206787109376
aF3.101171569824219
aF3.080164489746094
aF3.071458740234375
aF3.0781011962890625
aF3.0646484375
aF3.041883544921875
aF3.03899169921875
aF3.037039794921875
aF3.0267535400390626
aF3.033327331542969
aF3.0188848876953127
aF3.0115765380859374
aF2.9931765747070314
aF3.0022952270507814
aF2.9730050659179685
aF3.0050103759765623
aF2.978287658691406
aF2.9644622802734375
aF2.9534832763671877
aF2.958095703125
aF2.952334289550781
aF2.941077880859375
aF2.9477297973632814
aF2.925396728515625
aF2.9166107177734375
aF2.91748779296875
aF2.9296844482421873
aF2.9007513427734377
aF2.8879583740234374
aF2.8799957275390624
aF2.89271728515625
aF2.88662109375
aF2.8698690795898436
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.000167284172829982
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 42s'
p10
sS'final_test_loss'
p11
F2.8698690795898436
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb4\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.