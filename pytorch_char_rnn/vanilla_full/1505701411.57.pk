(dp0
S'train_loss'
p1
(lp2
F4.651488037109375
aF4.381984252929687
aF4.106773681640625
aF3.7692294311523438
aF3.457661437988281
aF3.303857421875
aF3.28931640625
aF3.1703369140625
aF3.09844970703125
aF3.0082196044921874
aF2.9660610961914062
aF2.9276693725585936
aF2.884316101074219
aF2.8373483276367186
aF2.807684020996094
aF2.7768463134765624
aF2.7500421142578126
aF2.7002548217773437
aF2.7005548095703125
aF2.6706671142578124
aF2.643366394042969
aF2.622577819824219
aF2.6100949096679686
aF2.581659240722656
aF2.565260925292969
aF2.5293898010253906
aF2.53351806640625
aF2.5268479919433595
aF2.5133932495117186
aF2.4923272705078126
aF2.472431945800781
aF2.4597547912597655
aF2.451578369140625
aF2.433613586425781
aF2.4306376647949217
aF2.3931109619140627
aF2.408642120361328
aF2.398448028564453
aF2.3823541259765624
aF2.3785809326171874
aF2.3668589782714844
aF2.3495086669921874
aF2.3443304443359376
aF2.349029846191406
aF2.333688049316406
aF2.3158319091796873
aF2.340332794189453
aF2.3199542236328123
aF2.3171340942382814
aF2.306426544189453
aF2.2955950927734374
aF2.290455780029297
aF2.3044772338867188
aF2.288557281494141
aF2.266904144287109
aF2.26438232421875
aF2.2462727355957033
aF2.261981201171875
aF2.2646910095214845
aF2.2460145568847656
aF2.239171905517578
aF2.2487228393554686
aF2.219794921875
aF2.237196197509766
aF2.228998718261719
aF2.2131889343261717
aF2.206033172607422
aF2.188384552001953
aF2.1958357238769532
aF2.185072479248047
aF2.184959716796875
aF2.195734558105469
aF2.1845108032226563
aF2.1505186462402346
aF2.17850830078125
aF2.1605900573730468
aF2.1593824768066407
aF2.1728170776367186
aF2.157186737060547
aF2.138316650390625
aF2.1511239624023437
aF2.140918731689453
aF2.140719299316406
aF2.141904296875
aF2.1351490783691407
aF2.1287159729003906
aF2.1394044494628908
aF2.136659851074219
aF2.1392262268066404
aF2.110836639404297
aF2.101030731201172
aF2.1218081665039064
aF2.100206756591797
aF2.1060302734375
aF2.101093292236328
aF2.1018443298339844
aF2.1059091186523435
aF2.0891595458984376
aF2.091331329345703
aF2.0758406066894532
asS'test_loss'
p3
(lp4
F4.385777893066407
aF4.107731628417969
aF3.7642022705078126
aF3.4725408935546875
aF3.28152587890625
aF3.2291510009765627
aF3.1661895751953124
aF3.112734069824219
aF3.0319073486328123
aF2.9593536376953127
aF2.8981143188476564
aF2.8782879638671877
aF2.863697509765625
aF2.802803649902344
aF2.7618087768554687
aF2.7175747680664064
aF2.725467529296875
aF2.6988784790039064
aF2.6538446044921873
aF2.633708190917969
aF2.6153414916992186
aF2.5900250244140626
aF2.6046383666992186
aF2.559836120605469
aF2.537333221435547
aF2.5025025939941408
aF2.511720733642578
aF2.4911148071289064
aF2.479359436035156
aF2.4710035705566407
aF2.4543890380859374
aF2.437441101074219
aF2.4265762329101563
aF2.4074967956542968
aF2.4172897338867188
aF2.407220916748047
aF2.3995431518554686
aF2.3729739379882813
aF2.360551300048828
aF2.3490582275390626
aF2.325317687988281
aF2.3297245788574217
aF2.3014674377441406
aF2.318693084716797
aF2.332250213623047
aF2.3140411376953125
aF2.307855224609375
aF2.2934588623046874
aF2.2737091064453123
aF2.297241668701172
aF2.288418426513672
aF2.2805747985839844
aF2.275654754638672
aF2.259866180419922
aF2.2545680236816406
aF2.266401824951172
aF2.239444122314453
aF2.2348924255371094
aF2.222939453125
aF2.230465087890625
aF2.2261398315429686
aF2.2254901123046875
aF2.22384765625
aF2.2062998962402345
aF2.206742401123047
aF2.1912083435058594
aF2.1966256713867187
aF2.200647430419922
aF2.1818524169921876
aF2.1829264831542967
aF2.1967547607421873
aF2.177789306640625
aF2.1797406005859377
aF2.163080139160156
aF2.170351104736328
aF2.1424714660644533
aF2.1443939208984375
aF2.1602217102050782
aF2.161684875488281
aF2.1484490966796876
aF2.1377561950683592
aF2.1441273498535156
aF2.1362738037109374
aF2.1086936950683595
aF2.1163726806640626
aF2.1198284912109373
aF2.1011811828613283
aF2.119675598144531
aF2.1253146362304687
aF2.123942413330078
aF2.115093231201172
aF2.107625274658203
aF2.104665985107422
aF2.111702728271484
aF2.0978712463378906
aF2.104744110107422
aF2.110944366455078
aF2.094621124267578
aF2.1105525207519533
aF2.0720509338378905
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004864438229431745
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 48s'
p10
sS'final_test_loss'
p11
F2.0720509338378905
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'Y\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.