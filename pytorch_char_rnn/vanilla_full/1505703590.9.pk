(dp0
S'train_loss'
p1
(lp2
F4.65083984375
aF4.597367248535156
aF4.5411770629882815
aF4.4857003784179685
aF4.421642761230469
aF4.357589416503906
aF4.28251708984375
aF4.205570068359375
aF4.118563232421875
aF4.023080749511719
aF3.924013671875
aF3.808473815917969
aF3.7489453125
aF3.67149169921875
aF3.582662353515625
aF3.5481756591796874
aF3.5127682495117187
aF3.45566162109375
aF3.4479745483398436
aF3.4035940551757813
aF3.367284851074219
aF3.3620184326171874
aF3.364926452636719
aF3.3302972412109373
aF3.3351370239257814
aF3.2780996704101564
aF3.27801025390625
aF3.256419982910156
aF3.234222717285156
aF3.22133056640625
aF3.180460205078125
aF3.1740902709960936
aF3.1411508178710936
aF3.127632751464844
aF3.083104553222656
aF3.0721841430664063
aF3.1039163208007814
aF3.0761734008789063
aF3.0432369995117186
aF3.037308654785156
aF3.0234417724609375
aF3.0198568725585937
aF3.0210028076171875
aF2.9639932250976564
aF2.9941232299804685
aF2.9434942626953124
aF2.9328628540039063
aF2.9327069091796876
aF2.9240643310546877
aF2.9221511840820313
aF2.8875888061523436
aF2.8838796997070313
aF2.8654669189453124
aF2.85701171875
aF2.858084411621094
aF2.844324645996094
aF2.8560369873046874
aF2.803389892578125
aF2.8282821655273436
aF2.7933221435546876
aF2.799368591308594
aF2.784709167480469
aF2.764464111328125
aF2.7684814453125
aF2.768155212402344
aF2.7209869384765626
aF2.7460687255859373
aF2.7222857666015625
aF2.709171142578125
aF2.702374267578125
aF2.7094375610351564
aF2.6668768310546875
aF2.6912896728515623
aF2.6598883056640625
aF2.67184326171875
aF2.6549835205078125
aF2.640348205566406
aF2.6341989135742185
aF2.6547528076171876
aF2.6143563842773436
aF2.609296875
aF2.607735900878906
aF2.5972946166992186
aF2.6030474853515626
aF2.5911160278320313
aF2.5952719116210936
aF2.56992431640625
aF2.567237548828125
aF2.5526580810546875
aF2.5486732482910157
aF2.5592958068847658
aF2.526933135986328
aF2.5356321716308594
aF2.542427673339844
aF2.5328155517578126
aF2.513419952392578
aF2.510126953125
aF2.5283758544921877
aF2.495516662597656
aF2.4772303771972655
asS'test_loss'
p3
(lp4
F4.595582885742187
aF4.5401303100585935
aF4.482034606933594
aF4.423227844238281
aF4.356746520996094
aF4.283360900878907
aF4.201864318847656
aF4.106387939453125
aF4.01763427734375
aF3.9210260009765623
aF3.8282595825195314
aF3.732066650390625
aF3.6686419677734374
aF3.5963494873046873
aF3.5518203735351563
aF3.4938980102539063
aF3.4627090454101563
aF3.4204788208007812
aF3.4083056640625
aF3.389603271484375
aF3.384379577636719
aF3.3271279907226563
aF3.314176330566406
aF3.2889886474609376
aF3.3003839111328124
aF3.243175048828125
aF3.222448425292969
aF3.215962219238281
aF3.2067828369140625
aF3.157724609375
aF3.1658056640625
aF3.1559686279296875
aF3.1167868041992186
aF3.102357177734375
aF3.0965435791015623
aF3.0879461669921877
aF3.0554498291015624
aF3.050806579589844
aF3.0217721557617185
aF3.0090713500976562
aF2.9966998291015625
aF2.99881591796875
aF3.000934753417969
aF2.9875521850585938
aF2.9418814086914065
aF2.9340060424804686
aF2.9380667114257815
aF2.9275173950195312
aF2.890037841796875
aF2.896636962890625
aF2.876204528808594
aF2.863283996582031
aF2.8587469482421874
aF2.8408758544921877
aF2.8194479370117187
aF2.82152587890625
aF2.7684521484375
aF2.79234130859375
aF2.784517517089844
aF2.798585205078125
aF2.795970458984375
aF2.792915954589844
aF2.7796511840820313
aF2.7490631103515626
aF2.7303005981445314
aF2.735290832519531
aF2.7062921142578125
aF2.699619140625
aF2.67913818359375
aF2.6914797973632814
aF2.68854248046875
aF2.6747161865234377
aF2.665777587890625
aF2.631980285644531
aF2.6561654663085936
aF2.625199279785156
aF2.6146875
aF2.622758483886719
aF2.621300048828125
aF2.6027862548828127
aF2.5981234741210937
aF2.613680725097656
aF2.6029083251953127
aF2.5707464599609375
aF2.5536029052734377
aF2.546599578857422
aF2.5741921997070314
aF2.564250793457031
aF2.5504893493652343
aF2.536640167236328
aF2.5334542846679686
aF2.5329486083984376
aF2.5187628173828127
aF2.503734588623047
aF2.5031370544433593
aF2.501393127441406
aF2.503325958251953
aF2.493730163574219
aF2.493894348144531
aF2.4911347961425783
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005367165721948351
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 26s'
p10
sS'final_test_loss'
p11
F2.4911347961425783
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x85\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.