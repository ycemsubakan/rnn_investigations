(dp0
S'train_loss'
p1
(lp2
F4.626183776855469
aF4.414100036621094
aF4.176444091796875
aF3.861541442871094
aF3.578790588378906
aF3.445204772949219
aF3.3473272705078125
aF3.2667385864257814
aF3.1949578857421876
aF3.1472238159179686
aF3.092911682128906
aF3.024951171875
aF2.996701965332031
aF2.9639382934570313
aF2.8978509521484375
aF2.9228396606445313
aF2.866752014160156
aF2.818475341796875
aF2.7946267700195313
aF2.7747918701171876
aF2.7504736328125
aF2.6774557495117186
aF2.683757629394531
aF2.6672833251953123
aF2.64416015625
aF2.616768493652344
aF2.5856317138671874
aF2.562081298828125
aF2.5513356018066404
aF2.496507263183594
aF2.5255671691894532
aF2.475630645751953
aF2.477452392578125
aF2.4444683837890624
aF2.450753631591797
aF2.4306300354003905
aF2.4091424560546875
aF2.3878533935546873
aF2.3843121337890625
aF2.3692477416992186
aF2.357605438232422
aF2.348390655517578
aF2.3321409606933594
aF2.3531338500976564
aF2.317104949951172
aF2.329464111328125
aF2.3032467651367186
aF2.3155648803710935
aF2.299624328613281
aF2.286177062988281
aF2.282217254638672
aF2.27486083984375
aF2.2572999572753907
aF2.251639099121094
aF2.246224670410156
aF2.248383331298828
aF2.2558830261230467
aF2.237995452880859
aF2.225416412353516
aF2.2152532958984374
aF2.230121765136719
aF2.2179644775390623
aF2.2209588623046876
aF2.1975666809082033
aF2.1910609436035156
aF2.1831787109375
aF2.1917430114746095
aF2.161209716796875
aF2.1666378784179687
aF2.1653948974609376
aF2.165938720703125
aF2.1351382446289064
aF2.13736572265625
aF2.130027618408203
aF2.154710693359375
aF2.122261962890625
aF2.1539772033691404
aF2.1295292663574217
aF2.114590759277344
aF2.13181884765625
aF2.1157156372070314
aF2.117470550537109
aF2.1152850341796876
aF2.0911210632324218
aF2.086639862060547
aF2.1032699584960937
aF2.076357727050781
aF2.093749542236328
aF2.0710820007324218
aF2.078017272949219
aF2.0844708251953126
aF2.05740966796875
aF2.0795751953125
aF2.064941711425781
aF2.064898681640625
aF2.06033203125
aF2.060308837890625
aF2.0640000915527343
aF2.046885528564453
aF2.0363333129882815
asS'test_loss'
p3
(lp4
F4.4171621704101565
aF4.172501831054688
aF3.864218444824219
aF3.566286926269531
aF3.3978805541992188
aF3.3186856079101563
aF3.252158203125
aF3.2122702026367187
aF3.134288024902344
aF3.1113775634765624
aF3.016885070800781
aF3.0091162109375
aF2.968661193847656
aF2.8944107055664063
aF2.9068112182617187
aF2.8363665771484374
aF2.822760009765625
aF2.7824630737304688
aF2.771566162109375
aF2.7297735595703125
aF2.690912780761719
aF2.695057067871094
aF2.6393865966796874
aF2.6295455932617187
aF2.599521484375
aF2.579739685058594
aF2.5458920288085936
aF2.5623388671875
aF2.5185865783691406
aF2.4995875549316406
aF2.4894053649902346
aF2.4572901916503906
aF2.445760955810547
aF2.431401672363281
aF2.411990966796875
aF2.4082249450683593
aF2.4182652282714843
aF2.3884344482421875
aF2.3829747009277344
aF2.364832763671875
aF2.363053894042969
aF2.346890869140625
aF2.339520263671875
aF2.3262445068359376
aF2.3168399047851564
aF2.3266900634765624
aF2.2904132080078123
aF2.3018333435058596
aF2.2713800048828126
aF2.2781036376953123
aF2.2689642333984374
aF2.255110168457031
aF2.252449188232422
aF2.2508241271972658
aF2.247141876220703
aF2.239285430908203
aF2.2228118896484377
aF2.246151123046875
aF2.216229248046875
aF2.2193505859375
aF2.206637420654297
aF2.2073602294921875
aF2.201744384765625
aF2.1970115661621095
aF2.1825355529785155
aF2.1859803771972657
aF2.17118896484375
aF2.1688887023925782
aF2.164622344970703
aF2.1543328857421873
aF2.160117492675781
aF2.1405790710449217
aF2.1634861755371095
aF2.1600669860839843
aF2.138590087890625
aF2.119179382324219
aF2.122078399658203
aF2.1240412902832033
aF2.1125035095214844
aF2.1019842529296877
aF2.101239776611328
aF2.0910494995117186
aF2.0925096130371093
aF2.0918296813964843
aF2.0848529052734377
aF2.0852320861816405
aF2.084411315917969
aF2.084470672607422
aF2.070043029785156
aF2.077369842529297
aF2.0760528564453127
aF2.0639352416992187
aF2.056416320800781
aF2.0376194763183593
aF2.0516535949707033
aF2.0610447692871094
aF2.058371124267578
aF2.054390411376953
aF2.0393931579589846
aF2.031077575683594
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0025329741703765773
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 24s'
p10
sS'final_test_loss'
p11
F2.031077575683594
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'{\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.