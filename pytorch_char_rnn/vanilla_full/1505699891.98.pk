(dp0
S'train_loss'
p1
(lp2
F4.659508972167969
aF3.9377667236328127
aF3.2098489379882813
aF3.090509948730469
aF2.9167193603515624
aF2.7953338623046875
aF2.7199417114257813
aF2.6816583251953126
aF2.629951171875
aF2.5798440551757813
aF2.5279193115234375
aF2.490411834716797
aF2.4758966064453123
aF2.4365821838378907
aF2.412938690185547
aF2.3707957458496094
aF2.388932647705078
aF2.3486448669433595
aF2.3238063049316406
aF2.3249761962890627
aF2.28951416015625
aF2.2820533752441405
aF2.26710205078125
aF2.255482482910156
aF2.2438352966308592
aF2.227984619140625
aF2.2252195739746092
aF2.202777557373047
aF2.204967041015625
aF2.1900784301757814
aF2.2049928283691407
aF2.1568064880371094
aF2.161421813964844
aF2.1600657653808595
aF2.1266743469238283
aF2.138766632080078
aF2.1244793701171876
aF2.123407897949219
aF2.1051559448242188
aF2.095819549560547
aF2.0830673217773437
aF2.0997274780273436
aF2.0878448486328125
aF2.070818328857422
aF2.0860856628417968
aF2.063614959716797
aF2.0445606994628904
aF2.026189422607422
aF2.052659149169922
aF2.042480010986328
aF2.043870544433594
aF2.007901153564453
aF2.020704345703125
aF2.014452362060547
aF2.00301513671875
aF1.9874053955078126
aF1.9943992614746093
aF1.9929792785644531
aF2.0123741149902346
aF1.9642198181152344
aF1.98083740234375
aF1.962336883544922
aF1.9588023376464845
aF1.9633969116210936
aF1.9730699157714844
aF1.95571533203125
aF1.9776603698730468
aF1.9310467529296875
aF1.949497528076172
aF1.9404281616210937
aF1.952621307373047
aF1.9233015441894532
aF1.909197998046875
aF1.9046340942382813
aF1.9561054992675782
aF1.9150851440429688
aF1.9135997009277343
aF1.90627685546875
aF1.9039756774902343
aF1.89351806640625
aF1.9146142578125
aF1.8969914245605468
aF1.881057891845703
aF1.8899806213378907
aF1.918035125732422
aF1.8709892272949218
aF1.8747830200195312
aF1.8876809692382812
aF1.8923605346679688
aF1.8730894470214843
aF1.8738923645019532
aF1.8713706970214843
aF1.8832064819335939
aF1.8749977111816407
aF1.8617866516113282
aF1.8629354858398437
aF1.8860398864746093
aF1.8592503356933594
aF1.8531002807617187
aF1.8567387390136718
asS'test_loss'
p3
(lp4
F3.9316766357421873
aF3.2183523559570313
aF3.0862655639648438
aF2.876805725097656
aF2.758652648925781
aF2.707852478027344
aF2.6533380126953126
aF2.599776611328125
aF2.576424255371094
aF2.5292343139648437
aF2.49467041015625
aF2.4665728759765626
aF2.4458856201171875
aF2.409581604003906
aF2.371604309082031
aF2.367513427734375
aF2.3226029968261717
aF2.3267085266113283
aF2.315381622314453
aF2.2864263916015624
aF2.2670428466796877
aF2.2648057556152343
aF2.260134429931641
aF2.2282089233398437
aF2.2297439575195312
aF2.2154615783691405
aF2.2168312072753906
aF2.187685241699219
aF2.1815943908691406
aF2.1798031616210936
aF2.1635258483886717
aF2.1590342712402344
aF2.137900543212891
aF2.1390667724609376
aF2.14988037109375
aF2.131148681640625
aF2.127640686035156
aF2.0865823364257814
aF2.108042449951172
aF2.0845994567871093
aF2.088981475830078
aF2.0789256286621094
aF2.0705731201171873
aF2.0490675354003907
aF2.0518428039550782
aF2.055643768310547
aF2.022555236816406
aF2.025867156982422
aF2.0326356506347656
aF2.0522695922851564
aF2.007259521484375
aF2.01687255859375
aF2.041784210205078
aF2.0116313171386717
aF2.0128907775878906
aF1.9816282653808595
aF1.9743801879882812
aF1.9759919738769531
aF1.981370849609375
aF1.9880949401855468
aF1.9748304748535157
aF1.9791416931152344
aF1.9673394775390625
aF1.9467227172851562
aF1.9738284301757814
aF1.9558305358886718
aF1.949945068359375
aF1.962000274658203
aF1.9398828125
aF1.9476005554199218
aF1.9205978393554688
aF1.9480657958984375
aF1.9220878601074218
aF1.9288043212890624
aF1.9407078552246093
aF1.9003811645507813
aF1.907781982421875
aF1.921481170654297
aF1.9173149108886718
aF1.931865234375
aF1.904961395263672
aF1.8805210876464844
aF1.8951731872558595
aF1.880956573486328
aF1.8652651977539063
aF1.9307354736328124
aF1.9117855834960937
aF1.8753251647949218
aF1.9157026672363282
aF1.884617919921875
aF1.865162353515625
aF1.8703541564941406
aF1.8533395385742188
aF1.8727534484863282
aF1.8896914672851564
aF1.8693093872070312
aF1.837197265625
aF1.8670709228515625
aF1.85602783203125
aF1.869576416015625
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.006507059484167906
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 2s'
p10
sS'final_test_loss'
p11
F1.869576416015625
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xae\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.