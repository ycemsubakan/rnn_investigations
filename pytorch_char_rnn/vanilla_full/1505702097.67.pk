(dp0
S'train_loss'
p1
(lp2
F4.680040893554687
aF4.534645690917968
aF4.3896856689453125
aF4.23204833984375
aF4.055703735351562
aF3.8794482421875
aF3.670546569824219
aF3.4874746704101565
aF3.359804992675781
aF3.264998779296875
aF3.19587646484375
aF3.1666793823242188
aF3.112469482421875
aF3.0525299072265626
aF3.000102844238281
aF2.960360412597656
aF2.914764404296875
aF2.9151010131835937
aF2.8975296020507812
aF2.855555419921875
aF2.85037109375
aF2.8111862182617187
aF2.7701260375976564
aF2.7379962158203126
aF2.7501654052734374
aF2.7288653564453127
aF2.6994122314453124
aF2.666315612792969
aF2.6726910400390627
aF2.6342462158203124
aF2.6392440795898438
aF2.6472100830078125
aF2.620802307128906
aF2.5947079467773437
aF2.579124755859375
aF2.5740032958984376
aF2.558913269042969
aF2.5649774169921873
aF2.545391387939453
aF2.5301614379882813
aF2.5275299072265627
aF2.5308135986328124
aF2.5030580139160157
aF2.4991358947753906
aF2.5115167236328126
aF2.4761174011230467
aF2.473033905029297
aF2.462584686279297
aF2.441048431396484
aF2.4602700805664064
aF2.4244705200195313
aF2.4196922302246096
aF2.4424560546875
aF2.4072611999511717
aF2.402771453857422
aF2.410410614013672
aF2.3907093811035156
aF2.3904122924804687
aF2.3788462829589845
aF2.3811239624023437
aF2.361764678955078
aF2.364532775878906
aF2.343094940185547
aF2.335807189941406
aF2.3440733337402344
aF2.3521968078613282
aF2.3593836975097657
aF2.3523965454101563
aF2.32634765625
aF2.3175091552734375
aF2.2959584045410155
aF2.330319366455078
aF2.318673858642578
aF2.313906555175781
aF2.287932586669922
aF2.2914646911621093
aF2.2909614562988283
aF2.2910919189453125
aF2.296949768066406
aF2.279213562011719
aF2.2823806762695313
aF2.2607688903808594
aF2.2733056640625
aF2.2790293884277344
aF2.2493319702148438
aF2.269666748046875
aF2.2538128662109376
aF2.2442564392089843
aF2.251561737060547
aF2.2264309692382813
aF2.2139599609375
aF2.22649169921875
aF2.221040802001953
aF2.2224627685546876
aF2.2045333862304686
aF2.2060113525390626
aF2.2193638610839845
aF2.192454376220703
aF2.2328993225097657
aF2.2200982666015623
asS'test_loss'
p3
(lp4
F4.529747009277344
aF4.383234252929688
aF4.230838317871093
aF4.058416748046875
aF3.8690936279296877
aF3.649872131347656
aF3.4742861938476564
aF3.3494549560546876
aF3.2474664306640624
aF3.1838259887695313
aF3.1724179077148436
aF3.0953411865234375
aF3.060565185546875
aF3.0059548950195314
aF2.94289306640625
aF2.9353143310546876
aF2.8754998779296876
aF2.877970886230469
aF2.853543701171875
aF2.8127822875976562
aF2.8106430053710936
aF2.7604135131835936
aF2.7566793823242186
aF2.7245086669921874
aF2.716256408691406
aF2.699137878417969
aF2.6705172729492186
aF2.6517129516601563
aF2.6414053344726565
aF2.6307669067382813
aF2.6287030029296874
aF2.5912155151367187
aF2.589622802734375
aF2.6032330322265627
aF2.5736007690429688
aF2.549655303955078
aF2.522574157714844
aF2.5315670776367187
aF2.5413697814941405
aF2.534748077392578
aF2.4883163452148436
aF2.502642822265625
aF2.4855755615234374
aF2.4631370544433593
aF2.4690321350097655
aF2.4699627685546877
aF2.439009552001953
aF2.4561557006835937
aF2.431898193359375
aF2.441483459472656
aF2.4129037475585937
aF2.4150531005859377
aF2.4040826416015624
aF2.4003514099121093
aF2.395600128173828
aF2.375424652099609
aF2.374747772216797
aF2.3501678466796876
aF2.3577365112304687
aF2.3703614807128908
aF2.3752377319335936
aF2.342618255615234
aF2.34036376953125
aF2.3320651245117188
aF2.3470884704589845
aF2.328160858154297
aF2.3340869140625
aF2.330035400390625
aF2.3192396545410157
aF2.299791717529297
aF2.287771759033203
aF2.2970877075195313
aF2.2825379943847657
aF2.287894287109375
aF2.2880490112304686
aF2.2602134704589845
aF2.283116912841797
aF2.2573789978027343
aF2.265650329589844
aF2.2758920288085935
aF2.272976531982422
aF2.2315104675292967
aF2.274028472900391
aF2.2555612182617186
aF2.2300399780273437
aF2.2329667663574218
aF2.2365415954589842
aF2.2462446594238283
aF2.228970794677734
aF2.2441099548339842
aF2.215129089355469
aF2.216874847412109
aF2.224354705810547
aF2.199795837402344
aF2.213644714355469
aF2.2179104614257814
aF2.2042095947265623
aF2.2017756652832032
aF2.213785858154297
aF2.1827627563476564
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0015316744366947804
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 58s'
p10
sS'final_test_loss'
p11
F2.1827627563476564
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x90\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.