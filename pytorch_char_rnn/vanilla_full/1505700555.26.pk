(dp0
S'train_loss'
p1
(lp2
F4.608719787597656
aF4.530512084960938
aF4.459803161621093
aF4.376124877929687
aF4.280335693359375
aF4.170728759765625
aF4.053258666992187
aF3.9296490478515627
aF3.7951239013671874
aF3.65115966796875
aF3.547685241699219
aF3.487021179199219
aF3.422221984863281
aF3.355546875
aF3.3577511596679686
aF3.314763488769531
aF3.2819247436523438
aF3.223396911621094
aF3.2141497802734373
aF3.2200436401367187
aF3.17861572265625
aF3.113848876953125
aF3.112210998535156
aF3.071492004394531
aF3.0564703369140624
aF3.0330780029296873
aF3.0257098388671877
aF2.967626647949219
aF2.9914309692382814
aF2.952950439453125
aF2.950359191894531
aF2.911292724609375
aF2.8700408935546875
aF2.88755859375
aF2.861916809082031
aF2.8600140380859376
aF2.8516876220703127
aF2.824127197265625
aF2.80258544921875
aF2.8009988403320314
aF2.7655560302734377
aF2.751032409667969
aF2.733431396484375
aF2.723011474609375
aF2.705296630859375
aF2.7101211547851562
aF2.6799044799804688
aF2.676391906738281
aF2.6648297119140625
aF2.6400820922851564
aF2.6360440063476562
aF2.6236065673828124
aF2.59762451171875
aF2.601206970214844
aF2.5920391845703126
aF2.5891351318359375
aF2.5707992553710937
aF2.5784100341796874
aF2.552248687744141
aF2.5664788818359376
aF2.5473114013671876
aF2.5283349609375
aF2.5265386962890624
aF2.5161195373535157
aF2.498179168701172
aF2.49775146484375
aF2.475106048583984
aF2.496964416503906
aF2.4897311401367186
aF2.4758738708496093
aF2.469377288818359
aF2.4762228393554686
aF2.4489381408691404
aF2.4587269592285157
aF2.4315956115722654
aF2.4697576904296876
aF2.4193409729003905
aF2.4256106567382814
aF2.4081179809570314
aF2.4103466796875
aF2.375907745361328
aF2.4032766723632815
aF2.386022491455078
aF2.404874114990234
aF2.370809326171875
aF2.3750296020507813
aF2.3679904174804687
aF2.3450886535644533
aF2.351334228515625
aF2.3704240417480467
aF2.3445057678222656
aF2.3563662719726564
aF2.348389892578125
aF2.3449191284179687
aF2.33550537109375
aF2.3261587524414065
aF2.3283401489257813
aF2.324556579589844
aF2.326773681640625
aF2.3245440673828126
asS'test_loss'
p3
(lp4
F4.532863159179687
aF4.449815063476563
aF4.371130065917969
aF4.2746408081054685
aF4.1720974731445315
aF4.05228759765625
aF3.9236712646484375
aF3.779327392578125
aF3.6615631103515627
aF3.5818753051757812
aF3.45992431640625
aF3.419971618652344
aF3.3343106079101563
aF3.314757995605469
aF3.3048516845703126
aF3.2916464233398437
aF3.2382086181640624
aF3.2321957397460936
aF3.2000723266601563
aF3.1513912963867186
aF3.120033874511719
aF3.1042266845703126
aF3.09679443359375
aF3.0463104248046875
aF3.0219601440429686
aF2.99510986328125
aF2.989158020019531
aF2.969837341308594
aF2.943985595703125
aF2.9461065673828126
aF2.9251531982421874
aF2.9067913818359377
aF2.865212707519531
aF2.858082580566406
aF2.833260192871094
aF2.8264825439453123
aF2.8055889892578123
aF2.824295959472656
aF2.7949649047851564
aF2.759393310546875
aF2.7627902221679688
aF2.732191467285156
aF2.740634765625
aF2.71886962890625
aF2.719757385253906
aF2.6629519653320313
aF2.6620599365234376
aF2.675755615234375
aF2.661642761230469
aF2.6160723876953127
aF2.6125277709960937
aF2.6135662841796874
aF2.61085205078125
aF2.58194091796875
aF2.583266296386719
aF2.561970520019531
aF2.549895935058594
aF2.5597462463378906
aF2.537293243408203
aF2.5413299560546876
aF2.5037994384765625
aF2.515364685058594
aF2.510915985107422
aF2.5141397094726563
aF2.4976409912109374
aF2.483876190185547
aF2.481010284423828
aF2.4773320007324218
aF2.475028381347656
aF2.43764404296875
aF2.4464451599121095
aF2.4508399963378906
aF2.4263259887695314
aF2.4450227355957033
aF2.4417999267578123
aF2.424145050048828
aF2.4161312866210936
aF2.3974853515625
aF2.40041015625
aF2.3968707275390626
aF2.394667510986328
aF2.382039031982422
aF2.392799072265625
aF2.3727801513671873
aF2.3596226501464845
aF2.3564125061035157
aF2.3711407470703123
aF2.3577955627441405
aF2.3515870666503904
aF2.3440536499023437
aF2.356131744384766
aF2.342690734863281
aF2.333588104248047
aF2.3299993896484374
aF2.3436708068847656
aF2.331062774658203
aF2.337136688232422
aF2.324919891357422
aF2.302776031494141
aF2.321089630126953
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.000562317567685183
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 46s'
p10
sS'final_test_loss'
p11
F2.321089630126953
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb4\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.