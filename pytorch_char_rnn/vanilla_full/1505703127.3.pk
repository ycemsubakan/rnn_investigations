(dp0
S'train_loss'
p1
(lp2
F4.6856103515625
aF4.657320251464844
aF4.626588439941406
aF4.603697814941406
aF4.568656005859375
aF4.547843017578125
aF4.523039245605469
aF4.491473083496094
aF4.46671875
aF4.432723388671875
aF4.413836059570312
aF4.381818237304688
aF4.35116455078125
aF4.321228637695312
aF4.280399780273438
aF4.254234313964844
aF4.219488525390625
aF4.172245788574219
aF4.141993713378906
aF4.085735168457031
aF4.0564877319335935
aF4.0095849609375
aF3.9721951293945312
aF3.92052978515625
aF3.869603576660156
aF3.8151495361328127
aF3.7689144897460936
aF3.712508239746094
aF3.6822552490234375
aF3.6147476196289063
aF3.576158447265625
aF3.530728759765625
aF3.4939541625976562
aF3.450689392089844
aF3.4124908447265625
aF3.390472717285156
aF3.3901641845703123
aF3.34008056640625
aF3.3166159057617186
aF3.300738525390625
aF3.2932229614257813
aF3.2820977783203125
aF3.2547900390625
aF3.23443603515625
aF3.197396240234375
aF3.2338922119140623
aF3.1917950439453127
aF3.2067855834960937
aF3.160928039550781
aF3.154150085449219
aF3.14489501953125
aF3.1320654296875
aF3.124276123046875
aF3.0667205810546876
aF3.0859576416015626
aF3.071806640625
aF3.043211669921875
aF3.0625784301757815
aF3.0414169311523436
aF3.048614196777344
aF3.030318603515625
aF3.015311279296875
aF3.031946105957031
aF3.018438720703125
aF2.9858694458007813
aF2.964244384765625
aF3.015352478027344
aF2.9976156616210936
aF2.9849124145507813
aF2.934032287597656
aF2.9509832763671877
aF2.9629397583007813
aF2.9507882690429685
aF2.9339950561523436
aF2.8996771240234374
aF2.9290252685546876
aF2.902244873046875
aF2.9070590209960936
aF2.8836801147460935
aF2.887726135253906
aF2.883627624511719
aF2.8592486572265625
aF2.8889950561523436
aF2.8797207641601563
aF2.857244567871094
aF2.845125732421875
aF2.84962646484375
aF2.846202392578125
aF2.857466735839844
aF2.8179364013671875
aF2.828694152832031
aF2.809464111328125
aF2.79687255859375
aF2.824082336425781
aF2.8131964111328127
aF2.79810546875
aF2.780886535644531
aF2.7768545532226563
aF2.763008728027344
aF2.778028564453125
asS'test_loss'
p3
(lp4
F4.651731872558594
aF4.631102600097656
aF4.601769104003906
aF4.570756530761718
aF4.545977478027344
aF4.520947570800781
aF4.494832458496094
aF4.467360229492187
aF4.433087158203125
aF4.401871948242188
aF4.373685913085938
aF4.344458923339844
aF4.312597351074219
aF4.2797415161132815
aF4.242003784179688
aF4.215035095214843
aF4.1713671875
aF4.136141357421875
aF4.091219787597656
aF4.053425903320313
aF4.000252380371093
aF3.960335998535156
aF3.9117172241210936
aF3.871851501464844
aF3.8056964111328124
aF3.7667251586914063
aF3.713818359375
aF3.6660980224609374
aF3.610495910644531
aF3.5768988037109377
aF3.5191119384765623
aF3.4884384155273436
aF3.44388916015625
aF3.405584411621094
aF3.3754537963867186
aF3.35192138671875
aF3.3417877197265624
aF3.330063781738281
aF3.2719219970703124
aF3.2684246826171877
aF3.261998291015625
aF3.25524658203125
aF3.208579406738281
aF3.207315673828125
aF3.207641296386719
aF3.207397155761719
aF3.1720343017578125
aF3.1763665771484373
aF3.130735168457031
aF3.137839660644531
aF3.128653564453125
aF3.1010479736328125
aF3.0985928344726563
aF3.0836248779296875
aF3.0513204956054687
aF3.041981201171875
aF3.0715850830078124
aF3.0469976806640626
aF3.046622314453125
aF3.033819580078125
aF3.006103210449219
aF3.0042706298828126
aF2.9912103271484374
aF2.9851504516601564
aF2.983891906738281
aF2.9729135131835935
aF2.974997863769531
aF2.9578460693359374
aF2.961122741699219
aF2.951202392578125
aF2.937396240234375
aF2.9177081298828127
aF2.907581787109375
aF2.90827880859375
aF2.905192565917969
aF2.890566101074219
aF2.8925045776367186
aF2.8914471435546876
aF2.883375549316406
aF2.858895568847656
aF2.8591708374023437
aF2.8846728515625
aF2.8838540649414064
aF2.8449227905273435
aF2.8444174194335936
aF2.825663757324219
aF2.818988037109375
aF2.8407061767578123
aF2.8111666870117187
aF2.8069699096679686
aF2.81623779296875
aF2.8006820678710938
aF2.8088638305664064
aF2.7808538818359376
aF2.805761413574219
aF2.7741998291015624
aF2.770033264160156
aF2.7741799926757813
aF2.749339904785156
aF2.750124206542969
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0003551109921987689
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 52s'
p10
sS'final_test_loss'
p11
F2.750124206542969
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'z\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.