(dp0
S'train_loss'
p1
(lp2
F4.634412231445313
aF4.577297973632812
aF4.518989868164063
aF4.461480407714844
aF4.409892883300781
aF4.349152221679687
aF4.298833312988282
aF4.234135131835938
aF4.1744381713867185
aF4.10667236328125
aF4.027403564453125
aF3.9581863403320314
aF3.889637145996094
aF3.801829833984375
aF3.722952575683594
aF3.645809326171875
aF3.559803771972656
aF3.47507080078125
aF3.4267388916015626
aF3.3776104736328123
aF3.323050231933594
aF3.2695367431640623
aF3.233617858886719
aF3.2266506958007812
aF3.2042776489257814
aF3.177481689453125
aF3.1590066528320313
aF3.1273684692382813
aF3.1252212524414062
aF3.0944454956054686
aF3.0682003784179686
aF3.039356994628906
aF3.024315185546875
aF3.010221252441406
aF2.9923297119140626
aF2.960555114746094
aF2.9286648559570314
aF2.9431817626953123
aF2.935689697265625
aF2.9182427978515624
aF2.900438232421875
aF2.90015869140625
aF2.860225830078125
aF2.866593017578125
aF2.8567779541015623
aF2.8252288818359377
aF2.8295553588867186
aF2.816620178222656
aF2.8086477661132814
aF2.825722961425781
aF2.775226745605469
aF2.7969146728515626
aF2.7928121948242186
aF2.785879821777344
aF2.759559326171875
aF2.755697937011719
aF2.7336599731445315
aF2.716338806152344
aF2.7025277709960935
aF2.7273602294921875
aF2.714486083984375
aF2.6847686767578125
aF2.69741455078125
aF2.688138732910156
aF2.6731130981445315
aF2.6806607055664062
aF2.6781301879882813
aF2.6520681762695313
aF2.63769287109375
aF2.64952880859375
aF2.640736083984375
aF2.6442742919921876
aF2.646588134765625
aF2.6313458251953126
aF2.618907165527344
aF2.5975604248046875
aF2.6093655395507813
aF2.617681579589844
aF2.6050579833984373
aF2.590748596191406
aF2.60382568359375
aF2.5787997436523438
aF2.5565226745605467
aF2.5590283203125
aF2.5373521423339844
aF2.558178253173828
aF2.579717712402344
aF2.5412393188476563
aF2.5477012634277343
aF2.5335322570800782
aF2.5286679077148437
aF2.5131234741210937
aF2.5334794616699217
aF2.4911459350585936
aF2.506441955566406
aF2.5185615539550783
aF2.5075653076171873
aF2.5110833740234373
aF2.476840362548828
aF2.4924876403808596
asS'test_loss'
p3
(lp4
F4.57607421875
aF4.524613647460938
aF4.463480834960937
aF4.404008178710938
aF4.3569332885742185
aF4.283362121582031
aF4.2313525390625
aF4.1656015014648435
aF4.097370300292969
aF4.043397827148437
aF3.9607647705078124
aF3.882908935546875
aF3.8055184936523436
aF3.7188351440429686
aF3.6250518798828124
aF3.567110900878906
aF3.484361267089844
aF3.4208355712890626
aF3.3666500854492187
aF3.30129150390625
aF3.3023471069335937
aF3.2388616943359376
aF3.2259896850585936
aF3.1931466674804687
aF3.1492355346679686
aF3.12942138671875
aF3.1363226318359376
aF3.10108154296875
aF3.0790484619140623
aF3.0524893188476563
aF3.053431091308594
aF3.00192626953125
aF2.983681945800781
aF2.9836715698242187
aF2.9319168090820313
aF2.9645941162109377
aF2.959010009765625
aF2.919093017578125
aF2.9243988037109374
aF2.8924734497070315
aF2.8953036499023437
aF2.8707919311523438
aF2.8544659423828125
aF2.8644281005859376
aF2.8374188232421873
aF2.8247918701171875
aF2.830675048828125
aF2.8126422119140626
aF2.770736083984375
aF2.786997985839844
aF2.7729782104492187
aF2.755834045410156
aF2.76796630859375
aF2.7439947509765625
aF2.748612365722656
aF2.707189025878906
aF2.7249847412109376
aF2.715740051269531
aF2.7272384643554686
aF2.7030020141601563
aF2.6871658325195313
aF2.6937600708007814
aF2.6926263427734374
aF2.6782772827148436
aF2.6637722778320314
aF2.6701058959960937
aF2.6665267944335938
aF2.644688720703125
aF2.6359173583984377
aF2.6384619140625
aF2.6363592529296875
aF2.6288238525390626
aF2.6125244140625
aF2.616365661621094
aF2.62385009765625
aF2.6016845703125
aF2.5854397583007813
aF2.594917907714844
aF2.589169921875
aF2.5624765014648436
aF2.5613568115234373
aF2.57036376953125
aF2.5554653930664064
aF2.5372303771972655
aF2.541505889892578
aF2.548780670166016
aF2.538462829589844
aF2.5372552490234375
aF2.5472410583496092
aF2.5108238220214845
aF2.5140261840820313
aF2.5088525390625
aF2.5367886352539064
aF2.503387451171875
aF2.5048748779296877
aF2.5237240600585937
aF2.4967724609375
aF2.478626251220703
aF2.488564300537109
aF2.4601824951171873
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0004335823729733922
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 4s'
p10
sS'final_test_loss'
p11
F2.4601824951171873
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xb5\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.