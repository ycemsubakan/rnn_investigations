(dp0
S'train_loss'
p1
(lp2
F4.658568115234375
aF4.083668823242188
aF3.4256723022460935
aF3.147542724609375
aF2.9733877563476563
aF2.881730041503906
aF2.819122009277344
aF2.7182040405273438
aF2.6788824462890624
aF2.5942367553710937
aF2.57918212890625
aF2.574180908203125
aF2.530856018066406
aF2.486061248779297
aF2.4665089416503907
aF2.4369500732421874
aF2.428703155517578
aF2.4109510803222656
aF2.3770657348632813
aF2.345146026611328
aF2.3323748779296873
aF2.336645050048828
aF2.3093412780761717
aF2.2916705322265627
aF2.299308166503906
aF2.2633705139160156
aF2.268152313232422
aF2.249555206298828
aF2.2501954650878906
aF2.2264675903320312
aF2.208023681640625
aF2.215060119628906
aF2.221302185058594
aF2.179526519775391
aF2.1901918029785157
aF2.1714280700683593
aF2.1786233520507814
aF2.1555606079101564
aF2.1304103088378907
aF2.1354225158691404
aF2.1321417236328126
aF2.1502415466308595
aF2.1120664978027346
aF2.1068074035644533
aF2.110703430175781
aF2.092005615234375
aF2.0976901245117188
aF2.0896435546875
aF2.0843690490722655
aF2.0734385681152343
aF2.083284606933594
aF2.0618525695800782
aF2.0598905944824217
aF2.067239227294922
aF2.0397549438476563
aF2.04244384765625
aF2.0197137451171874
aF2.039920196533203
aF2.033643798828125
aF2.0339761352539063
aF2.0252947998046875
aF2.013685455322266
aF2.031349334716797
aF1.991968231201172
aF1.9889942932128906
aF1.9937977600097656
aF2.002582244873047
aF1.997947998046875
aF1.9714198303222656
aF1.9762002563476562
aF1.9582186889648439
aF1.9868589782714843
aF1.956516876220703
aF1.9741876220703125
aF1.9514476013183595
aF1.9618185424804688
aF1.9513017272949218
aF1.9635693359375
aF1.9445867919921875
aF1.9541648864746093
aF1.940543212890625
aF1.9454818725585938
aF1.9269793701171876
aF1.9453732299804687
aF1.9108966064453126
aF1.91602783203125
aF1.9112075805664062
aF1.9113075256347656
aF1.904119415283203
aF1.9035728454589844
aF1.8933262634277344
aF1.904736328125
aF1.89004638671875
aF1.900039825439453
aF1.8826176452636718
aF1.8871577453613282
aF1.8666839599609375
aF1.8727813720703126
aF1.88303955078125
aF1.855194091796875
asS'test_loss'
p3
(lp4
F4.081965026855468
aF3.4226895141601563
aF3.112152099609375
aF2.951227111816406
aF2.882763671875
aF2.7906610107421876
aF2.7383914184570313
aF2.6742959594726563
aF2.625782470703125
aF2.5848312377929688
aF2.557168426513672
aF2.5149229431152342
aF2.482886962890625
aF2.4194786071777346
aF2.432833709716797
aF2.4223147583007814
aF2.4030856323242187
aF2.3663484191894533
aF2.336534423828125
aF2.323908386230469
aF2.3188491821289063
aF2.3081199645996096
aF2.288890838623047
aF2.285381622314453
aF2.271690673828125
aF2.262717437744141
aF2.2559686279296876
aF2.2470608520507813
aF2.2085992431640626
aF2.2025408935546875
aF2.205813140869141
aF2.2032476806640626
aF2.172996368408203
aF2.173564453125
aF2.162866516113281
aF2.1659063720703124
aF2.158294677734375
aF2.133559265136719
aF2.132118377685547
aF2.134946746826172
aF2.135784912109375
aF2.112257232666016
aF2.098449401855469
aF2.0922344970703124
aF2.082741241455078
aF2.084937744140625
aF2.0836671447753905
aF2.0536042785644533
aF2.083678741455078
aF2.0513070678710936
aF2.02825927734375
aF2.055478973388672
aF2.044206390380859
aF2.0390826416015626
aF2.0548968505859375
aF2.033395538330078
aF2.031464080810547
aF2.0246417236328127
aF2.0201145935058595
aF1.9974098205566406
aF2.0090757751464845
aF1.9857180786132813
aF2.0016949462890623
aF1.9901727294921876
aF1.9941378784179689
aF1.9605120849609374
aF1.9896420288085936
aF1.9621153259277344
aF1.9841935729980469
aF1.959229736328125
aF1.9672311401367188
aF1.949081268310547
aF1.9453094482421875
aF1.9541192626953126
aF1.9463829040527343
aF1.9574954223632812
aF1.9316786193847657
aF1.9452496337890626
aF1.9266880798339843
aF1.9224526977539063
aF1.9170417785644531
aF1.9292085266113281
aF1.9227728271484374
aF1.9437138366699218
aF1.9135888671875
aF1.9110832214355469
aF1.9256370544433594
aF1.91552001953125
aF1.8794435119628907
aF1.9094140625
aF1.919361572265625
aF1.895225830078125
aF1.8595903015136719
aF1.8866221618652343
aF1.9025076293945313
aF1.8901446533203126
aF1.8754057312011718
aF1.9005436706542969
aF1.9058567810058593
aF1.8573674011230468
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.006115760280337606
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 59s'
p10
sS'final_test_loss'
p11
F1.8573674011230468
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x9f\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.