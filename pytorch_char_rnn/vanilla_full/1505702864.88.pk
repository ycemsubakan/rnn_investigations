(dp0
S'train_loss'
p1
(lp2
F4.597354431152343
aF4.591460266113281
aF4.576692199707031
aF4.568459167480468
aF4.563466491699219
aF4.553088073730469
aF4.543319396972656
aF4.533445434570313
aF4.520796813964844
aF4.512434692382812
aF4.503142700195313
aF4.496370239257812
aF4.4816015625
aF4.468818664550781
aF4.461875610351562
aF4.451082153320312
aF4.437709350585937
aF4.425520629882812
aF4.418470764160157
aF4.404195556640625
aF4.388215026855469
aF4.384302673339843
aF4.3631588745117185
aF4.356701354980469
aF4.337136840820312
aF4.3255441284179685
aF4.311549377441406
aF4.297836608886719
aF4.28676025390625
aF4.266620178222656
aF4.261187744140625
aF4.228635559082031
aF4.209949035644531
aF4.208192443847656
aF4.187803344726563
aF4.158829040527344
aF4.1465234375
aF4.131756286621094
aF4.110713500976562
aF4.096101379394531
aF4.076454162597656
aF4.045321655273438
aF4.030536499023437
aF4.014299926757812
aF3.9816339111328123
aF3.95083984375
aF3.9460043334960937
aF3.9194189453125
aF3.9051364135742186
aF3.8561932373046877
aF3.8367208862304687
aF3.846010437011719
aF3.79859130859375
aF3.770543518066406
aF3.75615478515625
aF3.7368539428710936
aF3.72437744140625
aF3.70067626953125
aF3.6607794189453124
aF3.671719970703125
aF3.6412142944335937
aF3.6239218139648437
aF3.6131387329101563
aF3.5844329833984374
aF3.5788067626953124
aF3.5677005004882814
aF3.5602850341796874
aF3.5382730102539064
aF3.5320260620117185
aF3.5088925170898437
aF3.4957290649414063
aF3.4903079223632814
aF3.484480895996094
aF3.4631143188476563
aF3.430079345703125
aF3.4268679809570313
aF3.4430056762695314
aF3.4075717163085937
aF3.3981423950195313
aF3.4001708984375
aF3.3920187377929687
aF3.3993133544921874
aF3.394422912597656
aF3.3837255859375
aF3.356649169921875
aF3.3516357421875
aF3.3690594482421874
aF3.354292907714844
aF3.3683743286132812
aF3.3320217895507813
aF3.333822021484375
aF3.3353366088867187
aF3.314777526855469
aF3.332591247558594
aF3.3037744140625
aF3.324085693359375
aF3.3124273681640624
aF3.3219345092773436
aF3.2851187133789064
aF3.2924578857421873
asS'test_loss'
p3
(lp4
F4.591859741210937
aF4.583279418945312
aF4.569245910644531
aF4.560496215820312
aF4.553872985839844
aF4.5457461547851565
aF4.5300814819335935
aF4.519541015625
aF4.510578002929687
aF4.5021240234375
aF4.495543823242188
aF4.481249694824219
aF4.470090026855469
aF4.4627597045898435
aF4.453297424316406
aF4.439306030273437
aF4.425076904296875
aF4.416769104003906
aF4.40840576171875
aF4.390410766601563
aF4.388523864746094
aF4.366062927246094
aF4.352900390625
aF4.343351745605469
aF4.326112365722656
aF4.314047241210938
aF4.301351013183594
aF4.2832421875
aF4.265270080566406
aF4.262452697753906
aF4.231010131835937
aF4.219574890136719
aF4.200977478027344
aF4.186132202148437
aF4.173632202148437
aF4.145082702636719
aF4.124539184570312
aF4.104059143066406
aF4.0794091796875
aF4.0663058471679685
aF4.044623107910156
aF4.023656005859375
aF4.004710388183594
aF3.977666320800781
aF3.9601324462890624
aF3.9377642822265626
aF3.912389831542969
aF3.8947216796875
aF3.874444580078125
aF3.8459783935546876
aF3.82630126953125
aF3.804438171386719
aF3.77830810546875
aF3.7763995361328124
aF3.7364569091796875
aF3.7357388305664063
aF3.6915765380859376
aF3.689863586425781
aF3.651858215332031
aF3.636000061035156
aF3.618135681152344
aF3.606141357421875
aF3.5870046997070313
aF3.57490966796875
aF3.5600320434570314
aF3.544896240234375
aF3.5242034912109377
aF3.528909912109375
aF3.497126159667969
aF3.4817849731445314
aF3.491518249511719
aF3.458968505859375
aF3.4652639770507814
aF3.463507080078125
aF3.447856140136719
aF3.4266207885742186
aF3.422263488769531
aF3.41614013671875
aF3.3920193481445313
aF3.3839450073242188
aF3.3760940551757814
aF3.3795965576171874
aF3.377696838378906
aF3.346194152832031
aF3.372420349121094
aF3.344313659667969
aF3.35060546875
aF3.3301119995117188
aF3.3487457275390624
aF3.33529541015625
aF3.324892883300781
aF3.33276123046875
aF3.3434222412109373
aF3.312594299316406
aF3.307408752441406
aF3.3106369018554687
aF3.3017532348632814
aF3.31250732421875
aF3.2901318359375
aF3.288782653808594
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00020768373966085135
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 44s'
p10
sS'final_test_loss'
p11
F3.288782653808594
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'M\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.