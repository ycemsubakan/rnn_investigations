(dp0
S'train_loss'
p1
(lp2
F4.640892028808594
aF3.9930938720703124
aF3.3795196533203127
aF3.1094937133789062
aF2.966469421386719
aF2.839041748046875
aF2.7539990234375
aF2.7178109741210936
aF2.6386843872070314
aF2.6184756469726564
aF2.5708941650390624
aF2.526871795654297
aF2.5130755615234377
aF2.4645310974121095
aF2.4298533630371093
aF2.403941802978516
aF2.3969660949707032
aF2.362804260253906
aF2.342488098144531
aF2.333850860595703
aF2.322368316650391
aF2.3177560424804686
aF2.284668426513672
aF2.303786163330078
aF2.2630239868164064
aF2.2627513122558596
aF2.232017822265625
aF2.2480398559570314
aF2.2057736206054686
aF2.202891540527344
aF2.208809661865234
aF2.2003073120117187
aF2.1527394104003905
aF2.149399719238281
aF2.1635693359375
aF2.142653350830078
aF2.145706787109375
aF2.133989715576172
aF2.108436737060547
aF2.0975474548339843
aF2.117276916503906
aF2.1253207397460936
aF2.093719177246094
aF2.084102783203125
aF2.06871337890625
aF2.080000915527344
aF2.0799888610839843
aF2.0587310791015625
aF2.062380065917969
aF2.052748565673828
aF2.0509515380859376
aF2.0458860778808594
aF2.0191361999511717
aF2.0197084045410154
aF2.0385064697265625
aF2.028867492675781
aF2.035264587402344
aF2.0160260009765625
aF2.010881042480469
aF2.0157260131835937
aF1.9858251953125
aF1.9640354919433594
aF1.9900906372070313
aF1.9710087585449219
aF1.9855300903320312
aF1.984164276123047
aF1.968455810546875
aF1.9839157104492187
aF1.9709490966796874
aF1.9624436950683595
aF1.9651658630371094
aF1.9658905029296876
aF1.9464439392089843
aF1.9646492004394531
aF1.946315155029297
aF1.9263623046875
aF1.9305995178222657
aF1.9221136474609375
aF1.9346101379394531
aF1.9238427734375
aF1.9070355224609374
aF1.9074739074707032
aF1.9169625854492187
aF1.8949813842773438
aF1.9179530334472656
aF1.8962710571289063
aF1.9244769287109376
aF1.8898028564453124
aF1.8827769470214843
aF1.8715740966796874
aF1.9142909240722656
aF1.9023072814941406
aF1.8829306030273438
aF1.857467803955078
aF1.8824331665039062
aF1.8704443359375
aF1.8793771362304688
aF1.8418605041503906
aF1.8679275512695312
aF1.8615536499023437
asS'test_loss'
p3
(lp4
F3.987459411621094
aF3.3578204345703124
aF3.106770324707031
aF2.9752044677734375
aF2.8288479614257813
aF2.7618228149414064
aF2.6918084716796873
aF2.657842102050781
aF2.6096295166015624
aF2.585538635253906
aF2.507912139892578
aF2.4893624877929685
aF2.443482208251953
aF2.448378143310547
aF2.417001495361328
aF2.380997772216797
aF2.3651499938964844
aF2.355214080810547
aF2.317911834716797
aF2.305549774169922
aF2.2898651123046876
aF2.2891087341308594
aF2.2683409118652342
aF2.2478631591796874
aF2.244087371826172
aF2.225370330810547
aF2.2250970458984374
aF2.2065080261230468
aF2.207580261230469
aF2.1890435791015626
aF2.187560577392578
aF2.1737100219726564
aF2.179095458984375
aF2.1688520812988283
aF2.1561567687988283
aF2.133877868652344
aF2.117412414550781
aF2.1416644287109374
aF2.114344024658203
aF2.1189801025390627
aF2.0916143798828126
aF2.093622283935547
aF2.074535064697266
aF2.07059326171875
aF2.0772657775878907
aF2.071053009033203
aF2.0418637084960936
aF2.0659881591796876
aF2.053592529296875
aF2.020361022949219
aF2.0391847229003908
aF2.0417129516601564
aF2.0115315246582033
aF2.001470031738281
aF2.039376220703125
aF2.0019781494140627
aF2.0080780029296874
aF2.0110455322265626
aF2.0045429992675783
aF1.9906622314453124
aF1.998887176513672
aF1.9843728637695313
aF1.9840232849121093
aF1.974659423828125
aF1.9689299011230468
aF1.9624461364746093
aF1.9830647277832032
aF1.9493988037109375
aF1.9517942810058593
aF1.9544668579101563
aF1.961004638671875
aF1.9416795349121094
aF1.9248306274414062
aF1.9459098815917968
aF1.9415902709960937
aF1.9249424743652344
aF1.9274687194824218
aF1.92172119140625
aF1.9533309936523438
aF1.9114096069335937
aF1.8981622314453126
aF1.8847593688964843
aF1.8970260620117188
aF1.9042977905273437
aF1.9018231201171876
aF1.8868417358398437
aF1.8912779235839843
aF1.903062744140625
aF1.8903366088867188
aF1.8667286682128905
aF1.8811451721191406
aF1.8693873596191406
aF1.874961395263672
aF1.8704681396484375
aF1.8775985717773438
aF1.866754913330078
aF1.8622917175292968
aF1.877144775390625
aF1.8778245544433594
aF1.835258026123047
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004901899605094485
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 7s'
p10
sS'final_test_loss'
p11
F1.835258026123047
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xc8\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.