(dp0
S'train_loss'
p1
(lp2
F4.652033996582031
aF4.276647338867187
aF3.7122848510742186
aF3.407825927734375
aF3.3108734130859374
aF3.2069671630859373
aF3.115644836425781
aF3.0359954833984375
aF2.96123779296875
aF2.907461853027344
aF2.82047119140625
aF2.7864459228515623
aF2.7601943969726563
aF2.7078680419921874
aF2.6606375122070314
aF2.641726379394531
aF2.603102722167969
aF2.578302001953125
aF2.5311492919921874
aF2.513880310058594
aF2.4984153747558593
aF2.458907012939453
aF2.4391383361816406
aF2.4459774780273436
aF2.4148550415039063
aF2.4035595703125
aF2.3733753967285156
aF2.369932861328125
aF2.3699002075195312
aF2.3383905029296876
aF2.3310447692871095
aF2.3108505249023437
aF2.3241127014160154
aF2.2939823913574218
aF2.2794595336914063
aF2.2712339782714843
aF2.2623001098632813
aF2.2339299011230467
aF2.240273132324219
aF2.211186981201172
aF2.2223626708984376
aF2.2168231201171875
aF2.1948651123046874
aF2.177806549072266
aF2.180475158691406
aF2.1635708618164062
aF2.1662753295898436
aF2.1446903991699218
aF2.1486085510253905
aF2.152494201660156
aF2.1266168212890624
aF2.109188690185547
aF2.115486907958984
aF2.1021487426757814
aF2.1047541809082033
aF2.109770965576172
aF2.1015289306640623
aF2.093835754394531
aF2.1003047180175782
aF2.081421356201172
aF2.0694667053222657
aF2.073494720458984
aF2.0567558288574217
aF2.061789093017578
aF2.037186584472656
aF2.044575958251953
aF2.0116500854492188
aF2.0066590881347657
aF2.0406784057617187
aF2.0169252014160155
aF2.03076171875
aF2.0280213928222657
aF1.9948875427246093
aF1.9900828552246095
aF1.9782948303222656
aF2.0069038391113283
aF2.002125701904297
aF1.985497283935547
aF1.9805931091308593
aF1.9712335205078124
aF1.958232421875
aF1.970938720703125
aF1.9567909240722656
aF1.936631622314453
aF1.9526016235351562
aF1.943738250732422
aF1.956520538330078
aF1.9398692321777344
aF1.9530056762695311
aF1.9384796142578125
aF1.9363807678222655
aF1.914567108154297
aF1.928504638671875
aF1.9254917907714844
aF1.9364268493652343
aF1.9026808166503906
aF1.9234587097167968
aF1.9266844177246094
aF1.9306159973144532
aF1.9160130310058594
asS'test_loss'
p3
(lp4
F4.280696411132812
aF3.721640930175781
aF3.4290704345703125
aF3.2948593139648437
aF3.1959698486328123
aF3.1214108276367187
aF3.008525390625
aF2.97572021484375
aF2.8708535766601564
aF2.85048828125
aF2.81375
aF2.746451110839844
aF2.7105789184570312
aF2.6541473388671877
aF2.6204046630859374
aF2.557625274658203
aF2.5422842407226565
aF2.5321917724609375
aF2.5056492614746095
aF2.44752685546875
aF2.470300750732422
aF2.4507572937011717
aF2.434609832763672
aF2.399180145263672
aF2.3896665954589844
aF2.372425994873047
aF2.352902679443359
aF2.3371119689941406
aF2.3416812133789064
aF2.3319140625
aF2.314034118652344
aF2.280306549072266
aF2.2855908203125
aF2.264498748779297
aF2.2795584106445315
aF2.2429963684082033
aF2.240050048828125
aF2.230066833496094
aF2.2241456604003904
aF2.2039739990234377
aF2.1771174621582032
aF2.195351104736328
aF2.1888951110839843
aF2.1817181396484373
aF2.194979248046875
aF2.1742312622070314
aF2.135072784423828
aF2.1539869689941407
aF2.1259036254882813
aF2.139568634033203
aF2.1211114501953126
aF2.122462158203125
aF2.138909912109375
aF2.1014918518066406
aF2.1057087707519533
aF2.1019219970703125
aF2.0688430786132814
aF2.0864993286132814
aF2.085251922607422
aF2.0573345947265627
aF2.047474822998047
aF2.0554527282714843
aF2.062542266845703
aF2.028828887939453
aF2.0420912170410155
aF2.048420257568359
aF2.0262216186523436
aF2.027537078857422
aF2.021120758056641
aF2.0193878173828126
aF2.0167610168457033
aF2.0098582458496095
aF1.9890394592285157
aF1.9766903686523438
aF1.9849461364746093
aF1.9896092224121094
aF1.9814401245117188
aF1.9767465209960937
aF1.9712521362304687
aF1.9748204040527344
aF1.9598638916015625
aF1.9528805541992187
aF1.9600645446777343
aF1.9513459777832032
aF1.9403367614746094
aF1.953636474609375
aF1.9467066955566406
aF1.9481961059570312
aF1.9554364013671874
aF1.9184284973144532
aF1.9371031188964845
aF1.9322108459472656
aF1.9361512756347656
aF1.9207374572753906
aF1.91019287109375
aF1.9250828552246093
aF1.9100437927246094
aF1.9127699279785155
aF1.8723219299316407
aF1.903900146484375
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.004590255812740704
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 29s'
p10
sS'final_test_loss'
p11
F1.903900146484375
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x84\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.