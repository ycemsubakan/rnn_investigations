(dp0
S'train_loss'
p1
(lp2
F4.610830688476563
aF4.578414611816406
aF4.545217895507813
aF4.518619689941406
aF4.4838388061523435
aF4.452142639160156
aF4.415430908203125
aF4.382543640136719
aF4.340852966308594
aF4.300346069335937
aF4.254441528320313
aF4.221189270019531
aF4.171700134277343
aF4.120885009765625
aF4.075243835449219
aF4.01220458984375
aF3.956276550292969
aF3.8941909790039064
aF3.855885314941406
aF3.7917446899414062
aF3.7440499877929687
aF3.6852667236328127
aF3.6479241943359373
aF3.6031893920898437
aF3.5528628540039064
aF3.533861083984375
aF3.493580322265625
aF3.4756024169921873
aF3.4891207885742186
aF3.437851867675781
aF3.399466247558594
aF3.389500732421875
aF3.3636688232421874
aF3.3755401611328124
aF3.346299133300781
aF3.335089111328125
aF3.2806500244140624
aF3.271912841796875
aF3.247091979980469
aF3.2434130859375
aF3.218436279296875
aF3.2136166381835936
aF3.210743713378906
aF3.188523864746094
aF3.1727520751953127
aF3.143824157714844
aF3.13815185546875
aF3.1319219970703127
aF3.1167538452148436
aF3.11509765625
aF3.081455078125
aF3.0999075317382814
aF3.0834811401367186
aF3.0461236572265626
aF3.0434085083007814
aF3.0517477416992187
aF3.0200540161132814
aF3.0358651733398436
aF2.989055480957031
aF2.995542297363281
aF2.9844232177734376
aF3.0007513427734374
aF2.978101806640625
aF2.989765930175781
aF2.9578518676757812
aF2.958682861328125
aF2.9763424682617186
aF2.943644714355469
aF2.9498193359375
aF2.920894775390625
aF2.922635803222656
aF2.9158026123046876
aF2.894854736328125
aF2.882069091796875
aF2.8810519409179687
aF2.8612429809570314
aF2.8858218383789063
aF2.863428955078125
aF2.843489685058594
aF2.845440673828125
aF2.8303231811523437
aF2.8399951171875
aF2.818792724609375
aF2.801749267578125
aF2.8097845458984376
aF2.7981158447265626
aF2.8027554321289063
aF2.7938711547851565
aF2.806488342285156
aF2.77416015625
aF2.76982666015625
aF2.7905929565429686
aF2.763564147949219
aF2.759296875
aF2.7408212280273436
aF2.7799758911132812
aF2.7229150390625
aF2.7226666259765624
aF2.7493161010742186
aF2.7197525024414064
asS'test_loss'
p3
(lp4
F4.5785894775390625
aF4.548211975097656
aF4.514644470214844
aF4.4848330688476565
aF4.447984313964843
aF4.417239990234375
aF4.379804077148438
aF4.339559326171875
aF4.3017730712890625
aF4.25444091796875
aF4.215864562988282
aF4.167315063476562
aF4.125731201171875
aF4.062288513183594
aF4.011540222167969
aF3.9660604858398436
aF3.9003164672851565
aF3.8512728881835936
aF3.7835989379882813
aF3.73351806640625
aF3.6847024536132813
aF3.6375796508789064
aF3.5964706420898436
aF3.5732876586914064
aF3.536435241699219
aF3.5080670166015624
aF3.483297119140625
aF3.4358551025390627
aF3.4158514404296874
aF3.404219970703125
aF3.373981628417969
aF3.344649658203125
aF3.3550323486328124
aF3.3417303466796877
aF3.3074871826171877
aF3.2830966186523436
aF3.2685305786132814
aF3.2477789306640625
aF3.2351113891601564
aF3.2330023193359376
aF3.207316589355469
aF3.1911019897460937
aF3.165250549316406
aF3.156927185058594
aF3.1404364013671877
aF3.1103125
aF3.1215133666992188
aF3.1114190673828124
aF3.0775247192382813
aF3.0959112548828127
aF3.0994277954101563
aF3.0726528930664063
aF3.0651058959960937
aF3.079527587890625
aF3.03316162109375
aF3.044553527832031
aF3.01697265625
aF3.0073675537109374
aF3.0042684936523436
aF2.97783447265625
aF2.988489074707031
aF2.9643545532226563
aF2.9679690551757814
aF2.98410400390625
aF2.94537353515625
aF2.9425372314453124
aF2.9085989379882813
aF2.933166198730469
aF2.9104080200195312
aF2.9110317993164063
aF2.88904541015625
aF2.8974624633789063
aF2.8892141723632814
aF2.87075439453125
aF2.882359619140625
aF2.8536395263671874
aF2.8632196044921874
aF2.8403781127929686
aF2.8330419921875
aF2.8387484741210938
aF2.814451904296875
aF2.8173541259765624
aF2.827216796875
aF2.806147155761719
aF2.8059725952148438
aF2.798436584472656
aF2.775740966796875
aF2.782147216796875
aF2.7689154052734377
aF2.763077697753906
aF2.7937918090820313
aF2.75001953125
aF2.73493896484375
aF2.731567687988281
aF2.729808349609375
aF2.727314758300781
aF2.7367398071289064
aF2.7121429443359375
aF2.714122619628906
aF2.720552673339844
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00016115908108960638
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 7s'
p10
sS'final_test_loss'
p11
F2.720552673339844
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe3\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.