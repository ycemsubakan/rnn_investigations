(dp0
S'train_loss'
p1
(lp2
F4.638272399902344
aF4.587427978515625
aF4.534828491210938
aF4.488579406738281
aF4.438496398925781
aF4.383294982910156
aF4.33233642578125
aF4.283512878417969
aF4.229669189453125
aF4.167505187988281
aF4.135152587890625
aF4.064186401367188
aF4.006744995117187
aF3.9479745483398436
aF3.865514221191406
aF3.804875793457031
aF3.7563177490234376
aF3.6865225219726563
aF3.611294860839844
aF3.5613037109375
aF3.4685589599609377
aF3.417508544921875
aF3.3741839599609373
aF3.347862548828125
aF3.2821786499023435
aF3.2755618286132813
aF3.2675625610351564
aF3.2139132690429686
aF3.198187255859375
aF3.205277099609375
aF3.1546771240234377
aF3.135847473144531
aF3.1497027587890627
aF3.097161865234375
aF3.1008377075195312
aF3.0612246704101564
aF3.0585528564453126
aF3.0329742431640625
aF3.0420266723632814
aF2.9935311889648437
aF2.9789688110351564
aF2.9777737426757813
aF2.9525192260742186
aF2.9479098510742188
aF2.9309564208984376
aF2.8979397583007813
aF2.90679443359375
aF2.9187579345703125
aF2.8899542236328126
aF2.8769976806640627
aF2.892714538574219
aF2.843399963378906
aF2.852872314453125
aF2.84054443359375
aF2.8192950439453126
aF2.832444152832031
aF2.8100009155273438
aF2.799361267089844
aF2.8131790161132812
aF2.8005084228515624
aF2.7718896484375
aF2.7833847045898437
aF2.7649932861328126
aF2.7636367797851564
aF2.778101806640625
aF2.7487411499023438
aF2.7553414916992187
aF2.7429998779296874
aF2.724814147949219
aF2.7349972534179687
aF2.714621276855469
aF2.7145620727539064
aF2.6975115966796874
aF2.6796609497070314
aF2.6812570190429685
aF2.681009521484375
aF2.674216613769531
aF2.661785888671875
aF2.6771560668945313
aF2.661246032714844
aF2.6419992065429687
aF2.6536941528320312
aF2.6512911987304686
aF2.645230407714844
aF2.6470379638671875
aF2.62430419921875
aF2.6348526000976564
aF2.6107843017578123
aF2.6401901245117188
aF2.6300299072265627
aF2.586014709472656
aF2.5897091674804686
aF2.600968933105469
aF2.612041015625
aF2.588472595214844
aF2.5947341918945312
aF2.584881591796875
aF2.592235412597656
aF2.5829733276367186
aF2.5825186157226563
asS'test_loss'
p3
(lp4
F4.583636169433594
aF4.534159240722656
aF4.480923156738282
aF4.435098266601562
aF4.3848583984375
aF4.3378994750976565
aF4.285209045410157
aF4.229376220703125
aF4.175590209960937
aF4.12262451171875
aF4.056322937011719
aF3.9946923828125
aF3.942354431152344
aF3.8674798583984376
aF3.8084771728515623
aF3.7603469848632813
aF3.68852783203125
aF3.610889892578125
aF3.551156005859375
aF3.482738037109375
aF3.434549560546875
aF3.368912658691406
aF3.3504486083984375
aF3.290621032714844
aF3.278175048828125
aF3.2727261352539063
aF3.226319580078125
aF3.2040090942382813
aF3.1862368774414063
aF3.1436721801757814
aF3.128542175292969
aF3.135042419433594
aF3.084400634765625
aF3.0975106811523436
aF3.056260070800781
aF3.0453750610351564
aF3.035636291503906
aF3.0193341064453123
aF3.013294982910156
aF2.9825704956054686
aF2.9808270263671877
aF2.944310302734375
aF2.9389797973632814
aF2.9336322021484373
aF2.927645568847656
aF2.9197064208984376
aF2.9158807373046876
aF2.887744140625
aF2.8809161376953125
aF2.85572021484375
aF2.855013732910156
aF2.8626638793945314
aF2.8489617919921875
aF2.8351846313476563
aF2.82405029296875
aF2.8344781494140623
aF2.7928262329101563
aF2.7893768310546876
aF2.779281005859375
aF2.793473205566406
aF2.759845886230469
aF2.766929016113281
aF2.7583126831054687
aF2.7347811889648437
aF2.741210632324219
aF2.744056396484375
aF2.736600036621094
aF2.7219305419921875
aF2.7081088256835937
aF2.7031103515625
aF2.6899444580078127
aF2.696152648925781
aF2.697134704589844
aF2.689483947753906
aF2.670934753417969
aF2.6626361083984373
aF2.66246826171875
aF2.6630825805664062
aF2.6506301879882814
aF2.6527633666992188
aF2.6512374877929688
aF2.651910400390625
aF2.644713134765625
aF2.6291995239257813
aF2.61292236328125
aF2.631880187988281
aF2.6039349365234377
aF2.6096261596679686
aF2.623251647949219
aF2.607929382324219
aF2.5907077026367187
aF2.6032333374023438
aF2.5731820678710937
aF2.5801791381835937
aF2.60276123046875
aF2.597898254394531
aF2.564599609375
aF2.5623309326171877
aF2.5774496459960936
aF2.5585113525390626
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.00025591046556567876
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 17s'
p10
sS'final_test_loss'
p11
F2.5585113525390626
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xe9\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.