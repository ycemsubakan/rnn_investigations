(dp0
S'train_loss'
p1
(lp2
F4.635324401855469
aF4.536419982910156
aF4.4323727416992185
aF4.3259579467773435
aF4.202404174804688
aF4.03920166015625
aF3.8861962890625
aF3.7370648193359375
aF3.623535461425781
aF3.4937667846679688
aF3.455808410644531
aF3.401017150878906
aF3.3041671752929687
aF3.3046231079101562
aF3.264676513671875
aF3.227280578613281
aF3.172845458984375
aF3.127598876953125
aF3.0923712158203127
aF3.067604675292969
aF3.0408248901367188
aF3.0351858520507813
aF2.97423828125
aF2.9574847412109375
aF2.9205563354492186
aF2.9059963989257813
aF2.8971905517578125
aF2.8725457763671876
aF2.854884033203125
aF2.818074951171875
aF2.7999777221679687
aF2.8091806030273436
aF2.7852947998046873
aF2.746204528808594
aF2.74994873046875
aF2.7250335693359373
aF2.6878292846679686
aF2.6950009155273436
aF2.6813970947265626
aF2.6700173950195314
aF2.6762759399414064
aF2.645030822753906
aF2.6165945434570315
aF2.61912109375
aF2.5948312377929685
aF2.59822509765625
aF2.58240234375
aF2.574068603515625
aF2.5646197509765627
aF2.5597184753417968
aF2.5478775024414064
aF2.537924957275391
aF2.5137095642089844
aF2.496593322753906
aF2.5148716735839844
aF2.4887750244140623
aF2.4696392822265625
aF2.471788177490234
aF2.4763455200195312
aF2.452370300292969
aF2.4526766967773437
aF2.4498907470703126
aF2.449166717529297
aF2.425995635986328
aF2.4140963745117188
aF2.4031861877441405
aF2.418557434082031
aF2.406879730224609
aF2.397643585205078
aF2.384690856933594
aF2.381208038330078
aF2.375914306640625
aF2.367845916748047
aF2.365095367431641
aF2.362443695068359
aF2.3672857666015625
aF2.34279541015625
aF2.351995086669922
aF2.353919677734375
aF2.337821350097656
aF2.346375274658203
aF2.313495330810547
aF2.3098690795898436
aF2.3391375732421875
aF2.3183610534667967
aF2.320898132324219
aF2.2961991882324218
aF2.3017567443847655
aF2.3104476928710938
aF2.289894714355469
aF2.289924774169922
aF2.3013714599609374
aF2.2806137084960936
aF2.276929931640625
aF2.2723463439941405
aF2.271345367431641
aF2.2917776489257813
aF2.2492791748046876
aF2.2625665283203125
aF2.2604302978515625
asS'test_loss'
p3
(lp4
F4.533646850585938
aF4.432374267578125
aF4.315609436035157
aF4.197613830566406
aF4.054949340820312
aF3.89238525390625
aF3.7467074584960938
aF3.6216375732421877
aF3.510378723144531
aF3.4486703491210937
aF3.39418212890625
aF3.3441851806640623
aF3.2667477416992186
aF3.253279113769531
aF3.211947021484375
aF3.1621539306640627
aF3.1362408447265624
aF3.061712341308594
aF3.0506881713867187
aF3.017857666015625
aF2.98110107421875
aF2.97198486328125
aF2.9473092651367185
aF2.8854031372070312
aF2.887329406738281
aF2.878375244140625
aF2.867734680175781
aF2.844767761230469
aF2.8332229614257813
aF2.8042706298828124
aF2.760718994140625
aF2.7742965698242186
aF2.731986083984375
aF2.722070617675781
aF2.71014404296875
aF2.702112121582031
aF2.690146789550781
aF2.676693115234375
aF2.658813171386719
aF2.6649029541015623
aF2.6412762451171874
aF2.616954650878906
aF2.61465087890625
aF2.5762814331054686
aF2.5707302856445313
aF2.560180358886719
aF2.5631671142578125
aF2.544012451171875
aF2.545978851318359
aF2.510467987060547
aF2.525367889404297
aF2.5130747985839843
aF2.498638458251953
aF2.5052191162109376
aF2.484030914306641
aF2.4771800231933594
aF2.484316558837891
aF2.4588386535644533
aF2.4664492797851563
aF2.435483856201172
aF2.437524871826172
aF2.432447814941406
aF2.43890380859375
aF2.417322998046875
aF2.4192747497558593
aF2.4080906677246094
aF2.381322021484375
aF2.389766845703125
aF2.374803924560547
aF2.3857571411132814
aF2.364929962158203
aF2.3593560791015626
aF2.351309967041016
aF2.3581161499023438
aF2.3357777404785156
aF2.3383474731445313
aF2.3452586364746093
aF2.33807861328125
aF2.3445004272460936
aF2.324991455078125
aF2.329294891357422
aF2.32009033203125
aF2.3194674682617187
aF2.306691131591797
aF2.2976609802246095
aF2.3049722290039063
aF2.299547576904297
aF2.294263458251953
aF2.3053143310546873
aF2.286371612548828
aF2.287165222167969
aF2.275234375
aF2.278874969482422
aF2.2712840270996093
aF2.2737936401367187
aF2.262139892578125
aF2.254969177246094
aF2.269464569091797
aF2.2510494995117187
aF2.253398132324219
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005375429588331481
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'2m 3s'
p10
sS'final_test_loss'
p11
F2.253398132324219
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xde\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.