(dp0
S'train_loss'
p1
(lp2
F4.614767150878906
aF4.300465087890625
aF3.994581604003906
aF3.64135986328125
aF3.3398187255859373
aF3.185175476074219
aF3.092504577636719
aF3.0400482177734376
aF2.9863162231445313
aF2.9450506591796874
aF2.8329986572265624
aF2.8004934692382815
aF2.7641250610351564
aF2.718428039550781
aF2.711716613769531
aF2.66819580078125
aF2.6484951782226562
aF2.61130126953125
aF2.6146884155273438
aF2.568331604003906
aF2.5806927490234375
aF2.538004913330078
aF2.5445703125
aF2.501282958984375
aF2.4870928955078124
aF2.4782102966308592
aF2.473207702636719
aF2.465149230957031
aF2.410804901123047
aF2.4194482421875
aF2.421819763183594
aF2.3862908935546874
aF2.393472442626953
aF2.3838304138183593
aF2.3755235290527343
aF2.3505511474609375
aF2.3613812255859377
aF2.3436163330078124
aF2.3299661254882813
aF2.3062785339355467
aF2.3149224853515626
aF2.2984684753417968
aF2.302597961425781
aF2.2913511657714842
aF2.282397918701172
aF2.2769979858398437
aF2.255277557373047
aF2.2585565185546876
aF2.2626544189453126
aF2.2592697143554688
aF2.2421383666992187
aF2.2300096130371094
aF2.2275601196289063
aF2.229438934326172
aF2.213918914794922
aF2.2111582946777344
aF2.212349395751953
aF2.2008097839355467
aF2.181945037841797
aF2.202850341796875
aF2.181260223388672
aF2.169752502441406
aF2.178555908203125
aF2.1728102111816407
aF2.156183166503906
aF2.1568858337402346
aF2.167854919433594
aF2.1441200256347654
aF2.1444989013671876
aF2.1363214111328124
aF2.140875244140625
aF2.111825714111328
aF2.113736877441406
aF2.1237063598632813
aF2.095991363525391
aF2.1219802856445313
aF2.1169593811035154
aF2.102145080566406
aF2.075918884277344
aF2.096181182861328
aF2.1117498779296877
aF2.0922080993652346
aF2.102075500488281
aF2.074830322265625
aF2.0674452209472656
aF2.0745201110839844
aF2.077892303466797
aF2.0805929565429686
aF2.0568978881835935
aF2.0570423889160154
aF2.035036773681641
aF2.037691802978516
aF2.062611083984375
aF2.0310903930664064
aF2.0488272094726563
aF2.02713134765625
aF2.0218572998046875
aF2.020151824951172
aF2.0230189514160157
aF2.0340898132324217
asS'test_loss'
p3
(lp4
F4.306170043945312
aF3.9884283447265627
aF3.6519036865234376
aF3.3376010131835936
aF3.2003997802734374
aF3.072424011230469
aF3.045182800292969
aF2.984022521972656
aF2.9037106323242186
aF2.810552062988281
aF2.79235107421875
aF2.751910400390625
aF2.7163705444335937
aF2.7009771728515624
aF2.678433837890625
aF2.64802734375
aF2.62497802734375
aF2.5834771728515626
aF2.579446105957031
aF2.564144287109375
aF2.5299198913574217
aF2.5042796325683594
aF2.52864013671875
aF2.49783203125
aF2.4858978271484373
aF2.4695809936523436
aF2.4419064331054687
aF2.4214418029785154
aF2.4097581481933594
aF2.3913113403320314
aF2.412339172363281
aF2.3943800354003906
aF2.3781951904296874
aF2.3733958435058593
aF2.3535284423828124
aF2.338553924560547
aF2.314499816894531
aF2.3220941162109376
aF2.310079803466797
aF2.3055865478515627
aF2.288710479736328
aF2.2855078125
aF2.287106475830078
aF2.280945129394531
aF2.260552978515625
aF2.26165283203125
aF2.247404022216797
aF2.2524288940429686
aF2.246553955078125
aF2.2298680114746094
aF2.2346012878417967
aF2.2134214782714845
aF2.231900939941406
aF2.1997615051269532
aF2.198716278076172
aF2.206307067871094
aF2.195064697265625
aF2.2122100830078124
aF2.1687583923339844
aF2.1813201904296875
aF2.1542352294921874
aF2.1650323486328125
aF2.1624441528320313
aF2.150057678222656
aF2.137864837646484
aF2.153389739990234
aF2.1498068237304686
aF2.153643035888672
aF2.1118095397949217
aF2.1314056396484373
aF2.130076446533203
aF2.1179463195800783
aF2.115495300292969
aF2.1231787109375
aF2.1117051696777343
aF2.0988255310058594
aF2.100931396484375
aF2.1085337829589843
aF2.0808168029785157
aF2.0897151184082032
aF2.1105606079101564
aF2.096396636962891
aF2.0686488342285156
aF2.086600494384766
aF2.0779983520507814
aF2.0759822082519532
aF2.0474124145507813
aF2.0579649353027345
aF2.064250183105469
aF2.057243194580078
aF2.059037170410156
aF2.053714141845703
aF2.043270263671875
aF2.0509698486328123
aF2.052931671142578
aF2.028775329589844
aF2.040419158935547
aF2.0339405822753904
aF2.008892059326172
aF2.025341033935547
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0018614094887880453
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 10s'
p10
sS'final_test_loss'
p11
F2.025341033935547
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xd6\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.