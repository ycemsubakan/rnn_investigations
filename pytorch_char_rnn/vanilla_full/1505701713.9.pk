(dp0
S'train_loss'
p1
(lp2
F4.609443359375
aF4.477468872070313
aF4.345962219238281
aF4.222978210449218
aF4.073904418945313
aF3.9331631469726562
aF3.796632995605469
aF3.6567922973632814
aF3.4986053466796876
aF3.373079833984375
aF3.302665100097656
aF3.2139263916015626
aF3.1680364990234375
aF3.1173040771484377
aF3.047540588378906
aF3.0213552856445314
aF2.979996643066406
aF2.94043701171875
aF2.9261044311523436
aF2.8886102294921874
aF2.875743103027344
aF2.850813293457031
aF2.8359222412109375
aF2.7983694458007813
aF2.776248779296875
aF2.7698529052734373
aF2.74106689453125
aF2.7398016357421877
aF2.6968829345703127
aF2.6887716674804687
aF2.6934515380859376
aF2.644903564453125
aF2.6864913940429687
aF2.6540362548828127
aF2.6463107299804687
aF2.6136154174804687
aF2.593433532714844
aF2.5879745483398438
aF2.5800091552734377
aF2.6019515991210938
aF2.5538619995117187
aF2.559855194091797
aF2.5269725036621096
aF2.5412918090820313
aF2.5399766540527344
aF2.511222686767578
aF2.49478271484375
aF2.496352233886719
aF2.4855389404296875
aF2.496781921386719
aF2.4776988220214844
aF2.4793026733398436
aF2.4652964782714846
aF2.473825225830078
aF2.4510240173339843
aF2.4695205688476562
aF2.459109039306641
aF2.450808258056641
aF2.4275157165527346
aF2.4270684814453123
aF2.40634765625
aF2.424887847900391
aF2.4168646240234377
aF2.3913331604003907
aF2.3956597900390624
aF2.3687699890136718
aF2.385939178466797
aF2.369871063232422
aF2.3685931396484374
aF2.3744082641601563
aF2.355238800048828
aF2.3362826538085937
aF2.3455270385742186
aF2.342917785644531
aF2.334230651855469
aF2.3428411865234375
aF2.331407318115234
aF2.330220947265625
aF2.3316473388671874
aF2.3071556091308594
aF2.3132366943359375
aF2.330790710449219
aF2.2835614013671877
aF2.30595458984375
aF2.2925376892089844
aF2.2836677551269533
aF2.2974252319335937
aF2.293329162597656
aF2.2729547119140623
aF2.278148345947266
aF2.2776890563964844
aF2.276413879394531
aF2.27302001953125
aF2.2568280029296877
aF2.238026123046875
aF2.2666389465332033
aF2.2500006103515626
aF2.2552415466308595
aF2.2368450927734376
aF2.2326701354980467
asS'test_loss'
p3
(lp4
F4.48064208984375
aF4.34481201171875
aF4.209940795898437
aF4.070879821777344
aF3.917615966796875
aF3.771666259765625
aF3.6422747802734374
aF3.4839376831054687
aF3.3608392333984374
aF3.2908334350585937
aF3.22792724609375
aF3.1580718994140624
aF3.09307373046875
aF3.0588751220703125
aF3.00649658203125
aF2.9744998168945314
aF2.9588140869140624
aF2.9272665405273437
aF2.9021902465820313
aF2.8667483520507813
aF2.8419573974609373
aF2.827489929199219
aF2.7998126220703123
aF2.787046203613281
aF2.7493692016601563
aF2.7558203125
aF2.743333740234375
aF2.708013000488281
aF2.704767761230469
aF2.6674658203125
aF2.66181396484375
aF2.6611505126953126
aF2.638492431640625
aF2.622242126464844
aF2.6061578369140626
aF2.6095123291015625
aF2.5967031860351564
aF2.5971917724609375
aF2.5727493286132814
aF2.571188659667969
aF2.5608349609375
aF2.5568714904785157
aF2.552622833251953
aF2.5353053283691405
aF2.511064910888672
aF2.5110128784179686
aF2.489399871826172
aF2.50837158203125
aF2.492650451660156
aF2.4874996948242187
aF2.4833522033691406
aF2.4557798767089842
aF2.4696400451660154
aF2.429427032470703
aF2.460846405029297
aF2.4314566040039063
aF2.4243678283691406
aF2.4226849365234373
aF2.4186505126953124
aF2.4005992126464846
aF2.4072340393066405
aF2.3846885681152346
aF2.379602508544922
aF2.38700927734375
aF2.3950819396972656
aF2.3830940246582033
aF2.363139801025391
aF2.372415008544922
aF2.348532257080078
aF2.35860595703125
aF2.335092010498047
aF2.337623748779297
aF2.338933410644531
aF2.350078887939453
aF2.3362449645996093
aF2.339930114746094
aF2.3130601501464843
aF2.3081805419921877
aF2.2891563415527343
aF2.3004736328125
aF2.304998016357422
aF2.292491912841797
aF2.301294708251953
aF2.294464111328125
aF2.280614929199219
aF2.271798858642578
aF2.279887390136719
aF2.2733613586425783
aF2.2710797119140627
aF2.2624839782714843
aF2.267685852050781
aF2.2533726501464844
aF2.2444146728515624
aF2.2587351989746094
aF2.2569314575195314
aF2.2599659729003907
aF2.249894866943359
aF2.248911590576172
aF2.236589202880859
aF2.234565887451172
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0005264391814016043
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 30s'
p10
sS'final_test_loss'
p11
F2.234565887451172
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x1e\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.