(dp0
S'train_loss'
p1
(lp2
F4.646798095703125
aF4.514351196289063
aF4.386385803222656
aF4.262060852050781
aF4.099998474121094
aF3.9579718017578127
aF3.799646301269531
aF3.619208984375
aF3.4671649169921874
aF3.3240847778320313
aF3.2264724731445313
aF3.1873663330078124
aF3.1320761108398436
aF3.127826843261719
aF3.0722622680664062
aF3.024374694824219
aF3.0041824340820313
aF2.928860778808594
aF2.9063916015625
aF2.8585330200195314
aF2.852899475097656
aF2.8444970703125
aF2.82836669921875
aF2.787205810546875
aF2.788165283203125
aF2.7582763671875
aF2.7324044799804685
aF2.7329254150390625
aF2.704022216796875
aF2.652061767578125
aF2.65837158203125
aF2.679273681640625
aF2.6163641357421876
aF2.632906188964844
aF2.630931396484375
aF2.59251220703125
aF2.594585266113281
aF2.5790609741210937
aF2.558963623046875
aF2.5542025756835938
aF2.5480783081054685
aF2.533056640625
aF2.5251344299316405
aF2.504336090087891
aF2.535550231933594
aF2.5057917785644532
aF2.5016542053222657
aF2.4664134216308593
aF2.4587496948242187
aF2.4515000915527345
aF2.4441433715820313
aF2.436933135986328
aF2.4489364624023438
aF2.418244171142578
aF2.433141326904297
aF2.4135263061523435
aF2.414439849853516
aF2.395190887451172
aF2.4057101440429687
aF2.39232177734375
aF2.3730921936035156
aF2.3757675170898436
aF2.3775373840332032
aF2.3632444763183593
aF2.359391784667969
aF2.3523089599609377
aF2.3351708984375
aF2.3569180297851564
aF2.3289610290527345
aF2.3230458068847657
aF2.334284210205078
aF2.3281854248046874
aF2.3219784545898436
aF2.3132260131835936
aF2.321123504638672
aF2.3043299865722657
aF2.2880381774902343
aF2.298748779296875
aF2.3088299560546877
aF2.288716583251953
aF2.2869082641601564
aF2.2712493896484376
aF2.2755746459960937
aF2.2635342407226564
aF2.2783772277832033
aF2.249125518798828
aF2.272734832763672
aF2.2365650939941406
aF2.259757080078125
aF2.2410276794433592
aF2.247630310058594
aF2.249197540283203
aF2.219418487548828
aF2.2089186096191407
aF2.2349166870117188
aF2.2163438415527343
aF2.23632080078125
aF2.2266525268554687
aF2.2405015563964845
aF2.214007568359375
asS'test_loss'
p3
(lp4
F4.510697631835938
aF4.384640808105469
aF4.2447021484375
aF4.114579772949218
aF3.965529479980469
aF3.8056719970703123
aF3.6213427734375
aF3.468319091796875
aF3.31719970703125
aF3.2340997314453124
aF3.1836062622070314
aF3.138303527832031
aF3.1102584838867187
aF3.0549774169921875
aF3.0426593017578125
aF2.9696307373046875
aF2.9146502685546873
aF2.9014874267578126
aF2.855757751464844
aF2.8507754516601564
aF2.832327880859375
aF2.8156094360351562
aF2.7875927734375
aF2.7775152587890624
aF2.7228399658203126
aF2.7371975708007814
aF2.676705017089844
aF2.711607666015625
aF2.686683349609375
aF2.685289001464844
aF2.6477978515625
aF2.619684143066406
aF2.618272705078125
aF2.6085284423828123
aF2.5760089111328126
aF2.5834951782226563
aF2.562230224609375
aF2.563603820800781
aF2.560941467285156
aF2.538113861083984
aF2.5323388671875
aF2.508295440673828
aF2.499928283691406
aF2.474196319580078
aF2.500289306640625
aF2.4954707336425783
aF2.4640657043457033
aF2.475255126953125
aF2.452952117919922
aF2.4443182373046874
aF2.4410220336914064
aF2.4065016174316405
aF2.4150732421875
aF2.4222366333007814
aF2.397857360839844
aF2.398996124267578
aF2.3883840942382815
aF2.395122222900391
aF2.374192810058594
aF2.385431213378906
aF2.3684922790527345
aF2.3554447937011718
aF2.367168121337891
aF2.3537144470214844
aF2.3419563293457033
aF2.3288848876953123
aF2.333226623535156
aF2.328130340576172
aF2.3116023254394533
aF2.3158811950683593
aF2.3174369812011717
aF2.3043109130859376
aF2.301910095214844
aF2.2894747924804686
aF2.3005084228515624
aF2.2834323120117186
aF2.28801025390625
aF2.2736051940917967
aF2.2764361572265623
aF2.2878765869140625
aF2.2747320556640624
aF2.282725524902344
aF2.2539141845703123
aF2.2707180786132812
aF2.264970397949219
aF2.2409814453125
aF2.2451585388183593
aF2.243799743652344
aF2.2509341430664063
aF2.2395933532714842
aF2.22884521484375
aF2.23594970703125
aF2.237532958984375
aF2.2221525573730467
aF2.216311340332031
aF2.2322538757324217
aF2.218971862792969
aF2.2027554321289062
aF2.2007623291015626
aF2.1976136779785156
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0012240424398692678
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'0m 59s'
p10
sS'final_test_loss'
p11
F2.1976136779785156
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x9f\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.