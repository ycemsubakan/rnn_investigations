(dp0
S'train_loss'
p1
(lp2
F4.655150756835938
aF4.296306762695313
aF3.936933898925781
aF3.54562744140625
aF3.2228909301757813
aF3.1050384521484373
aF3.0044970703125
aF2.901020812988281
aF2.8192279052734377
aF2.7122479248046876
aF2.7318829345703124
aF2.7111477661132812
aF2.663204345703125
aF2.6175460815429688
aF2.5906643676757812
aF2.551668853759766
aF2.54391357421875
aF2.5050914001464846
aF2.482910614013672
aF2.4762298583984377
aF2.4678468322753906
aF2.4219430541992186
aF2.437098083496094
aF2.4131166076660158
aF2.3817156982421874
aF2.3635604858398436
aF2.3569267272949217
aF2.351587829589844
aF2.329742889404297
aF2.308003387451172
aF2.3229054260253905
aF2.3149905395507813
aF2.2894766235351565
aF2.2845013427734373
aF2.2704400634765625
aF2.270374603271484
aF2.266457977294922
aF2.245813446044922
aF2.2401744079589845
aF2.256968536376953
aF2.2066453552246093
aF2.236875
aF2.214885406494141
aF2.206898498535156
aF2.1954603576660157
aF2.185332946777344
aF2.1758432006835937
aF2.1657655334472654
aF2.164070892333984
aF2.1647181701660156
aF2.16626220703125
aF2.157462158203125
aF2.1274703979492187
aF2.1511880493164064
aF2.120430603027344
aF2.1288365173339843
aF2.136760559082031
aF2.125438995361328
aF2.116820983886719
aF2.1092616271972657
aF2.094525909423828
aF2.0940480041503906
aF2.1075
aF2.0961073303222655
aF2.09046630859375
aF2.084966583251953
aF2.0614387512207033
aF2.0658351135253907
aF2.0845716857910155
aF2.080674285888672
aF2.0740528869628907
aF2.075220031738281
aF2.0496824645996092
aF2.0580712890625
aF2.038022308349609
aF2.031606903076172
aF2.0483123779296877
aF2.0326107788085936
aF2.0284967041015625
aF2.0330046081542967
aF2.0546005249023436
aF2.025495147705078
aF2.0105908203125
aF2.021170959472656
aF1.9934767150878907
aF2.015425720214844
aF2.0117428588867186
aF2.0181619262695314
aF2.0012944030761717
aF2.0169984436035158
aF1.97859130859375
aF2.0000048828125
aF2.0005253601074218
aF1.9945037841796875
aF1.9584681701660156
aF1.9827688598632813
aF1.9748750305175782
aF1.9647560119628906
aF1.9516058349609375
aF1.9743637084960937
asS'test_loss'
p3
(lp4
F4.2935787963867185
aF3.95113525390625
aF3.5443060302734377
aF3.2285540771484373
aF3.0676229858398436
aF2.9613803100585936
aF2.9176126098632813
aF2.81089599609375
aF2.754629821777344
aF2.706519775390625
aF2.6855453491210937
aF2.6385488891601563
aF2.6166043090820312
aF2.608206787109375
aF2.5330247497558593
aF2.5474134826660157
aF2.4970448303222654
aF2.4775660705566405
aF2.4701947021484374
aF2.437894287109375
aF2.437338409423828
aF2.4348464965820313
aF2.3887315368652344
aF2.3715711975097657
aF2.343841094970703
aF2.369044189453125
aF2.3362905883789065
aF2.3265203857421874
aF2.2976995849609376
aF2.3001473999023436
aF2.2976678466796874
aF2.2926708984375
aF2.266515808105469
aF2.276979675292969
aF2.2579803466796875
aF2.2366671752929688
aF2.2568910217285154
aF2.239107208251953
aF2.2261940002441407
aF2.228920440673828
aF2.2028074645996094
aF2.1988075256347654
aF2.1945513916015624
aF2.2225347900390626
aF2.1805575561523436
aF2.1759687805175782
aF2.174043884277344
aF2.179565887451172
aF2.152238616943359
aF2.1576974487304685
aF2.147382965087891
aF2.130870819091797
aF2.164315032958984
aF2.135928955078125
aF2.1273764038085936
aF2.1115049743652343
aF2.113123321533203
aF2.096727142333984
aF2.1181675720214845
aF2.103993377685547
aF2.0949755859375
aF2.1098960876464843
aF2.0874305725097657
aF2.0991282653808594
aF2.0875384521484377
aF2.0863037109375
aF2.0703515625
aF2.0770468139648437
aF2.076180725097656
aF2.0782734680175783
aF2.068780822753906
aF2.051502532958984
aF2.0539447021484376
aF2.0411531066894533
aF2.0534042358398437
aF2.0254393005371094
aF2.048384246826172
aF2.0310372924804687
aF2.027647705078125
aF2.0243927001953126
aF2.029149169921875
aF2.026918487548828
aF2.030506134033203
aF2.0050338745117187
aF1.99846923828125
aF2.0243748474121093
aF1.9900115966796874
aF1.9906573486328125
aF1.9809065246582032
aF1.9864295959472655
aF1.9779969787597655
aF1.9914447021484376
aF1.9746145629882812
aF1.9709933471679688
aF1.9740507507324219
aF1.963577423095703
aF1.984602813720703
aF1.9716168212890626
aF1.9767724609375
aF1.9712655639648438
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.002137941587953181
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 14s'
p10
sS'final_test_loss'
p11
F1.9712655639648438
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\xec\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.