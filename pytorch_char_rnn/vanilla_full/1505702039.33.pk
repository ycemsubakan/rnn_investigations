(dp0
S'train_loss'
p1
(lp2
F4.671473083496093
aF3.6453863525390626
aF3.0641940307617186
aF2.8689199829101564
aF2.746856994628906
aF2.6688406372070315
aF2.576105041503906
aF2.5377456665039064
aF2.5054672241210936
aF2.4507797241210936
aF2.404519500732422
aF2.3809938049316406
aF2.3454713439941406
aF2.332854766845703
aF2.2953709411621093
aF2.2776611328125
aF2.2801441955566406
aF2.256028900146484
aF2.2478382873535154
aF2.223632049560547
aF2.215564880371094
aF2.2030790710449217
aF2.1922041320800782
aF2.172957458496094
aF2.1565802001953127
aF2.121424102783203
aF2.142084045410156
aF2.126453552246094
aF2.096072540283203
aF2.1000830078125
aF2.07454345703125
aF2.080058898925781
aF2.0837754821777343
aF2.096837921142578
aF2.0596966552734375
aF2.04812255859375
aF2.0232334899902344
aF2.0332737731933594
aF2.015652313232422
aF2.0042240905761717
aF2.0260993957519533
aF1.9843138122558595
aF1.993963623046875
aF1.9792173767089845
aF1.9700953674316406
aF1.9763067626953126
aF1.9527963256835938
aF1.960036163330078
aF1.9726853942871094
aF1.952145538330078
aF1.9636204528808594
aF1.9493350219726562
aF1.9581297302246095
aF1.9170797729492188
aF1.9132594299316406
aF1.9157858276367188
aF1.9014553833007812
aF1.8943922424316406
aF1.8766134643554688
aF1.8960472106933595
aF1.9041073608398438
aF1.875259552001953
aF1.8870204162597657
aF1.854713134765625
aF1.8655667114257812
aF1.8695611572265625
aF1.866876678466797
aF1.8508332824707032
aF1.8706130981445312
aF1.8702407836914063
aF1.861658477783203
aF1.8337660217285157
aF1.8511460876464845
aF1.8151007080078125
aF1.8198220825195313
aF1.8279586791992188
aF1.8217669677734376
aF1.8210508728027344
aF1.8002047729492188
aF1.8270101928710938
aF1.839757080078125
aF1.7983993530273437
aF1.8032713317871094
aF1.8034756469726563
aF1.8023629760742188
aF1.7922468566894532
aF1.7881021118164062
aF1.815282745361328
aF1.780760498046875
aF1.79164794921875
aF1.8037547302246093
aF1.7635113525390624
aF1.7741114807128906
aF1.7668104553222657
aF1.7681306457519532
aF1.780714569091797
aF1.7690969848632812
aF1.7817779541015626
aF1.774662628173828
aF1.7518524169921874
asS'test_loss'
p3
(lp4
F3.62061767578125
aF3.046807861328125
aF2.856396484375
aF2.731870422363281
aF2.647355651855469
aF2.5857171630859375
aF2.518620910644531
aF2.47163818359375
aF2.431976318359375
aF2.3938839721679686
aF2.380657501220703
aF2.3485124206542967
aF2.3352655029296874
aF2.3119798278808594
aF2.2774864196777345
aF2.2654066467285157
aF2.25154052734375
aF2.229523162841797
aF2.223017578125
aF2.2058395385742187
aF2.1905792236328123
aF2.170762176513672
aF2.1673480224609376
aF2.1538957214355468
aF2.1516909790039063
aF2.12562744140625
aF2.107757873535156
aF2.1009097290039063
aF2.0886859130859374
aF2.0908741760253906
aF2.0967425537109374
aF2.062006072998047
aF2.0603668212890627
aF2.064264221191406
aF2.041705780029297
aF2.0490216064453124
aF2.015095977783203
aF2.0127883911132813
aF1.9943574523925782
aF2.0091586303710938
aF2.0122114562988282
aF1.9895980834960938
aF2.000785675048828
aF1.995751190185547
aF1.9629745483398438
aF1.973082275390625
aF1.9603941345214844
aF1.9551113891601561
aF1.9414300537109375
aF1.9354908752441407
aF1.923376007080078
aF1.93231201171875
aF1.9381704711914063
aF1.8926669311523439
aF1.913102569580078
aF1.902763671875
aF1.920472412109375
aF1.9169413757324218
aF1.8840675354003906
aF1.893228759765625
aF1.901888885498047
aF1.894893798828125
aF1.8790621948242188
aF1.9008790588378905
aF1.8564317321777344
aF1.8803929138183593
aF1.8369313049316407
aF1.8704278564453125
aF1.8218400573730469
aF1.8539398193359375
aF1.8397109985351563
aF1.8293962097167968
aF1.8450344848632811
aF1.840401611328125
aF1.8556198120117187
aF1.7899423217773438
aF1.8035269165039063
aF1.8200079345703124
aF1.8070790100097656
aF1.8010794067382812
aF1.8041386413574219
aF1.7754310607910155
aF1.8115493774414062
aF1.8053187561035156
aF1.8088162231445313
aF1.8052366638183595
aF1.8017416381835938
aF1.7700881958007812
aF1.7818000793457032
aF1.8008955383300782
aF1.7816473388671874
aF1.7660272216796875
aF1.774532470703125
aF1.7457025146484375
aF1.7778347778320311
aF1.7688682556152344
aF1.7780754089355468
aF1.778966522216797
aF1.7521739196777344
aF1.7761781311035156
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.005890872989547282
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 26s'
p10
sS'final_test_loss'
p11
F1.7761781311035156
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x01\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'!\x01\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.