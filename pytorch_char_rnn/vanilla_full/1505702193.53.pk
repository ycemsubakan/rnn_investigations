(dp0
S'train_loss'
p1
(lp2
F4.606697692871093
aF4.5067376708984375
aF4.4011376953125
aF4.283930969238281
aF4.1563021850585935
aF4.0009765625
aF3.8398443603515626
aF3.6837228393554686
aF3.591816101074219
aF3.513313903808594
aF3.4433380126953126
aF3.4133905029296874
aF3.358793029785156
aF3.28577392578125
aF3.284299011230469
aF3.2451406860351564
aF3.2213778686523438
aF3.18468017578125
aF3.1419879150390626
aF3.140120849609375
aF3.105782775878906
aF3.109247131347656
aF3.053752746582031
aF3.0337014770507813
aF3.008531188964844
aF3.0077972412109375
aF2.9656500244140624
aF2.9559515380859374
aF2.9083792114257814
aF2.907071533203125
aF2.8781671142578125
aF2.889925231933594
aF2.863397216796875
aF2.845735778808594
aF2.8124102783203124
aF2.806773376464844
aF2.7926019287109374
aF2.750188903808594
aF2.7690283203125
aF2.734330749511719
aF2.7483950805664064
aF2.7120086669921877
aF2.69423828125
aF2.6743359375
aF2.6501849365234373
aF2.678541259765625
aF2.6261541748046877
aF2.6455047607421873
aF2.594785461425781
aF2.5988665771484376
aF2.59547607421875
aF2.5863519287109376
aF2.588013000488281
aF2.558851776123047
aF2.547857360839844
aF2.537109375
aF2.5285833740234374
aF2.5233578491210937
aF2.516055908203125
aF2.516744842529297
aF2.507623596191406
aF2.506003570556641
aF2.503043212890625
aF2.486968688964844
aF2.469756774902344
aF2.4565803527832033
aF2.455307922363281
aF2.4382699584960936
aF2.4426544189453123
aF2.4161907958984377
aF2.4246864318847656
aF2.410897674560547
aF2.417342071533203
aF2.4097213745117188
aF2.3757473754882814
aF2.3974147033691406
aF2.3964476013183593
aF2.3746620178222657
aF2.3512339782714844
aF2.3535757446289063
aF2.378793182373047
aF2.3591658020019532
aF2.3443170166015626
aF2.338956604003906
aF2.3546580505371093
aF2.3527159118652343
aF2.332680511474609
aF2.347209167480469
aF2.322242431640625
aF2.31976318359375
aF2.322640380859375
aF2.301883544921875
aF2.3025384521484376
aF2.302384948730469
aF2.300855865478516
aF2.2931504821777344
aF2.2907252502441406
aF2.2833322143554686
aF2.281505126953125
aF2.244848327636719
asS'test_loss'
p3
(lp4
F4.505090637207031
aF4.402333374023438
aF4.283316650390625
aF4.144716491699219
aF3.9954354858398435
aF3.8401889038085937
aF3.6946652221679686
aF3.5903643798828124
aF3.490886535644531
aF3.450714416503906
aF3.3937753295898436
aF3.3589358520507813
aF3.2991619873046876
aF3.267633056640625
aF3.2197637939453125
aF3.19945556640625
aF3.1593582153320314
aF3.1476925659179686
aF3.12013916015625
aF3.0815695190429686
aF3.0991561889648436
aF3.058899230957031
aF3.006182861328125
aF2.9881732177734377
aF2.98882568359375
aF2.94953857421875
aF2.9543115234375
aF2.938692321777344
aF2.911845703125
aF2.8895504760742186
aF2.86372314453125
aF2.827498779296875
aF2.85273193359375
aF2.8302850341796875
aF2.794697265625
aF2.7721136474609374
aF2.774947204589844
aF2.742786560058594
aF2.7455682373046875
aF2.7470932006835938
aF2.7287091064453124
aF2.7234982299804686
aF2.6722882080078123
aF2.6486669921875
aF2.655867004394531
aF2.645731201171875
aF2.628380126953125
aF2.6424993896484374
aF2.6212762451171874
aF2.576752014160156
aF2.593963623046875
aF2.5780099487304686
aF2.548819274902344
aF2.547238464355469
aF2.5500274658203126
aF2.5148480224609373
aF2.529307098388672
aF2.5040908813476563
aF2.485682373046875
aF2.487631378173828
aF2.4874447631835936
aF2.4597718811035154
aF2.4613247680664063
aF2.4499237060546877
aF2.4435154724121095
aF2.4288070678710936
aF2.4217460632324217
aF2.434309387207031
aF2.429185638427734
aF2.399705810546875
aF2.4197230529785156
aF2.401533660888672
aF2.389141845703125
aF2.394597930908203
aF2.3962496948242187
aF2.366785888671875
aF2.367505645751953
aF2.3549530029296877
aF2.3648590087890624
aF2.3557325744628907
aF2.3413020324707032
aF2.348510437011719
aF2.3236763000488283
aF2.3272987365722657
aF2.331562957763672
aF2.3310807800292968
aF2.3173155212402343
aF2.3141238403320314
aF2.3068251037597656
aF2.302833251953125
aF2.3169464111328124
aF2.3013961791992186
aF2.278717041015625
aF2.2841943359375
aF2.2856985473632814
aF2.2630433654785156
aF2.2859762573242186
aF2.291908416748047
aF2.2722479248046876
aF2.270851745605469
asS'chunk_len'
p5
I200
sS'learning_rate'
p6
F0.0007804476823494159
sS'batch_size'
p7
I100
sS'n_epochs'
p8
I100
sS'elapsed_time'
p9
S'1m 35s'
p10
sS'final_test_loss'
p11
F2.270851745605469
sS'num_layers'
p12
cnumpy.core.multiarray
scalar
p13
(cnumpy
dtype
p14
(S'i8'
p15
I0
I1
tp16
Rp17
(I3
S'<'
p18
NNNI-1
I-1
I0
tp19
bS'\x02\x00\x00\x00\x00\x00\x00\x00'
p20
tp21
Rp22
sS'model'
p23
S'vanilla_tanh'
p24
sS'hidden_size'
p25
g13
(g17
S'\x9c\x00\x00\x00\x00\x00\x00\x00'
p26
tp27
Rp28
s.